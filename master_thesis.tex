\documentclass[12pt]{article}

\usepackage[top=2cm,bottom=3cm,left=3cm,right=2cm]{geometry}
\usepackage{amsmath}


\DeclareMathOperator*{\argmax}{argmax}
\DeclareMathOperator*{\argmin}{argmin}
\DeclareMathOperator*{\E}{\mathbb{E}}

\usepackage{amsfonts}
\usepackage{amsthm}
\usepackage{fontspec}
\usepackage{hyperref}
\usepackage[thinc]{esdiff}
\usepackage{cite}
\hypersetup{
    colorlinks=true,
    linkcolor=blue,
    filecolor=magenta,      
    urlcolor=cyan,
    citecolor=cyan
}


% this package for increase or decrease vertical space between section chapter paragraph etc.
\usepackage{titlesec}

%tikz package for drawing graphs%
\usepackage{tkz-berge}
\usepackage{tikz}

\usetikzlibrary{arrows}

\usetikzlibrary{decorations.markings}
\usetikzlibrary{positioning,chains,fit,shapes,calc}

\usetikzlibrary{trees,positioning,fit,arrows,decorations.pathreplacing}

\usetikzlibrary{shapes.geometric, shapes.misc, positioning, calc, arrows.meta}
\renewcommand{\figurename}{Նկ.}
% the figure insert specific place with [H] param
\usepackage{float}




\newcommand{\undertextline}[2]{ {
\renewcommand{\arraystretch}{0.7}
\begin{tabular}{p{#2cm}}
\\
\hline
\centering{{\fontsize{8pt}{8pt} \textit{#1}}}
\end{tabular}} }


\newcommand*{\vertbar}{\rule[-1ex]{0.5pt}{2.5ex}}
\newcommand*{\horzbar}{\rule[.5ex]{2.5ex}{0.5pt}}




% increase distance between two lines%
\renewcommand{\baselinestretch}{1.5}

% increase vertical space between section subsection%
\titlespacing*{\section}
{0pt}{8ex plus 1ex minus .2ex}{10ex plus .2ex}
\titlespacing*{\subsection}
{0pt}{5ex plus 1ex minus .2ex}{3ex plus .2ex}


\setmainfont[
BoldItalicFont=arnamu_italic_bold.ttf,
BoldFont      =arnamu_bold.ttf,
ItalicFont    =arnamu_italic.ttf]{arnamu.ttf}

\renewcommand{\contentsname}{Բովանդակություն}
\renewcommand{\refname}{Գրականություն}

\renewcommand{\tablename}{Աղյուսակ}
%\usepackage{mathtools}
%\newcommand\defeq{\stackrel{\mathclap{\normalfont\mbox{def}}}{=}}
\newcommand\defeq{\mathrel{\overset{\makebox[0pt]{\mbox{\normalfont\tiny\sffamily def}}}{=}}}

\sloppy
\begin{document}

\newtheorem{theorem}{Թեորեմ}
\newtheorem{lemma}{Լեմմա}
\newtheorem{corollary}{Հետևանք}
\newtheorem{preposition}{Պնդում}
\newtheorem{defination}{Սահմանում}


\theoremstyle{definition} %After this line new defined commands by \newthorem will be non italic.%
\newtheorem{innercustomcase}{Դեպք}
\newenvironment{customcase}[1]
  {\renewcommand\theinnercustomcase{#1}\innercustomcase}
  {\endinnercustomcase}
\newtheorem{case}{Դեպք}

\raggedbottom


\begin{titlepage}

\begin{center}
{\fontsize{18pt}{18pt} \selectfont \textbf{ԵՐԵՎԱՆԻ ՊԵՏԱԿԱՆ ՀԱՄԱԼՍԱՐԱՆ\\}} 
\vspace{7mm}
{\fontsize{18pt}{18pt} \selectfont \textbf{ԻՆՖՈՐՄԱՏԻԿԱՅԻ ԵՎ ԿԻՐԱՌԱԿԱՆ ՄԱԹԵՄԱՏԻԿԱՅԻ ՖԱԿՈՒԼՏԵՏ\\}}
\vspace{7mm}
{\fontsize{16pt}{16pt} \selectfont \textbf{ Թվային անալիզի և մաթեմատիկական մոդելավորման ամբիոն\\}}
\vspace{7mm}
{\fontsize{18pt}{18pt} \selectfont \textbf{ «ԹՎԱՅԻՆ ԱՆԱԼԻԶ ԵՎ ՄԱԹԵՄԱՏԻԿԱԿԱՆ ՄՈԴԵԼԱՎՈՐՈՒՄ»
ԿՐԹԱԿԱՆ ԾՐԱԳԻՐ\\}}

\vspace{25mm}
{\fontsize{18pt}{18pt} \selectfont \textbf{ՄԻՆԱՍՅԱՆ ԳԵՎՈՐԳ ՄԱՆՎԵԼԻ\\}}
\vspace{15mm}
{\fontsize{18pt}{18pt} \selectfont \textbf{ՄԱԳԻՍՏՐՈՍԱԿԱՆ ԹԵԶ\\}}
\vspace{5mm}
{\fontsize{16pt}{16pt} \selectfont \textbf{ՏՐԱՆՍՖԵՐԱՅԻՆ ՈՒՍՈՒՑՄԱՆ ՈՐՈՇԱԿԻ ՄԵԹՈԴԻ ԸՆԴՀԱՆՐԱՑՄԱՆ ՍԽԱԼԱՆՔԻ ԳՆԱՀԱՏՄԱՆ ՄԱՍԻՆ\\}}
\vspace{15mm}
{\fontsize{14pt}{14pt} \selectfont \textit{\textbf{«Ինֆորմատիկա և կիրառական մաթեմատիկա»  մասնագիտությամբ 
ինֆորմատիկայի և կիրառական մաթեմատիկայի մագիստրոսի որակավորման աստիճանի հայցման համար\\}}}
\vspace{30mm}
{\fontsize{13pt}{13pt} \selectfont \textbf{ ԵՐԵՎԱՆ 2019\\}}
\end{center}

\end{titlepage}

{


{\fontsize{13pt}{13pt} \selectfont \textit{\textbf{Ուսանող՝}}}
 \hspace{0.3cm} \undertextline{ստորագրություն}{7} \hspace{0.3cm}  {\fontsize{13pt}{13pt} \selectfont \textbf{Մինասյան Գ.Մ.}}\\
 
\vspace{5mm}

{\fontsize{13pt}{13pt} \selectfont \textit{\textbf{Գիտական ղեկավար՝  \hspace{0.3cm}ֆիզ.-մաթ. գիտությունների թեկնածու}}}\\


\large{\hspace{4cm} \undertextline{ստորագրություն}{7} \hspace{0.3cm} \hspace{0.3cm}  {\fontsize{13pt}{13pt} \selectfont \textbf{Դանոյան Հ.Է.}}}\\


\vspace{30mm}


{\fontsize{13pt}{13pt} \selectfont \textit{\textbf{«Թույլատրել պաշտպանության»}}}\\

{\fontsize{13pt}{13pt} \selectfont \textit{\textbf{Ամբիոնի վարիչ՝   \hspace{0.3cm}ֆիզ.-մաթ. գիտությունների դոկտոր, պրոֆեսոր}}}\\

\large{\hspace{3.5cm}  \undertextline{ստորագրություն}{7} \hspace{0.3cm} {\fontsize{13pt}{13pt} \selectfont \textbf{Հակոբյան Յու.Ռ.}}  } \\

{\fontsize{13pt}{13pt} \selectfont \textit{{31 մայիսի 2019 թ.}}}
}
\pagebreak






\begin{center}
\Large{\textbf{Համառոտագիր}}
 \end{center}
 \vspace{10mm}
{
\small Տրանսֆերային ուսուցման որոշակի մեթոդի ընդհանրացման սխալանքի գնահատման մասին\\
On Generalization Error Bound Estimation of Certain Transform Learning Method
}


\pagebreak



\pdfbookmark{Բովանդակություն}{contents}
\tableofcontents
\newpage


\section*{\hfill 
\begin{center}
Ներածություն
 \end{center}
 \hfill} \noindent
\phantomsection
\addcontentsline{toc}{section}{Ներածություն}
\pagebreak


\section*{\hfill 
\begin{center}
 Տրանսֆերային ուսուցում
 \end{center}
 \hfill} \noindent
\phantomsection
\addcontentsline{toc}{section}{Տրանսֆերային ուսուցում}



Մեքենայական ուսուցման բազմաթիվ մեթոդներում կատարվում է մի ընդհանուր ենթադրություն, որ ուսուցման և փորձարկվող տվյալները ընտրվում են միևնույն առանձնահատկությունների տարածությունից,  անկախ և միևնույն բաշխումից։ Երբ բաշխումը փոխվում է, վիճակագրական և մեքենայական ուսուցման մոդելների մեծ մասը պետք է սկզբից վերակառուցել՝ օգտագործելով նոր հավաքագրված ուսուցման տվյալներ։ Բազմաթիվ կիրառական խնդիրներում անհրաժեշտ  ուսուցման տվյալների հավաքագրումը  անհնար է կամ ծախսատար։   Խնդիրների որոշ տիրույթներ ինչպիսիք են բիոինֆորմատիկան և ռոբոտաշինությունը, տվյալների ձեռքբերման և անոտացման ծախսատար լինելու պատճառով՝  մեծ քանակությամբ անոտացված տվյալներ հավաքագրելը  շատ բարդ է և թանկարժեք, որն էլ սահմանափակում է այդ տիրույթների զարգացումը։ Շատ կարևոր է կրճատել ուսուցման տվյալների   հավաքագրման անհրաժեշտությունը խնդիրների այնպիսի տիրույթներում, որտեղ բավարար քանակությամբ ուսուցման տվյալներ չկան։ Անբավարար տվյալների դեպքում մեքենայական ուսուցման  հիմնական խնդիրները լուծելու համար տրանսֆերային ուսուցումը  շատ կարևոր գործիք է։ \par Մարդիկ հատուկ կարողություն ունեն տարբեր առաջադրանքներում օգտագործած գիտելիքները և հմտությունները կիրառել  նաև այլ առաջադրանքներում։ Ինչ-որ առաջադրանքի կատարումը սովորելու ընթացքում ձեռք բերված գիտելիքները մենք օգտագործում ենք նմանատիպ առաջադրանքներ լուծելու ժամանակ։  Որքան իրար հետ կապված և նման են առաջադրանքները,  մեզ համար այնքան հեշտ է  գիտելիքների փոխանցումը առաջադրանքների միջև։ Նոր առաջադրանքներ կատարելու համար, մենք այն չենք սովորում ամենասկզբից, այլ անցյալում ունեցած մեր գիտելիքները և հմտությունները օգտագործում ենք այդ առաջադրանքում։

 \par Բազմաթիվ մեքենայական ուսուցման և խորը ուսուցման ալգորիթմները նախատեսված են մեկուսացված առաջադրանքների մոդելավորման համար։  Տրանսֆերային ուսուցումը առաջարկում է նոր մոտեցում ըստ որի մեկուսացված առաջադրաքների ուսուցումը փոխարինվում է հետևյալ կերպ՝ արդեն իսկ սովորած ինչ-որ առաջադրանքից ձեռք բերված գիտելիքները օգտագործել առնչվող այլ  առաջադրանքներում։  Այսպիսով տրասֆերային ուսուցումը փորձում է խնդրի  կամ առաջադրանքի մի տիրույթի՝ այսպես կոչված աղբյուր տիրույթից, գիտելիքն ու փորձը փոխանցել դեպի այլ  առաջադրանքի թիրախային տիրույթ։ Սովորական մեքենայական ուսուցման և տրանսֆերային ուսուցման միջև տարբերությունները պատկերված են նկար \ref{fig:ml_vs_tl}-ում։


% for double arrows a la chef
% adapt line thickness and line width, if needed
\tikzstyle{vecArrow} = [thick, decoration={markings,mark=at position
   1 with {\arrow[semithick]{open triangle 60}}},
   double distance=2.5pt, shorten >= 5.5pt,
   preaction = {decorate},
   postaction = {draw,line width=2.5pt, white,shorten >= 4.5pt}]
\tikzstyle{innerWhite} = [semithick, white,line width=1.4pt, shorten >= 4.5pt]


\begin{figure}[h]
\centering

\begin{tikzpicture}[
    arr/.style={-{Latex[length=2mm]}},
    persistence/.style={cylinder, shape border rotate=90, 
        minimum height=1.3cm, minimum width=2cm, draw},
        persistence2/.style={cylinder, shape border rotate=90, 
        minimum height=0.8cm, minimum width=1.5cm, draw},
    task_rec/.style={rectangle, draw, minimum height=2cm, minimum width=3.5cm},
     knowledge_rec/.style={rectangle, draw, minimum height=1.5cm, minimum width=2.5cm}
    ]


\node[persistence, label=below : {Տվյալներ 1}] (per1)  at (0, 6) {};
\node(task1) [task_rec, text width=3cm,align=center] at (4, 6) {Առաջադրանք 1-ի  ուսուցում} ;
 \draw[vecArrow] (per1) to (task1);



\node(task2) [task_rec, text width=3cm,align=center] at (4, 0) {Առաջադրանք 2-ի   ուսուցում} ;
\node[persistence, label=below : {Տվյալներ 2}] (per2)  at (0, 0) {};
\draw[vecArrow] (per2) to (task2);



 
  
  \node[persistence, label=below : {Տվյալներ 1}] (per1_tl)  at (8, 6) {};
\node(task1_tl) [task_rec, text width=3cm,align=center] at (12, 6) {Առաջադրանք 1-ի  ուսուցում} ;
 \draw[vecArrow] (per1_tl) to (task1_tl);



\node(task2_tl) [task_rec, text width=3cm,align=center] at (12, 0) {Առաջադրանք 2-ի   ուսուցում} ;
\node[persistence2, label=below : {Տվյալներ 2}] (per2_tl)  at (8, 0) {};
\draw[vecArrow] (per2_tl) to (task2_tl);



\node(kn_rec_tl) [knowledge_rec] at (12, 3) {Գիտելիք} ;
\draw[vecArrow] (task1_tl) to (kn_rec_tl);
\draw[vecArrow] (kn_rec_tl) to (task2_tl);
 
 
  % grouping ml nodes
  \node[fit=(per1)(task1)](group_ml){};
  
  \draw[line width=1pt,decorate,decoration={amplitude=7pt,brace}]
  (group_ml.north west) -- (group_ml.north east);
  \node[above=of group_ml,anchor=center]{Մեկուսացված առաջադրանք 1};
  
  
    % grouping ml nodes
  \node[fit=(per2)(task2)](group_22_ml){};
  
  \draw[line width=1pt,decorate,decoration={amplitude=7pt,brace, mirror}]
  (group_22_ml.south west) -- (group_22_ml.south east);
  \node[below=of group_22_ml,anchor=center]{Մեկուսացված առաջադրանք 2};

  % grouping ml nodes
  \node[fit=(per1_tl)(task1_tl)](group_tl){};
  
  \draw[line width=1pt,decorate,decoration={amplitude=7pt,brace}]
  (group_tl.north west) -- (group_tl.north east);
  \node[above=of group_tl,anchor=center]{Աղբյուրի տիրույթ};
  
  
  
  % grouping ml nodes
  \node[fit=(per2_tl)(task2_tl)](group_2_tl){};
  
  \draw[line width=1pt,decorate,decoration={amplitude=7pt,brace, mirror}]
  (group_2_tl.south west) -- (group_2_tl.south east);
  \node[below=of group_2_tl,anchor=center]{Թիրախի տիրույթ};
  
  
  
  % grouping ml nodes
  \node[fit=(task1_tl)(task2_tl)](group_3_tl){};
  
  \draw[line width=1pt,decorate,decoration={amplitude=7pt,brace}]
  (group_3_tl.north east) -- (group_3_tl.south east);
  \node[right=of group_3_tl,anchor=center, rotate=90	]{\textbf{Տրանսֆերային ուսուցում}};
  
  
    % grouping ml nodes
  \node[fit=(per1)(per2)](group_2_ml){};
  
  \draw[line width=1pt,decorate,decoration={amplitude=7pt,brace, mirror}]
  (group_2_ml.north west) -- (group_2_ml.south west);
  \node[left=of group_2_ml,anchor=center, rotate=90]{\textbf{Սովորական մեքենյական ուսուցում}};

%\draw [->, arr] (dts.south) to (per.top);
\end{tikzpicture}
\caption{Տրանֆերային ուսուցուման և սովորական մեքենայական ուսուցման միջև տարբերությունը} \label{fig:ml_vs_tl}
\end{figure}



\begin{center}
\subsection*{
 \center{Տրանսֆերային ուսուցման տեսակները}} 
 \end{center}
 \noindent
\phantomsection
\addcontentsline{toc}{subsection}{Տրանսֆերային ուսուցման տեսակները}


Տրանսֆերային ուսուցման տեսակները և մեթոդները բազմաթիվ են (տես օրինակ \cite{bib_item_1, bib_item_2, bib_item_3}), որոնք կարելի կիրառել կախված առաջադրանքից և տվյալների հասանելիությունից։ Տրանսֆերային ուսուցման մեթոդները կարելի է դասակարգել ըստ սովորական մեքենայական ուսուցման ալգորիթմի տեսակների՝ \linebreak

\noindent \textbf{Ինդուկտիվ տրանսֆերային ուսուցում։}  Այս դեպքում աղբյուրի և թիրախի տիրույթները նույն են սակայն տարբեր են առաջադրանքները և թիրախ տիյույթի առաջադրանքները վերահսկվող են։ Այս տեսակին պատկանող ալգորիթմները փորձում են աղբյուր տիրույթի համար կատարված ենթադրությունները օգտագործելով  թիրախի առաջադրանքի լուծման արդյունավետությունը լավացնել։ Կախված նրանից, թե աղբյուր տիրույթը պարունակում է պիտակավորված տվյալնել, թե ոչ  կարելի ինդուկտիվ տրանսֆերային ուսուցումը համապատասխանաբար բաժանել ենթատեսակների՝ \textit{բազմառաջադրանքային ուսուցում} և  \textit{ինքնուրույն ուսուցում}։\\

\noindent \textbf{Ոչ վերահսկվող տրանսֆերային ուսուցում։} Նման է ինդուկտիվ տրանսֆերային ուսուցմանը, սակայն թիրախ տիրույթի առաջադրանքները ոչ վերահսկվող  են։ Աղբյուրի և թիրախի տիրույթները  նույնն են և երկու տիրույթներում էլ պիտակավորված տվյալներ հասանելի չեն։ \\


\noindent \textbf{Շարունակական տրանսֆերային ուսուցում։}
Այս դեպքում աղբյուրի և թիրախի առաջադրանքները նմանություններ ունեն, բայց համապատասխան տիրույթները տարբեր են։ Աղբյուրի տիրույթում կա մեծ քանակությամ անոտացված տվյալներ, մինչդեռ թիրախի տիրույթում անոտացված տվյալներ հասանելի չեն։ \\

\begin{center}
\subsection*{
 \center{Տրանսֆերային ուսուցումը խորը ուսուցման համար}} 
 \end{center}
 \noindent
\phantomsection
\addcontentsline{toc}{subsection}{Տրանսֆերային ուսուցումը խորը ուսուցման համար}

Վերջին տարիներին խորը ուսուցուման միջոցով կիրառական բազմաթիվ խնդիրներ հնարավոր է բարձր արդյունավետությամբ լուծել։ Խորը ուսուցման ալգորիթմները փորձում են զանգածային տվյալներից ավելի բարձր մակարդակի  ներկայացումներ սովորել։  Ոչ վերահսկվող կամ կիսավերահսվող խորը ուսուցման ալգորիթմները ավտոմատ կերպով տվյալներից նոր և բարձր մակարդակի ներկայացումներ է դուրս հանում։ Ի տարբերություն ավանդական մեքենայական ուսուցման մեթոդների, որտեղ  տվյալների առանձնահատկությունները ձեռքով են ընտրվում,  խորը ուսուցման մեթոդներում տվյալների առաձնահատկությունները ավտոմատ կերպով կառուցվում են ուսուցման ընթացքում։ Սակայն ուսուցման ժամանակը և անհրաժեշտ տվյալների քանակը խորը ուսուցման համակարգեր ստեղծելու համար շատ ավելին է՝ համեմատած ավանդական մեքենայական ուսուցման համակարգերի համար։ Վերջին տարիներին բնական լեզվի մշակման(տես օրինակ \cite{bib_item_6, bib_item_7}) և պատկերների ճանաչման (տես օրինակ \cite{bib_item_4, bib_item_5}) տարատեսակ խնդիրների համար բարձր արդյունավետությամբ(երբեմն մարդուն գերազանցող) խորը ուսուցման նեյրոնային ցանցեր են  մշակվել։ Շատ դեպքերում նախապես սովորած նեյոնային ցանցերը օգտագործվում են այլ առաջադրանքներում։ Խորը ուսուցման համատեքստում նախապես մշակված ցանցերը կամ մոդելները կազմում են հիմքը  խորը փոխանցումային ուսուցման համար։ Խորը փոխանցումային ուսուցման մեթոդները մտնում են ինդուկտիվ տրասֆերային ուսուցման մեթոդների մեջ։ Այժմ նկարագրենք խորը փոխանցումային ուսուցման ամենահայտնի մեթոդներից մեկը։ \\

\begin{center}
\subsection*{
 \center{Մշակված նեյրոնային ցանցը որպես տվյալների ներկայացման հիմք}} 
 \end{center}
 \noindent
\phantomsection
\addcontentsline{toc}{subsection}{Մշակված նեյրոնային ցանցը որպես տվյալների ներկայացման հիմք}

Խորը ուսուցման համակարգերը և մոդելները ունեն շերտային կառուցվածքներ, տարբեր շերտեր տվյալների տարբեր ներկայացումներ են սովորում։ Վերահսկվող ուսուցման դեպքում վերջին ելքային շերտը, որի նեյրոները քանակը համընկնում  է  դիտարկվող առաջադրանքում մասնակցող դասերի քանակին և յուրաքանչյուր դասի համապատասխանում է այդ շերտի մեկ նեյրոն,   միանում է տվյաների ներկայացման շերտերից վերջինին։ Այս շերտային կառուցվածքը հնարավոր է դարձնում է օգտագործել արդեն պատրաստի մշակված ցանցը առանց վերջին շերտի։ Նախավերջին շերտը օգտագործվում է որպես տվյալների ներկայացման աղբյուր և օգտագործվում այլ առաջադրանքներում։  Սա ամենատարածված մեթոդներից մեկն է տրանսֆերային ուսուցում իրականացնելու համար՝ օգտագործելով խորը նեյրոնային ցանցեր։ Փորձնական եղանակով ցույց է տրվել որը պատրաստի խորը նեյրոնայից ցանցերի ներկայացումների միջոցով սահմանափակ տվյալների վրա այլ դասակարգման առաջադրանքներ հնարավոր է լուծել բարձր արդյունավետությամբ (տես օրինակ \cite{bib_item_8})։ \\




\begin{figure}[h]
\centering

\begin{tikzpicture}[
    arr/.style={-{Latex[length=2mm]}},
    persistence/.style={cylinder, shape border rotate=90, 
        minimum height=1.3cm, minimum width=2cm, draw},
        persistence2/.style={cylinder, shape border rotate=90, 
        minimum height=0.8cm, minimum width=1.5cm, draw},
    task_rec/.style={rectangle, draw, minimum height=2cm, minimum width=3.5cm},
     input_rec/.style={rectangle, draw, minimum height=1cm, minimum width=4.8cm},
      conv_rec/.style={rectangle, draw, minimum height=1cm, minimum width=4.8cm},
      fc/.style={rectangle, draw, minimum height=1cm, minimum width=4.8cm},
      linclf/.style={rectangle, draw, minimum height=1cm, minimum width=3cm},
    ]
   
\node(linclf_1)[linclf] at (0, 9){\small Սոֆթմաքս};  
\node(fc_12)[fc] at (0, 7.5){\small Լրիվ կապակցված շերտ 2};  
\node(fc_11)[fc] at (0, 6){\small Լրիվ կապակցված շերտ 1};  
\node(conv_rec_13)[conv_rec] at (0, 4.5){\small փաթույթային շերտ 3};  

\node(conv_rec_12)[conv_rec] at (0,3){\small փաթույթային շերտ 2};   
\node(conv_rec_11)[conv_rec] at (0,1.5){\small փաթույթային շերտ 1};
\node(input_rec_11)[input_rec] at (0, -0.5){\small Աղբյուր տվյալների շերտ};

\draw[vecArrow] (input_rec_11) to (conv_rec_11);
\draw[vecArrow] (conv_rec_11) to (conv_rec_12);
\draw[vecArrow] (conv_rec_12) to (conv_rec_13);
\draw[vecArrow] (conv_rec_13) to (fc_11);
\draw[vecArrow] (fc_11) to (fc_12);
\draw[vecArrow] (fc_12) to (linclf_1);



\node(linclf_2)[linclf] at (10, 9){\small Գծային դասակարգիչ};  
\node(fc_21)[fc] at (10, 6){\small Լրիվ կապակցված շերտ 1};  
\node(conv_rec_23)[conv_rec] at (10, 4.5){\small փաթույթային շերտ 3};  

\node(conv_rec_22)[conv_rec] at (10,3){\small փաթույթային շերտ 2};   
\node(conv_rec_21)[conv_rec] at (10,1.5){\small փաթույթային շերտ 1};
\node(input_rec_21)[input_rec] at (10, -0.5){\small Թիրախ տվյալների շերտ};

\draw[vecArrow] (input_rec_21) to (conv_rec_21);
\draw[vecArrow] (conv_rec_21) to (conv_rec_22);
\draw[vecArrow] (conv_rec_22) to (conv_rec_23);
\draw[vecArrow] (conv_rec_23) to (fc_21);


\node(rep_node)[text width=3cm] at (10, 7.5) {Ներկայացումներ};

\draw[vecArrow] (fc_21) to (rep_node);
\draw[vecArrow] (rep_node) to (linclf_2);

\draw[dashed] (-3, 6.75) -- (3, 6.75);


\node(tr_arr)[draw, single arrow,
              minimum height=32mm, minimum width=15mm,
              single arrow head extend=3mm,
              anchor=west] at (3.5,3.7) {\small Փոխանցում};
              
              
%\draw (3,6.2) -- (3.5,3.7);
\draw [dashed](fc_11.north east) -- (tr_arr.west);
\draw[dashed] (conv_rec_11.south east) -- (tr_arr.west);



\draw[dashed] (fc_21.north west) -- (tr_arr.east);
\draw [dashed](conv_rec_21.south west) -- (tr_arr.east);

%\draw [->, arr] (dts.south) to (per.top);
\end{tikzpicture}
\caption{Տրանսֆերային ուսուցումը պատրաստի նեյրոնային ցանցի միջոցով } \label{fig:ml_vs_tl}
\end{figure}









\pagebreak


\section*{
 \center{Տրասֆերային ուսուցման որոշակի մեթոդի \\ տեսական երաշխիքներ}} \noindent
\phantomsection
\addcontentsline{toc}{section}{Տրասֆերային ուսուցման որոշակի մեթոդի տեսական երաշխիքներ}
 
 $\mathcal{X}$-ով նշանակենք բոլոր հնարավոր տվյալների օրինակները, իսկ $\mathcal{C}$-ով նշանակենք բոլոր պիտակների կամ դասերի բազմությունը։ Յուրաքանչյուր $c \in \mathcal{C}$ դասին համպատասխանում է $\mathcal{X}$ բազմության վրա որոշված ինչ-որ $\mathcal{D}_c(x)$ բաշխում, այն ցույց է տալիս, թե $x$ օրինակը ինչքանով է $c$ դասին համապատասխան։ Ուսուցումը կատարվում է $\mathcal{F}$ ներկայացումների ֆունկցիաների դասի վրա։ $\forall f \in \mathcal{F}$  ֆունկցիա $\mathcal{X}$ տվյաների բազմությունը արտապատկերում $d$-չափանի էվկլիդյան $\mathcal{R}^d$տարծություն՝ $f:\mathcal{X}\rightarrow\mathcal{R}^d$, բացի այդ կդիտարկենք միայն սահմանափակ ֆունկցիաները՝
 $$||f(x)|| \leq R \text{    } \forall x \in \mathcal{X} \text{ և } R > 0։$$ 


%\noindent Նաև կենթադրենք, որ դասերի վրա կա ինչ-որ $\rho$ բաշխում, որը բնութագրում է, թե ինչպես են այդ դասերը հանդիպում չպիտակավորված տվյալներում:

\begin{center}
\subsection*{
 \center{Վերահսկվող առաջադրանքներ}} 
 \end{center}
 \noindent
\phantomsection
\addcontentsline{toc}{subsection}{Վերահսկվող առաջադրանքներ}

\par Այժմ կնկարագրենք այն առաջադրանքները, որոնց միջոցով փորձարկվելու է ներկայացումների $f$ ֆուկցիան: $k+1$ դասերից բաղկացած $\mathcal{T}$ վերահսկվող առաջադրանքը, բաղկացած է $$\{c_1, ..., c_{k+1}\} \subseteq \mathcal{C}$$
միմյանցից տարբեր դասերից: Կենթադրենք որ վերահսկվող առաջադրանքները ունեն $\mathcal{P}(\mathcal{T})$ բաշխում, որը բնութագրում է այդ առաջադրանքը դիտարկվելու հավանականությունը: $k+1$ դասերից բաղկացած վերահսկվող առաջադրանքների բաշխումը հետևյալն է՝ $$\mathcal{P}(\mathcal{T} \text{ } |\text{ }  |\mathcal{T}| = k +1)$$ Պիտակավորված տվյալների բազմությունը $\mathcal{T}$ առաջադրանքի համար բաղկացած է $m$ հատ միմյանցից անկախ և միևնույն բաշխումից ընտրրված օրինակներից: Այդ օրինակները ընտրվում են ստորև նկարագրված պրոցեսով:

\textit{$c \in \{c_1, ..., c_{k+1}\} $   դասը ընտրվում է ըստ $\mathcal{D}_{\mathcal{T}}$ բաշխման, որից հետո $x$ օրինակը ընտրվում է $\mathcal{D}_c$ բաշխումից: Դրանք միասին ձևավորում են պիտակավորված $(x, c)$ զույգը, որը ունի հետևյալ բաշխումը՝
$$\mathcal{D}_{\mathcal{T}} (x, c) = \mathcal{D}_{c}(x)\mathcal{D}_{\mathcal{T}}(c):$$}

\begin{center}
\subsection*{
 \center{Վերահսկվող ներկայացումների գնահատման չափը}} 
 \end{center}
 \noindent
\phantomsection
\addcontentsline{toc}{subsection}{Վերահսկվող ներկայացումների գնահատման չափը}


\par $f$ ներկայացումների ֆունկցիաի որակի գնահատումը կատարվում է $\mathcal{T}$ բազմադաս դասակարգման առաջադրանքի միջոցով՝ օգտագործելով գծային դասակարգիչ։  Ֆիքսենք $\mathcal{T} = \{c_1, ..., c_{k+1}\}$ առաջադրանքը։ $\mathcal{T}$ առաջադրանքի բազմադաս դասակարգիչը ֆուկցիա է՝ $g:\mathcal{X} \rightarrow \mathcal{R}^{k+1}$, որի արժեքի կորդինատները ինդեքսավորված են $\mathcal{T}$ առաջադրանքի դասերով։ $(x, y) \in \mathcal{X} \times \mathcal{T}$ կետում $g$ դասակարգիչով պայմանավորված կորուստը սահմանենք հետևյալ կերպ՝
$$l(\{ g(x)_y-g(x)_{y'}\}_{y \neq y'}  ),$$
որը ֆունկցիա կախված $k$ չափանի վեկտորից,  այն ստացվում է $k+1$ չափանի  $g(x)$ վեկտորի կորդինատների տարբերությունից, բացի այդ $\{ g(x)_y-g(x)_{y'}\}_{y \neq y'}$ վեկտորի կոմպոնենտները կամայական հերթականությամբ կարելի է համարակալել և $l$-ի արժեքը կախված չէ վեկտորի կոմպոնենտների համարակալման հերթականությունից։  Պրակտիկայում մեծ կիրառություն ունեցող երկու կորուստի ֆունկցիաներ ենք դիտարկելու աշխատանքում՝ ստանդարտ հինջ կորստի ֆունկցիան որը սահմանվում է հետևյալ կերպ՝
$$l(v) = \max\{0, 1+\max_{i}\{-v_i\}\} $$ և լոգիստիկ կորստի ֆունկցիան՝
$$l(v) = \log_2(1+\sum_{i}{e^{-v_i}}),$$
որտեղ $v \in \mathcal{R}^k$։ $\mathcal{T}$ առաջադրանքի համար $g$ դասակարգիչի կորուստը հետևյալն է՝
$$L(\mathcal{T}, g) \defeq \E_{(x, c) \sim \mathcal{D}_{\mathcal{T}}} \left [ l(\{ g(x)_c-g(x)_{c'}\}_{c \neq c'}  ) \right ]$$

$f$ ներկայացումների ֆունկցիան օգտագործելու նպատակով,  $g(x) = Wf(x)$ տեսքի դասակարգիչներն ենք դիտարկելու, որտեղ $W \in \mathcal{R}^{(k+1)\times d }$, որը ունի սահմանափակ նորմ՝ $||W|| \leq Q \text{ և }Q >0։$
$\mathcal{W}$-ով նշանակենք սահմանափակ նորմ ունեցող մատրիցաների բազմությունը՝
$$\mathcal{V} = \{W: ||W|| \leq Q \text{ և } Q > 0\}$$
 $\mathcal{T}$ առաջադրանքի համար $g(x) = Wf(x)$ ներկայացումից կախված գծային դասակարգչի կորստի ֆունկցիան հետևյալն է՝
$$L(\mathcal{T}, f, W) \defeq \E_{(x, c) \sim \mathcal{D}_{\mathcal{T}}} \left [ l(\{ Wf(x)_c-Wf(x)_{c'}\}_{c \neq c'}  ) \right ]$$
 
 Ֆիքսելով որևէ $f$ ներկայացում կարելի լավագույն $W$ գտնել, այնպես որ $f$-ից կախված գծային դասակարգչի կորուստը լինի ամենափոքրը, ուստի $f$ ներկայացման վերահսկիչ կորուստը $\mathcal{T}$ առաջադրանքի համար կսահմանենք, այն կորուստը, երբ լավագույն $W$ ենք ընտրել $f$-ի համար՝
 $$L(\mathcal{T}, f) \defeq \inf_{W \in \mathcal{V}} L(\mathcal{T}, f, W)$$


\begin{defination}[վերահսկիչ միջին կորուստ]
$k+1$ դասերից բաղկացած առաջադրանքների վերահսկիչ միջին կորուստը $f$ ներկայացման համար սահմանվում է որպես՝ 
$$L(f) \defeq \E_{\mathcal{T} \sim \mathcal{P}} \left [L (\mathcal{T}, f) \text{ } | \text{ } |\mathcal{T}| = k+1\right]$$
\end{defination}

\begin{defination}[էմպիրիկ վերահսկիչ միջին կորուստ]
Դիցուք ունենք միմյանցից անկախ $\mathcal{P}(\mathcal{T} \text{ } |\text{ }  |\mathcal{T}| = k +1)$ բաշխումից ընտրված $N$ հատ առաջադրանքներ՝ $\mathcal{T}_1, ..., \mathcal{T}_N$:
էմպիրիկ վերահսկիչ միջին կորուստը $f$ ներկայացման համար հետևյալն է՝ 
$$\hat{L}(f) \defeq \frac{1}{N}\sum_{i=1}^{N}L (\mathcal{T}_i, f)$$
\end{defination}
\pagebreak

\subsection*{\hfill Ռադեմախերի բարդությունը \hfill} \noindent

\phantomsection
\addcontentsline{toc}{subsection}{Ռադեմախերի բարդությունը}

$\mathcal{H}$-ով նշանակենք այն ֆունկցիաների բազմությունը, որի վրա իրականացվելու է ուսուցումը՝
$$\forall h \in \mathcal{H}, h:\mathcal{X} \rightarrow \mathcal{C}$$

\noindent $\mathcal{H}$ ֆունկցիաների բազմությունը կոչվում է հիպոթեզների բազմություն կամ հիպոթեզների դաս։ Ուսուցման ալգորիթմը $\mathcal{H}$ ֆունկցիաների բազմությունից, ընտրում է $h \in \mathcal{H}$ հիպոթեզ։
 Վերջավոր հիպոթեզների համար էմպիրիկ սխալանքի մինիմիզացիայի միջոցով ընտրված հիպոթեզը, ուսուցման էֆեկտիվ ալգորիթմ է (տես օրինակ \cite{bib_item_9, bib_item_10})։ Սակայն մեքենայական ուսուցման մեջ օգտագործվող հիպոթեզների բազմությունների հզորությունները սովորաբար անվերջ են։ 
Հիպոթեզների դասերի բարդությունը գնահատող տարբեր մեծություններ կան։ Պարզվում է անվերջ հիպոթեզների համար էֆեկտիվ ուսուցման ալգորիթմի գոյությունը կապված է հիպոթեզների բարդությունը գնահատող մեծությունների հետ։ Ստորև կներկայացնենք հիպոթեզների բարդությունը գնահատող մեծություններից մեկը՝   \textit{Ռադեմախերի բարդությունը} (տես օրինակ \cite{bib_item_9, bib_item_10})։

\par Դիցուք $l:\mathcal{H}\times \mathcal{Z} \rightarrow \mathbb{R}$ արտապատկերում է, որտեղ $\mathcal{Z} = \mathcal{X} \times \mathcal{C}$։ $l(h, z)$-ը ցույց է տալիս $h$ հիպոթեզի կորուստը $z = (x, c)$  պիտակավորված օրինակի համար։ Ներմուծենք $\mathcal{G}$ կորստի ֆունկցիաների ընտանիքը $\mathcal{H}$ հիպոթեզների համար՝
$$\mathcal{G} = \{z \mapsto l(h, z) | h \in \mathcal{H}\}$$

Սակայն Ռադեմախերի բարդության սահմանումները կտանք ավելի ընդհանուր ֆունկցիաների $\mathcal{G}$ դասի համար, որի ֆունկցիաները $\mathcal{Z}$-ը արտապատկերում են դեպի $\mathbb{R}$։

Ռադեմախերի բարդությունը ցույց է տալիս թե ինչքանով է «հարուստ» ֆունկցիաների ընտանիքը՝ չափելով պատահական աղմուկի հետ կորելացիան։ Ստորև  ֆորմալ կտանք  էմպիրիկ և միջին Ռադեմախերի բարդության սահմանումները։

\begin{defination}[Էմպիրիկ Ռադեմախերի բարդություն]
Դիցուք $\mathcal{G}$-ն $\mathcal{Z}$-ից դեպի $[a, b]$ հատված արտապատկերող ֆունկցիաների ընտանիք է և
$$S = \{z_i  | z_i \in \mathcal{Z}, \forall i \in [m]\}$$
$m$ հատ ֆիքսված օրինակների բազմություն է։ Այդ դեպքում $\mathcal{G}$ ֆունկցիաների ընտանիքի Ռադեմախերի բարդությունը կախված օրինակների $S$ բազմությունից տրվում է հետևյալ կերպ՝
$$\hat{\mathcal{R}}_S(\mathcal{G})  = \frac{1}{m}\E_{\sigma \sim \{\pm1\}^m} \left [\sup_{g \in \mathcal{G}} \sum_{i=1}^m \sigma_ig(z_i) \right] $$
որտեղ $\sigma = (\sigma_1, ..., \sigma_m)^T$ և $\sigma_i$-երը պատահական մեծություններ են, հավասար հավանականությամբ արժեքներ են ընդունում $\{-1, +1\}$-ից։ $\sigma_i$ պատահական մեծությունները կոչվում են Ռադեմախերի փոփոխականներ։
\end{defination}

Դիսուք $g_S$-ով նշանակենք այն $m$ չափանի վեկտորը, որի կոմպոնենտները $g$ ֆունկցիայի $S$ բազմության օրինակների վրա ընդունած արժեքներն են՝ $g_S = (g(z_1), ..., g(z_m))^T$։ Այժմ էմպիրիկ Ռադեմախերի բարդությունը կարող ենք գրել հետևալ ձևով՝

$$\hat{\mathcal{R}}_S(\mathcal{G})  = \E  _{\sigma \sim \{ \pm 1\}^m}  \left [  \sup_{g \in \mathcal{G}} \frac{\langle \sigma, g_S \rangle }{m} \right ]$$

$\langle \sigma, g_S \rangle$ սկալյար արտադրյալը ցույց է տալիս $g_S$ վեկտորի և $\sigma$ պատահական աղմուկի միջև կորելացիայի չափը։ $\sup_{g \in \mathcal{G}}  \frac{\langle \sigma, g_S \rangle}{m}$ սուպրեմումը ցույց է տալիս թե ինչքան է $\mathcal{G}$ ֆունկցիաների ընտանիքը $S$ բազմության վրա կորելացված $\sigma$-ի հետ։ Այսպիսով էմպիրիկ Ռադեմախերի բարդությունը ցույց է տալիս այն միջին չափը, թե ինչքանով է $\mathcal{G}$ ֆունկցիաների ընտանիքը $S$ բազմության վրա կորելացված պատահական աղմուկի հետ։ Այս մեծությունը բնութագրում է, թե ինչքան «հարուստ» է $\mathcal{G}$ ֆունկցիաների ընտանիքը, եթե ավելի «հարուստ» կամ բարդ է $\mathcal{G}$ ֆունկցիաների ընտանիքը, ապա ավելի $g_S$ վեկտորներ կստեղծվեն $\mathcal{G}$-ի միջոցով և ավելի մեծ կլինի պատահական աղմուլի հետ միջին կորելացիան։


\begin{defination}[Ռադեմախերի բարդություն]
$\mathcal{D}$-ով նշանակենք այն բաշխումը որտեղից $S$ օրինակները գեներացվում են։ Կամայական $m$ բնական թվի համար $\mathcal{G}$ ֆունկցիաների ընտանիքի Ռադեմախերի բարդությունը սահմանվում է որպես էմպիրիկ Ռադեմախերի բարդության մաթսպասում ըստ բլոր հնարավոր $m$ օրինակների $\mathcal{D}^m$ բաշխման՝
$$\mathcal{R}_m(\mathcal{G}) = \E_{S \sim \mathcal{D}^m} \left [     \hat{\mathcal{R}}_S(\mathcal{G})  \right] $$
\end{defination}
Այժմ ձևակերպենք Ռադեմախերի բարդության վրա հիմնված ընդհանրական սխալանքի գնահատականը, որը կկիրառենք աշխատանքում։ 
\begin{theorem}[\cite{bib_item_9}]
\label{rad_comp_th}
Դիցուք $\mathcal{G}$ ֆուկցիաների բազմությունը, որի յուրաքանչյուր ֆունկցիա  $Z$-ը արտապատկերոմ է $[0, 1]$ և $S = \{z_i\}_{i=1}^m$ m հզորությամբ միմյանցից անկախ և միևնույն բաշխումից ընտրված օրինակների բազմություն է։ Այդ դեպքում ցանկացած $\delta$ դրական թվի համար առվազն $1 - \delta$ հավանականությամբ բոլոր $g \in \mathcal{G}$ ֆունկցիաների համար տեղի ունի հետևյալ անհավասարությունները՝
\begin{equation}
\E[g(z)] \leq \frac{1}{m}\sum_{i=1}^mg(z_i) + 2\mathcal{R}_m(\mathcal{G}) + \sqrt{\frac{\log\left( \frac{1}{\delta} \right)}{2m}}
\end{equation}
և
\begin{equation}
\E[g(z)] \leq \frac{1}{m}\sum_{i=1}^mg(z_i) + 2\hat{\mathcal{R}}_S(\mathcal{G}) + 3\sqrt{\frac{\log \left( \frac{2}{\delta} \right)}{2m}}
\end{equation}
\end{theorem}

\phantomsection
\addcontentsline{toc}{subsection}{Հոֆդինգի անհավասարությունը}


\subsection*{\hfill Հոֆդինգի անհավասարությունը \hfill} \noindent

 Դիցուք ունենք $Z_1, ..., Z_m$ անկախ և միևնույն բաշխման պատահական մեծություններ։ Հավանականությունների տեսությունից հայտնի \textit{մեծ թվերի օրենքը} երաշխավորում է, որ եթե $m$-ը ձգտի անվերջության, ապա
 այդ պատահական մեծությունների էմպիրիկ միջինը զուգամիտելու է դրանց մաթսպասմանը։ Սակայն մեծ թվերի օրենքը ընդամենը ասիմտոտիկ գնահատական է և տրված $m$ օրինակների համար ինֆորմացիա չի տալիս էմպիրիկ միջինի և մաթսպասման միջև տարբերության մոդուլի մասին։ Ստորև ձևակերպենք Հոֆդինգի անհավասարությունը, որը տրված $m$ օրինակների համար գնահատական է տալիս էմպիրիկ միջինի և դրանց մաթսպասման  միջև հեռավորության մասին։ Այս անհավասարությունը կկիրառենք աշխատանքում։ 

\begin{lemma}[Հոֆդինգի անհավասարություն \cite{bib_item_10}]
\label{hofding_inq}
Դիցուք $Z_1, ..., Z_m$ անկախ և միևնույն բաշխման պատահական մեծություններ են և $\bar{Z} = \frac{1}{m}\sum_{i=1}^m{Z_i}$: Ենթադրենք $\E[\bar{Z}] = \mu$ և յուրաքանչյուր $i$-ի համար $\mathbb{P}[a \leq Z_i \leq b] = 1$: Այդ դեպքում ցանկացած $\epsilon > 0$ թվի համար տեղի ունի հետևյալը՝
$$\mathbb{P}\left[ \frac{1}{m}\sum_{i=1}^m{Z_i}-\mu > \epsilon \right] \leq e^{\frac{-2m\epsilon^2}{(b-a)^2}}$$ 
և
$$\mathbb{P}\left[ \frac{1}{m}\sum_{i=1}^m{Z_i}-\mu < -\epsilon \right] \leq e^{\frac{-2m\epsilon^2}{(b-a)^2}}$$ 
\end{lemma}

\subsection*{\hfill Վերահսկիչ միջին կորուստի գնահատականը \hfill} \noindent
\phantomsection
\addcontentsline{toc}{subsection}{Վերահսկիչ միջին կորուստի գնահատականը}

\begin{lemma}
\label{task_conc_lemm}
Դիցուք ունենք միմյանցից անկախ $\mathcal{P}(\mathcal{T} \text{ } |\text{ }  |\mathcal{T}| = k +1)$ բաշխումից ընտրված $N$ հատ առաջադրանքներ՝ $\mathcal{T}_1, ..., \mathcal{T}_N$ և ֆիքսենք կամայական $f \in \mathcal{F}$ ներկայացում։ $\hat{L}(f)$ էմպիրիկ վերահսկիչ միջին կորուստն է $f$ ներկայացման համար,  իսկ $L(f)$-ը վերահսկիչ միջին կորուստը և դիցուք $|\cup_{i=1}^N{\mathcal{T}_i}| = n$։
Այդ դեպքում առնվազն $1-\delta$ հավանականությամբ տեղի ունի հետևյալ անհավասարությունը։
\begin{equation}
\hat{L}(f) \geq L(f) - B\sqrt{\frac{\left(k+1\right)\log \left(\frac{1}{\delta}\right) }{2n}}
\end{equation}
որտեղ $B$ ինչ-որ դրական հաստատուն է։
\end{lemma}
\begin{proof}[Ապացույց]
Օգտվելով $L(\mathcal{T}_i, f)$ սահմանումից և օգտագործելով $f$-ի սահմանափակությունը հեշտ է համոզվել որ գոյություն ունի $B$ դրական թիվ այնպես որ կամայական $i \in [N]$ տեղի ունի հետևալը՝
$$0 \leq L(T_i,f) \leq B$$
Այժմ նկատենք որ \hyperref [hofding_inq]{Հոֆդինգի լեմմայի} պայմանները բավարարված են և օգտվելով այդ լեմմայի անհավասարությունից կունենաք՝
$$\mathbb{P}[\hat{L}(f) - L(f)]< -\epsilon] \leq e^{\frac{-2N\epsilon^2}{B^2}}$$
որտեղից և հավանականության $\mathbb{P}[A] = 1 - \mathbb{P}[\bar A]$ հատկությունը օգտագործելով՝
$$\mathbb{P}[\hat{L}(f) - L(f) \geq -\epsilon] \geq 1 -e^{\frac{-2N\epsilon^2}{B^2}} $$
$e^{\frac{-2N\epsilon^2}{B^2}}$ հավասարեցնենք $\delta$-ի՝
$$\delta = e^{\frac{-2N\epsilon^2}{B^2}}$$
և լուծելով այն $\epsilon$-ի նկատմաբ՝ կունենաք հետևյալը՝
$$\epsilon = B \sqrt{ \frac{\log\left(\frac{1}{\delta}\right)}{2N}} $$
Այսպիսով առնվազն $1-\delta$ հավանականությամբ տեղի ունի հետևյալ անահավսարությունը՝
$$\hat{L}(f) \geq L(f) - B \sqrt{ \frac{\log\left(\frac{1}{\delta}\right)}{2N}}$$
Նկատենք որ $n \leq (k+1)N$, որտեղից անմիջապես հետևում է հետևյալ անհավասարությունը՝
$$\sqrt{\frac{k+1}{n}} \geq \sqrt{\frac{1}{N}}$$
Օգտագործելով վերջին անհավասարությունը կունենանք, որ առնվազն $1-\delta$ հավանականությամբ տեղի ունի
$$\hat{L}(f) \geq L(f) - B\sqrt{\frac{\left(k+1\right)\log \left(\frac{1}{\delta}\right) }{2n}}$$
անհավասարությունը։
\end{proof}


Նախապես ներմուծեք որոշակի նշանակումներ, որոնք  կօգտագործենք հաջորդ լեմմայի ձևակերպան մեջ և ապացույցի ընթացքում։ Դիցուք $\mathcal{T}_1, ..., \mathcal{T_N}$ առաջադրանքները միմյանցից անկախ ընտրված են $\mathcal{P}(\mathcal{T} \text{ } |\text{ }  |\mathcal{T}| = k +1)$ բաշխումից և $\mathcal{T} = \cup_{i=1}^N{\mathcal{T}_i}$, որի հզորությունը $n+1$ է՝ $|\mathcal{T}| =  n +1$։
Այժմ $\rho_{min}$-ով նշանակենք $\mathcal{T}$ առաջադրանքում պարունակող դասերից ամենափոքրի հավանականությունը՝  

\begin{align}
\label{rhomin}
\rho_{min}  \defeq \min_{c \in \mathcal{T} } D_\mathcal{T}(c)
\end{align}

$m(c)$-ով նշանակենք այն $T_i$ առաջադրանքների քանակը, որոնցում $c$ դասը մասնակցում է, իսկ $m_{max}$-ով նշանակենք $m(c)$-ի առավելագույն արժեքը ըստ բոլոր $c \in \mathcal{T}$ դասերի՝
\begin{align}
\label{mmax}
m_{max} \defeq \max_{c \in T} m(c) 
\end{align}

 Դիտարկենք $(n+1) \times  d$  չափանի մատրիցան մատրիցան, որի տողերը ինդեքսավորված են $\mathcal{T}$ առաջադրանքի $c_1, ..., c_n$ դասերով՝ 

$$W = \left[
  \begin{array}{ccc}
    \horzbar & w^{T}_{c_1} & \horzbar \\
    \horzbar & w^{T}_{c_2} & \horzbar \\
             & \vdots    &          \\
    \horzbar & w^{T}_{c_{n+1}} & \horzbar
  \end{array}
\right]$$


$W_{\mathcal{T}_i}$-ով նշանակենք $(k+1) \times d$ չափանի մատրիցան, որի տողերը կազմած են $W$ մատրիցայի այն դասերի տողերից, որոնք ձևավորում են  $c_{i,1}, ..., c_{i,k+1}$ դասերից բաղկացած $\mathcal{T}_i$ առաջադրանքը՝

$$W_{T_i} = \left[
  \begin{array}{ccc}
    \horzbar & w^{T}_{c_{i,1}} & \horzbar \\
    \horzbar & w^{T}_{c_{i, 2}} & \horzbar \\
             & \vdots    &          \\
    \horzbar & w^{T}_{c_{{i, k+1}}} & \horzbar
  \end{array}
\right]$$



\begin{lemma}
\label{bs_inequality}
Դիցուք $\mathcal{T}_1, ..., \mathcal{T_N}$ առաջադրանքները միմյանցից անկախ ընտրված են $\mathcal{P}(\mathcal{T} \text{ } |\text{ }  |\mathcal{T}| = k +1)$ բաշխումից և $\mathcal{T} = \cup_{i=1}^N{\mathcal{T}_i}$, որի հզորությունը $n+1$ է։ Այդ դեպքում կամայական $f \in \mathcal{F}$ ներկայացման և  կամայական $(n+1) \times d$ չափանի $W$  մատրիցայի համար տեղի ունի հետևյալ անհավասարությունը՝
\begin{align}
\label{mean_task_ineq}
\frac{1}{N}\sum_{i=1}^N L(T_i, f, W_{T_i}) \leq  \frac{m_{max}}{(n+1) \rho_{min}}  L(T, f, W) 
\end{align}

\noindent որտեղ $\rho_{max}$ և $m_{max}$ մեծությունները սահմանված են համապատասխանաբար \ref{rhomin}-ի և \ref{mmax}-ի միջոցով։
\end{lemma}
\begin{proof}[Ապացույց]

Առաջին հերթին կարելի է հեշտությամբ համոզվել որ դիտարկվող հինջ և լոգիստիկ կորստի ֆունկցիաները բավարարում են հետևյալ հատկությանը՝
\begin{equation}
\label{prop_log_hinge}
\forall I \subseteq [t] \text{   }   l(\{v_i\}_{i\in I}) \leq l(\{v_i\}_{i \in [t]})
\end{equation}


Ենթադրենք որ $T$  առաջադրանքը կազմող դասերը ինչ-որ կերպ համարարակալված են և ֆիքսենք այդ  $c_1, ..., c_{n+1}$  համարակալումը։ Իսկ $\mathcal{T}_i$ առաջադրանքի դասերը համարակալված են հետևյալ կերպ՝ $c_{i,1}, ..., c_{i, {k+1}} $։ $I(\mathcal{T}_i) \subseteq T$  նշանակենք այն ինդեքսների բազմությունը, որոնց համապատասխան դասերը կան նաև $\mathcal{T}_i$ առաջադրանքում։  Այժմ $\mathcal{D}_{\mathcal{T}_i}(c_{ij})$ հավանականությունը գնահատենք $\mathcal{D}_{\mathcal{T}}(c_{ij})$ հավանականության միջոցով`

\begin{align*}
D_{\mathcal{T}_i}(c_{ij}) = \frac{D_\mathcal{T}(c_{ij})}{\sum_{j \in I(\mathcal{T}_i)}D_\mathcal{T}(c_j) } \leq \frac{D_{T}(c_{ij})}{|I(\mathcal{T}_i)|\rho_{min}} = \frac{D_{T}(c_{ij})}{(k+1)\rho_{min}}
\end{align*}

Համաձայն $L(T, f, W)$-ի կորստի ֆունկցիայի սահմանման՝
\begin{align*}
L(T, f, W) &= \E_{(x,c) \sim D_{T}} \left [               l \left( \left\{    (Wf(x))_c - (Wf(x))_{c'}     \right\}_{\substack{c \neq c' \\ c'\in T_i}} \right)          \right] = \\
&= \E_{c \sim D_{T_i}(c)}      \E_{x \sim D_{c}(x)}                      \left [               l \left( \left\{    (Wf(x))_c - (Wf(x))_{c'}     \right\}_{\substack{c \neq c' \\ c'\in T}} \right)          \right]
\end{align*}

\noindent Կատարենք նոր նշանակում՝
$$\phi(c, T, f, W) \defeq \E_{x \sim D_{c}(x)}                      \left [               l \left( \left\{    (Wf(x))_c - (Wf(x))_{c'}     \right\}_{\substack{c \neq c' \\ c'\in T}} \right)          \right]$$
Տեղադրելով վերոնշյալ նոր նշանակումը $L(T, f, W)$-ի մաթսպասման մեջ կունենանք՝
\begin{align*}
L(T, f, W) =  \E_{c \sim D_{T}(c)}  \left[ \phi(c, T, f, W) \right ]
\end{align*}

\noindent Համանման ձևով $\forall i \in [N]$ համար՝
\begin{align*}
L(T_i, f, W_{T_i}) =  \E_{c \sim D_{T}(c)}  \left[ \phi(c, T_i, f, W_{T_i}) \right ]
\end{align*}


Օգտվելով \ref{prop_log_hinge} անհվասարությունից՝  հեշտ է նկատել, որ $\forall i \in [N], \forall f \in \mathcal{F}, \forall W \in \mathbb{R}^{(n+1)\times d}$  և $\forall c \in T_i$ համար տեղի ունի՝ 
$$\phi(c, T_i, f, W_{T_i}) \leq \phi(c, T, f, W)$$


\noindent Միավորելով այս ամենը կունենանք՝
\begin{align*}
&\frac{1}{N}\sum_{i=1}^NL(\mathcal{T}_i, f, W_{\mathcal{T}_i})  = \frac{1}{N} \sum_{i=1}^N \E_{c \sim D_{\mathcal{T}_i}(c)} \left [\phi(c, \mathcal{T}_i, f, W_{\mathcal{T}_i}) \right] = \frac{1}{N}   \sum_{i=1}^N \sum_{j = 1}^{k+1}  D_{\mathcal{T}_i}(c_{ij}) \phi(c_{ij}, \mathcal{T}_i, f, W_{\mathcal{T}_i}) \leq \\
&\leq  \frac{1}{N}   \sum_{i=1}^N \sum_{j = 1}^{k+1}   \frac{D_{T}(c_{ij})}{(k+1)\rho_{min}} \phi(c_{ij}, \mathcal{T}_i, f, W_{\mathcal{T}_i}) \leq
\frac{1}{N(k+1)\rho_{min}}   \sum_{i=1}^N \sum_{j = 1}^{k+1}   D_{T}(c_{ij}) \phi(c_{ij}, \mathcal{T}, f, W) \leq \\
&\leq   \frac{1}{N(k+1)\rho_{min}}   \sum_{i=1}^{n+1}  m(c)D_{T}(c_{i}) \phi(c_{i}, \mathcal{T}, f, W) \leq 
\frac{m_{max}}{N(k+1)\rho_{min}}   L(T, f, W) \leq \\
&\leq \frac{m_{max}}{(n+1)\rho_{min}}   L(T, f, W) 
\end{align*}
Հետևաբար կամայական $f \in \mathcal{F}$ և կամայական $W \in \mathbb{R}^{(n+1)\times d}$ մատրիցայի համար տեղի ունի \ref{mean_task_ineq} անահվասարությունը և լեմմայի ապացույցը ավարտված է։

\end{proof}


















\begin{preposition}
\label{prep_vec_ineq}
Կամայական $v \in \mathbb{R}^d$ վեկտորի համար տեղի ունի հետևյալը՝
$$||v|| \leq \sqrt{2}\E_{\sigma \sim \{\pm1\}^d} \left| \sum_{i=1}^d \sigma_iv_i \right|$$
\end{preposition}
\begin{theorem}
\label{contr_theorem}
Դիցուք $\mathcal{X}$-ը որևէ բազմություն է և $(x_1, x_2, ..., x_n) \in X^N$։ Տրված է նաև $\mathcal{F}$ ֆունկցիաների բազմություն, որի կամայական $f \in \mathcal{F}$ ֆունկցիա $\mathcal{X}$ բազմությունը արտապատկերում է $\mathbb{R}^d$ էվկլիդյան տարածություն՝ $f:\mathcal{X} \rightarrow \mathbb{R}^d$։ Դիցուք $h_i$ ֆունկցիաներ ունենք որոնք $\mathbb{R}^d$ էվկլիդյան տարածությունը արտապատկերում են  իրական թվերի $\mathbb{R}$ տարածություն՝
$h_i:\mathbb{R}^d \rightarrow \mathbb{R}$, կամայական $i \in [n]$ համար։ Կենթադրենք, որ բոլոր $h_i$ ֆունկցիաները, ինչ-որ $L$ դրական հաստատունով Լիպշից հատկությամբ օժտված ֆունկցիաներ են։ Այդ դեպքում տեղի ունի հետևյալ անհավասարությունը՝
\begin{equation}
\label{contradiction_ineq}
\E_{\sigma \sim \{\pm 1\}^n}\left[\sup_{f \in \mathcal{F}}  \sum_{i=1}^n{\sigma_ih_i(f(x_i))}  \right]    \leq \sqrt{2}L \E_{\sigma \sim \{\pm1\}^{nd}} \left[  \sup_{f \in \mathcal{F}}  \sum_{i=1}^n\sum_{j=1}^d{\sigma_{ij}f_j(x_i)}   \right]
\end{equation}
\end{theorem}


\noindent
Թեորեմ \ref{contr_theorem}-ը կարելի է ընդհանրացնել  $h_i(v, y) \in \mathbb{R}$ ֆունկցիաների համար, որտեղ $v \in \mathbb{R}^d$, $y \in \mathcal{Y}$ և $h_i$ ֆունկցիաները ըստ $v$ փոփոխականի $L$ հաստատունով Լիպշից հատկությամբ օժտված ֆունկցիաներ են կամայական $y \in \mathcal{Y}$ համար։ 

\begin{theorem}
\label{contr_theorem}
Դիցուք $\mathcal{X}$-ը և $\mathcal{Y}$-ը որևէ բազմություններ են  և $(x_1, x_2, ..., x_n) \in X^N$։ Տրված է նաև $\mathcal{F}$ ֆունկցիաների բազմություն, որի կամայական $f \in \mathcal{F}$ ֆունկցիա $\mathcal{X}$ բազմությունը արտապատկերում է $\mathbb{R}^d$ էվկլիդյան տարածություն՝ $f:\mathcal{X} \rightarrow \mathbb{R}^d$։ Դիցուք $h_i$ ֆունկցիաներ ունենք՝ $$h_i:\mathbb{R}^d \times \mathcal{Y} \rightarrow \mathbb{R}$$ կամայական $i \in [n]$ համար։ Կենթադրենք, որ բոլոր $h_i(v, y)$ ֆունկցիաները, ինչ-որ $L$ դրական հաստատունով Լիպշից հատկությամբ օժտված ֆունկցիաներ են ըստ $v$-ի կամայական $y \in \mathcal{Y}$ համար։ Այդ դեպքում տեղի ունի հետևյալ անհավասարությունը՝
\begin{equation}
\label{contradiction_ineq}
\E_{\sigma \sim \{\pm 1\}^n}\left[\sup_{\substack{f \in \mathcal{F} \\ y \in \mathcal{Y}} }  \sum_{i=1}^n{\sigma_ih_i(f(x_i), y)}  \right]    \leq \sqrt{2}L \E_{\sigma \sim \{\pm1\}^{nd}} \left[  \sup_{f \in \mathcal{F}}  \sum_{i=1}^n\sum_{j=1}^d{\sigma_{ij}f_j(x_i)}   \right]
\end{equation}
\end{theorem}

\begin{proof}[Ապացույց]
Սկզբում ցույց տանք, որ բոլոր $i \in [n]$-երի համար և կամայական $g:\mathcal{F}\times \mathcal{Y} \rightarrow \mathbb{R}$  ֆունկցիոնալի համար տեղի ունի հետևյալ անհավասարությունը՝
\begin{equation}
\label{sup_ineq}
\E_{\epsilon \sim \{\pm1\}}\sup_{\substack{f \in \mathcal{F} \\ y \in \mathcal{Y}}}    {\epsilon h_i(f(x_i), y)} + g(f, y)   \leq    \sqrt{2}L 
\E_{\epsilon \sim \{\pm1\}^d}\sup_{\substack{f \in \mathcal{F} \\ y \in \mathcal{Y}}}      {   \sum_{j=1}^d    \epsilon_jf_j(x_i)} + g(f, y)
\end{equation}
Դիցուք $\delta > 0$ կամայական դրական թիվ է։ Այդ դեպքում համաձայն Ռադեմախերի փոփոխականի սահմանաման կունենանք՝
\begin{align*}
 2\E_{\epsilon \sim \{\pm1\}}\sup_{\substack{f \in \mathcal{F} \\ y \in \mathcal{Y}}}    {\epsilon h_i(f(x_i), y)} - \delta  
= \sup_{\substack{f, \bar{f} \in \mathcal{F}  \\ y \in \mathcal{Y}}}    {h_i(f(x_i), y)  + g(\bar{f}, y) - h_i(\bar{f}(x_i), y) + g(\bar{f}, y) - \delta}
\end{align*}

Օգտվելով սուպրեմումի սահմանումից՝ գոյություն ունեն $f*, \bar{f}^* \in \mathcal{F}$ ֆունկցիաներ, որ տեղի ունի հետևյալը՝
\begin{align*}
 &\sup_{\substack{f, \bar{f} \in \mathcal{F}  \\ y \in \mathcal{Y}}}    {h_i(f(x_i), y)  + g(\bar{f}, y) - h_i(\bar{f}(x_i), y) + g(\bar{f}, y) -\delta}  \leq \\
 &\leq \sup_{y \in \mathcal{Y}}   h_i(f^*(x_i), y) - h_i(\bar{f}^*(x_i), y) + g(f^*, y) + g(\bar{f}^*, y)
\end{align*}

Օգտագործելով $h_i$ ֆունկցաի Լիպշիցի հատկությամբ օժտված լինելը կունենանք՝ 

\begin{align*}
 &\sup_{y \in \mathcal{Y}}   h_i(f^*(x_i), y) - h_i(\bar{f}^*(x_i), y) + g(f^*, y) + g(\bar{f}^*, y) \leq \\
& \leq   {L||f^*(x_i) - \bar{f}^*(x_i)||} + \sup_{y \in \mathcal{Y}}  g(f^*, y) + g(\bar{f}^*, y)
\end{align*}

Պնդում \ref{prep_vec_ineq}-ը կիրառելով կստանանք՝
\begin{align*}
 &{L||f^*(x_i) - \bar{f}^*(x_i)||} + \sup_{y \in \mathcal{Y}}  g(f^*, y) + g(\bar{f}^*, y) \leq \\
& \leq  {\sqrt{2}L} \E_{\epsilon \sim \{\pm 1\}^d}    \left |  \sum_{j=1}^d \epsilon_j (f^*_j(x_i) - \bar{f}_j^*(x_i))  \right|                    + \sup_{y \in \mathcal{Y}}    g(f^*, y) + g(\bar{f}^*, y)  \leq \\
&\leq  {\sqrt{2}L} \E_{\epsilon \sim \{\pm 1\}^d}        \sup_{f, \bar{f} \in \mathcal{F}}      \left |  \sum_{j=1}^d \epsilon_j f_j(x_i) - \sum_{j=1}^d \epsilon_j\bar{f}_j(x_i)  \right|                    + \sup_{y \in \mathcal{Y}}    g(f, y) + g(\bar{f}, y) 
\end{align*}

Հեշտ է նկատել, որ կամայական ֆիքսված $\epsilon$-ի դեպքում 
$$\sup_{f, \bar{f} \in \mathcal{F}}      \left |  \sum_{j=1}^d \epsilon_j f_j(x_i) - \sum_{j=1}^d \epsilon_j\bar{f}_j(x_i)  \right|  = 
\sup_{f, \bar{f} \in \mathcal{F}}         \sum_{j=1}^d \epsilon_j f_j(x_i) - \sum_{j=1}^d \epsilon_j\bar{f}_j(x_i)    $$
և քանի որ $\sup_{y \in \mathcal{Y}}    g(f, y) + g(\bar{f}, y)$ ինվարիանտ է $f, \bar{f}$ ֆունկցիաների փոփոխման նկատմամբ, կունենանք՝ 

\begin{align*}
&  {\sqrt{2}L} \E_{\epsilon \sim \{\pm 1\}^d}    \left |  \sum_{j=1}^d \epsilon_j (f^*_j(x_i) - \bar{f}_j^*(x_i))  \right|                    + \sup_{y \in \mathcal{Y}}    g(f^*, y) + g(\bar{f}^*, y)  \leq \\
&\leq  {\sqrt{2}L} \E_{\epsilon \sim \{\pm 1\}^d}        \sup_{f, \bar{f} \in \mathcal{F}}        \sum_{j=1}^d \epsilon_j f_j(x_i) - \sum_{j=1}^d \epsilon_j\bar{f}_j(x_i)                      + \sup_{y \in \mathcal{Y}}    g(f, y) + g(\bar{f}, y)  = \\
&=  {\sqrt{2}L} \E_{\epsilon \sim \{\pm 1\}^d}        \sup_{f \in \mathcal{F}}        \sum_{j=1}^d \epsilon_j f_j(x_i)    + \sup_{y \in \mathcal{Y}}    g(f, y)    +  \E_{\epsilon \sim \{\pm 1\}^d}        \sup_{\bar{f} \in \mathcal{F}}   -  \sum_{j=1}^d \epsilon_j\bar{f}_j(x_i)                      + \sup_{y \in \mathcal{Y}}  g(\bar{f}, y) 
\end{align*}


Հաշվի առնելով Ռադեմախերի $\epsilon_j$ փոփոխականների սիմետրիկություը կստանանք՝

\begin{align*}
&{\sqrt{2}L} \E_{\epsilon \sim \{\pm 1\}^d}        \sup_{f \in \mathcal{F}}        \sum_{j=1}^d \epsilon_j f_j(x_i)    + \sup_{y \in \mathcal{Y}}    g(f, y)    +  \E_{\epsilon \sim \{\pm 1\}^d}        \sup_{\bar{f} \in \mathcal{F}}   -  \sum_{j=1}^d \epsilon_j\bar{f}_j(x_i)                      + \sup_{y \in \mathcal{Y}}  g(\bar{f}, y)  = \\
&= 2\left({\sqrt{2}L} \E_{\epsilon \sim \{\pm 1\}^d}        \sup_{f \in \mathcal{F}}        \sum_{j=1}^d \epsilon_j f_j(x_i)    + \sup_{y \in \mathcal{Y}}    g(f, y)  \right)    = \\
&=  2\left({\sqrt{2}L} \E_{\epsilon \sim \{\pm 1\}^d}        \sup_{\substack{f \in \mathcal{F}  \\ y \in \mathcal{Y}  }}     \sum_{j=1}^d \epsilon_j f_j(x_i)    +   g(f, y)  \right)    
\end{align*}

Այսպիսով կամայական $\delta > 0$ դրական թվի համար՝
$$\E_{\epsilon \sim \{\pm1\}}\sup_{\substack{f \in \mathcal{F} \\ y \in \mathcal{Y}}}    {\epsilon h_i(f(x_i), y)} - \delta  \leq {\sqrt{2}L} \E_{\epsilon \sim \{\pm 1\}^d}        \sup_{\substack{f \in \mathcal{F}  \\ y \in \mathcal{Y}  }}     \sum_{j=1}^d \epsilon_j f_j(x_i)    +   g(f, y)   $$

Քանի որ վերջինս տեղի ունի ցանկացած  $\delta$-ի համար, այստեղից անմիջապես հետևում է \ref{sup_ineq} անհավասարությունը։

\par Այժմ ինդուկցիայի միջոցով ցույց տանք,
որ  ցանկացած $m \in \{0, ..., n\}$ համար տեղի ունի հետևյալ անհավասարությունը։

\begin{align*}
&\E_{\epsilon \sim \{\pm 1\}^n}\left[\sup_{\substack{f \in \mathcal{F} \\ y \in \mathcal{Y}} }  \sum_{i=1}^n{\epsilon_ih_i(f(x_i), y)}  \right]    \leq \sqrt{2}L \E_{\sigma \sim \{\pm1\}^{md}} \left[  \sup_{f \in \mathcal{F}}  \sum_{i=1}^m\sum_{j=1}^d{\sigma_{ij}f_j(x_i)}   \right] +\\ &+\E_{\epsilon \sim \{\pm 1\}^{n-m}}\left[\sup_{\substack{f \in \mathcal{F} \\ y \in \mathcal{Y}} }  \sum_{i=m+1}^n{\epsilon_ih_i(f(x_i), y)}  \right]   
\end{align*}

\ref{contradiction_ineq} անհավասարությունը անմիջապես հետևում է՝ վերցնելով $m = n$։ Երբ $m=0$ անհավասարության երկու կողմերում նույն արտահայտությունն է գրված և հետևաբար տեղի ունի անհավասարությունը։ Կատարենք ինդուկցիոն ենթադրություն և համարենք անհավասարությունը տեղի ունի $(m-1)$-ի համար, որտեղ $m \leq n$։



\begin{align*}
&\E_{\epsilon \sim \{\pm 1\}^n}\left[\sup_{\substack{f \in \mathcal{F} \\ y \in \mathcal{Y}} }  \sum_{i=1}^n{\epsilon_ih_i(f(x_i), y)}  \right]   
 \leq \sqrt{2}L \E_{\sigma \sim \{\pm1\}^{(m-1)d}} \left[  \sup_{f \in \mathcal{F}}  \sum_{i=1}^{m-1}\sum_{j=1}^d{\sigma_{ij}f_j(x_i)}   \right] +\\ 
 &+\E_{\epsilon \sim \{\pm 1\}^{n-m+1}}\left[\sup_{\substack{f \in \mathcal{F} \\ y \in \mathcal{Y}} }  \sum_{i=m}^n{\epsilon_ih_i(f(x_i), y)}  \right] = \\  
 &= \E_{\substack{\epsilon \sim \{ \pm 1\}^{n-m}   \\ \sigma \sim \{ \pm 1\}^{(m-1)d} }}     \E_{\epsilon_m \sim \{ \pm1 \}} \left [          \sup_{\substack{f \in \mathcal{F} \\ y \in \mathcal{Y}}}     \left(\epsilon_m h_m(f(x_m), y)   +   \sqrt{2}L 				 \sum_{i=1}^{m-1}\sum_{j=1}^d{\sigma_{ij}f_j(x_i)}  + \sum_{i = m+1}^n \epsilon_ih_i(f(x_i), y)				\right )	\right ]
\end{align*}


Սահմանենք $$g(f, y) = \sqrt{2}L\sum_{i=1}^{m-1}\sum_{j=1}^d{\sigma_{ij}f_j(x_i)}  + \sum_{i = m+1}^n\epsilon_ih_i(f(x_i), y)$$
և տեղադրելով այն վերջինիս մեջ և օգտագործելով \ref{sup_ineq} անհավասարությունը կստանանք՝


\begin{align*}
 &\E_{\substack{\epsilon \sim \{ \pm 1\}^{n-m}   \\ \sigma \sim \{ \pm 1\}^{(m-1)d} }}     \E_{\epsilon_m \sim \{ \pm1 \}} \left [          \sup_{\substack{f \in \mathcal{F} \\ y \in \mathcal{Y}}}     \left(\epsilon_m h_m(f(x_m), y)   +   g(f, y)			\right )	\right ] \leq \\
 &\leq  \E_{\substack{\epsilon \sim \{ \pm 1\}^{n-m}   \\ \sigma \sim \{ \pm 1\}^{(m-1)d} }}     \E_{\sigma_m \sim \{ \pm1 \}^d} \left [          \sup_{\substack{f \in \mathcal{F} \\ y \in \mathcal{Y}}}     \left( \sum_{j = 1}^d  \sigma_{mj}f(x_m) +   g(f, y)			\right )	\right ] =\\
 &= \sqrt{2}L \E_{\sigma \sim \{\pm1\}^{md}} \left[  \sup_{f \in \mathcal{F}}  \sum_{i=1}^m\sum_{j=1}^d{\sigma_{ij}f_j(x_i)}   \right] + \E_{\epsilon \sim \{\pm 1\}^{n-m}}\left[\sup_{\substack{f \in \mathcal{F} \\ y \in \mathcal{Y}} }  \sum_{i=m+1}^n{\epsilon_ih_i(f(x_i), y)}  \right] 
\end{align*}


\end{proof}
  
Դիցուք ունենք միմյանցից անկախ $\mathcal{P}(\mathcal{T} \text{ } |\text{ }  |\mathcal{T}| = k +1)$ բաշխումից ընտրված $N$ հատ առաջադրանքներ՝ $\mathcal{T}_1, ..., \mathcal{T}_N$  և $\mathcal{T} = \cup_{i=1}^N{\mathcal{T}_i}$։ Միավորված առաջադրանքի հզորությունը $n$ է՝  $|\mathcal{T}| =n $։ Այժմ ենթադրենք միավորված $\mathcal{T}$ առաջադրանքի համար ունենք միմյանցին ակախ և $D_{\mathcal{T}}$ բաշխումից ընտրված $M$ օրինակներ՝
$$S = \{(x_1, y_1), (x_2, y_2), ..., (x_M, y_M) | x_i \in \mathcal{X}, y_i \in \mathcal{T} \text{ և } i \in [M] \}$$
$\mathcal{T}$   առաջադրանքի համար դիցուք $g(x) = Wf(x)$ գծային դասակարգչն է ըստ $f \in \mathcal{F}$ ներկայացման, որտեղ $W$-ն $(n+1) \times d$ չափանի մատրիցա է և $W \in \mathcal{V}$։ $g(x)$ դասակարգչի էմպիրիկ սխալանքը $S$ բազմության վրա սահմանենք հետևյալ կերպ՝
$$\hat{L}(\mathcal{T},f, W) = \frac{1}{M}\sum_{i=1}^Ml(\{(Wf(x_i))_{y_i} - (Wf(x_i))_{y_j}\}_{y_i \neq y_j})$$ 
Ալգորիթմը որով սովորելու ենք ներկայցման ֆունկցիա $\mathcal{F}$ դասից հետևյալն է՝
$$(\hat{f}, \hat{W}) = \argmin_{\substack{f \in \mathcal{F} \\ W \in \mathcal{V}}} \hat{L}(\mathcal{T},f, W)$$
որտեղ $\hat{f}$ փնտրվող ներկայացումն է։ Այսպիսով ալգորթմը ըստ $f$ ներկայացման և գծային դասկարգիչի $W$ մատրիցայի  մինիմիզացնում է $\mathcal{T}$ առաջադրանքի վերահսկիչ էմպիրիկ սխալանքը $S$ օրինակների բազմության վրա։

\begin{lemma}
\label{rad_lemma_main}
Դիցուք $\delta$-ն կամայական դրական թիվ է։ Այդ դեպքում առնվազն $1-\delta$ հավանականությամբ կամայական $f \in \mathcal{F}$ ներկայացման և կամայական $W \in \mathcal{V}$ մատրիցայի համար տեղի ունի հետևյալ անհավասարությունը՝
$$L(\mathcal{T}, \hat{f}, \hat{W}) \leq L(\mathcal{T}, f, W) + Gen_M$$,
\end{lemma}
\begin{proof}[Ապացույց]
 Սահմանենք $G$ ֆունկցիաների բազմությունը հետևյալ կերպ՝
$$G = \left \{ (x, y) \mapsto  g_{f, W}(x, y) = \frac{1}{B}l(\{[Wf(x)]_y - [Wf(x)]_{y'}\}_{y\neq y'}   ) | f \in \mathcal{F}, W \in \mathcal{V} \right \}$$
Վերցնենք $Z = \mathcal{X} \times \mathcal{T}$ և $S = \left \{z_i = (x_i, y_i) \right \}_{i=1}^M$, կիրառելով \ref{rad_comp_th} թեորեմը $G$ ֆունկցիաների բազմության համար կունենանք՝
\begin{align}
\label{rad_ineq_1}
\E[g(z)] \leq \frac{1}{m}\sum_{i=1}^mg(z_i) + \frac{2}{M}\E_{\sigma \sim \{\pm1\}^M} \sup_{\substack{ f  \in \mathcal{F}  \\ W \in \mathcal{V}}}\sum_{i=1}^M \sigma_ig_{f,W}(z_i) +    3\sqrt{\frac{\log \left( \frac{2}{\delta} \right)}{2m}}
\end{align}

Այժմ ցույց տանք, որ ցանկացած $W \in \mathcal{V}$  և $i \in [M]$ համար $h_i(f(x_i), W) = g_{f,W}(z_i)$ ֆունկցիան ըստ $f(x_i)$-ի ինչ-որ $L$ հաստատունով օժտված է Լիպշիցի հատկությամբ։ Ներմուծենք $\Phi_y(f(x), W)$ ֆունկցիան, այնպես որ $h_i = \frac{1}{B} l \circ \Phi_{y_i}$։ Ֆիքսենք որևէ $y \in \mathcal{T}$ դաս և մնացած $n$ դասերը համարակալենք  $\mathcal{T} \setminus \{y\} =\{y'_1, y'_2, ..., y'_n\} $։ $\Phi_y : \mathbb{R}^d \times \mathcal{V} \rightarrow \mathbb{R}^n$ որի տեսքը հետևյալն է՝
$$\Phi_y(x, W) = (w_yx-w_{y'_{i}}x)_{i \in [n]}$$

Ըստ $x$ փոփոխականի $\Phi_y$  ֆունկցիայի Յակոբյանը նշանակենք $J_{\Phi_y} $-ով։
$$J_{\Phi_y}  = 
 \begin{pmatrix}
 \frac{\partial \Phi_{y1}}{\partial x_1} & \frac{\partial \Phi_{y1}}{\partial x_2} & \cdots & \frac{\partial \Phi_{y1}}{\partial x_d} \\
  \frac{\partial \Phi_{y2}}{\partial x_1} & \frac{\partial \Phi_{y2}}{\partial x_2} & \cdots & \frac{\partial \Phi_{y2}}{\partial x_d} \\
  \vdots  & \vdots  & \ddots & \vdots  \\
  \frac{\partial \Phi_{yn}}{\partial x_1} & \frac{\partial \Phi_{yn}}{\partial x_2} & \cdots & \frac{\partial \Phi_{yn}}{\partial x_d} \\
 \end{pmatrix} =
 \begin{pmatrix}
 w_{y1} - w_{{y'}_11} & w_{y2} - w_{{y'}_12} & \cdots & w_{yd} - w_{{y'}_1d} \\
   w_{y1} - w_{{y'}_21} & w_{y2} - w_{{y'}_22} & \cdots & w_{yd} - w_{{y'}_2d} \\
  \vdots  & \vdots  & \ddots & \vdots  \\
  w_{y1} - w_{{y'}_n1} & w_{y2} - w_{{y'}_n2} & \cdots & w_{yd} - w_{{y'}_nd} \\
 \end{pmatrix}
 $$

\begin{align*}
||J_{\Phi_y}||_F &= \sqrt{\sum_{i = 1}^n \sum_{k = 1}^d    \left(w_{yk} - w_{y'_ik}\right)^2} = \sqrt{n\sum_{k =1}^d w^2_{yk}   - 2 \sum_{i=1}^n \sum_{k=1}^d w_{yk} w_{y'_ik}  + \sum_{i=1}^n \sum_{k=1}^d  w^2_{y'_ik} } \\
&\leq  \sqrt{nQ^2+2nQ^2+nQ^2} = 2Q\sqrt{n}
\end{align*}
Այսպիսով $\Phi_y$ ֆունկցիան  ըստ $x$-ի փոփոխականի $2Q\sqrt{n}$ հաստատունով Լիպշիցի հատկությամբ օժտված ֆունկցիա է և քանի որ $l$-ը $\eta$ հաստատունով Լիպշիցի հատկությամբ էր օժտված, ապա կունենաք որ $h_i$ ֆունկցիաները բոլոր $i \in [M]$ համար $\frac{2 \eta Q\sqrt{n}}{B}$ հաստատունով ըստ $f(x_i)$-ի Լիպշիցի հատկություն ունի ցանկացած $W \in \mathcal{V}$ մատրիցայի համար։


Նկատենք  որ թեորեմ  \ref{contr_theorem}-ի պայմանները բավարարված են և կիրառելով այն կունենանք՝
$$\E_{\sigma \sim \{\pm1\}^M} \sup_{\substack{ f  \in \mathcal{F}  \\ W \in \mathcal{V}}}\sum_{i=1}^M \sigma_ig_{f,W}(z_i) \leq  \frac{2\sqrt{2} \eta Q\sqrt{n}}{B}        \E_{\sigma \sim \{\pm1\}^{Md}} \sup_{\substack{ f  \in \mathcal{F}  }}\sum_{i=1}^M \sum_{j=1}^d \sigma_{ij}f(x_i)$$


Վերջինս տեղադրենք \ref{rad_ineq_1}-ի մեջ և անհավասարության երկու կողմը բազմապատկենք $B$-ով, ցանկացած $g \in G$ համար կունենանք՝

\begin{align*}
\E[Bg(z)] \leq \frac{1}{M}\sum_{i=1}^MBg(z_i) + \frac{4\sqrt{2} \eta Q\sqrt{n}}{M} \E_{\sigma \sim \{\pm1\}^{Md}} \sup_{\substack{ f  \in \mathcal{F}  }}\sum_{i=1}^M \sum_{j=1}^d \sigma_{ij}f(x_i)+    3B\sqrt{\frac{\log \left( \frac{2}{\delta} \right)}{2M}}
\end{align*}
որտեղից էլ՝
\begin{align}
\label{ineq_rad_2}
L(\mathcal{T}, f, W) \leq \hat{L}(\mathcal{T}, f, W) + \frac{4\sqrt{2} \eta Q\sqrt{n}}{M} \E_{\sigma \sim \{\pm1\}^{Md}} \sup_{\substack{ f  \in \mathcal{F}  }}\sum_{i=1}^M \sum_{j=1}^d \sigma_{ij}f(x_i)+    3B\sqrt{\frac{\log \left( \frac{2}{\delta} \right)}{2M}}
\end{align}
որը տեղի ունի $\forall f \in \mathcal{ F}$ և $\forall W \in \mathcal{V}$։ Քանի որ  \ref{ineq_rad_2}-ը տեղի ունի $\forall f \in \mathcal{ F}$ և $\forall W \in \mathcal{V}$, հետևաբար այն տեղի ունի նաև $\hat{f}$ և $\hat{W}$-ի համար՝
\begin{align}
\label{ineq_rad_3}
L(\mathcal{T}, \hat{f}, \hat{W}) \leq \hat{L}(\mathcal{T}, \hat{f}, \hat{W}) + \frac{4\sqrt{2} \eta Q\sqrt{n}}{M} \E_{\sigma \sim \{\pm1\}^{Md}} \sup_{\substack{ f  \in \mathcal{F}  }}\sum_{i=1}^M \sum_{j=1}^d \sigma_{ij}f(x_i)+    3B\sqrt{\frac{\log \left( \frac{2}{\delta} \right)}{2M}}
\end{align}

Դիցուք $f^*, W^* = \argmin_{f \in \mathcal{F}, W \in \mathcal{V}}L(\mathcal{T}, f, W)$։ Կիրառելով Հոֆդինգի անհավասարությունը առնվազն $1- \frac{\delta}{2}$ հավանականությամբ տեղի ունի հետևյալը՝
$$\hat{L}(\mathcal{T}, f,^* W^*) \leq L(\mathcal{T}, f,^* W^*) + B\sqrt{\frac{\log\frac{2}{\delta}}{2M}}$$
Հաշվի առնելով որ $\hat{L}(\mathcal{T}, \hat{f}, \hat{W})  \leq \hat{L}(\mathcal{T}, f,^* W^*) $՝ \ref{ineq_rad_3} անհավասարությունը կարող ենք գրել հետևյալ կերպ՝
\begin{align}
\label{ineq_rad_4}
L(\mathcal{T}, \hat{f}, \hat{W}) \leq  L(\mathcal{T}, f,^* W^*) + \frac{4\sqrt{2} \eta Q\sqrt{n}}{M} \E_{\sigma \sim \{\pm1\}^{Md}} \sup_{\substack{ f  \in \mathcal{F}  }}\sum_{i=1}^M \sum_{j=1}^d \sigma_{ij}f(x_i)+    4B\sqrt{\frac{\log \left( \frac{2}{\delta} \right)}{2M}}
\end{align}

\noindent Հեշտ է նկատել որ \ref{ineq_rad_4} տեղի ունի  $\forall f \in \mathcal{ F}$ և $\forall W \in \mathcal{V}$ համար՝
\begin{align}
\label{ineq_rad_5}
L(\mathcal{T}, \hat{f}, \hat{W}) \leq  L(\mathcal{T}, f, W) + \frac{4\sqrt{2} \eta Q\sqrt{n}}{M} \E_{\sigma \sim \{\pm1\}^{Md}} \sup_{\substack{ f  \in \mathcal{F}  }}\sum_{i=1}^M \sum_{j=1}^d \sigma_{ij}f(x_i)+    4B\sqrt{\frac{\log \left( \frac{2}{\delta} \right)}{2M}}
\end{align}

Կիրառելով պատահույթների միավորման բանաձևը \ref{ineq_rad_5} տեղի ունի առնվազն $1-\delta$ հավանականությամբ և լեմման ապացուցված է։

\end{proof}

\begin{theorem}
Դիցուք $\delta$ կամայական դրական թիվ է, այդ դեպքում առնվազն $1-\delta$ հավանականությամբ տեղի ունի հետևյալ անհավասարությունը՝ 
$$L(\hat{f}) \leq L(T, f, W) + Gen_{M, n} \text{  } \forall f \in \mathcal{F} \text{ և } \forall{W} \in \mathcal{V}$$
Որտեղ $M$ ուսուցման օրինակների քանակն է, իսկ $N$-ը առաջադրանքների քանակը:
\end{theorem}

\begin{proof}[Ապացույց] 
Դիցուք ունենք $N$ հատ միմյանցից անկախ $\mathcal{T}_1,\mathcal{T}_2, ...,\mathcal{T}_N$  առաջադրանքները ընտրված 
$$\mathcal{P}(\mathcal{T} \text{ } |\text{ }  |\mathcal{T}| = k +1)$$
բաշխումից և $\mathcal{T} = \cup_{i=1}^N\mathcal{T}_i$, որի հզորությունը հավասար է $n+1$-ի: Ինչպես նաև ունենք $M$ հատ օրինակների ուսուցման բազմությունը ընտրված $\mathcal{D_\mathcal{T}}$ բաշխումից՝
$$S = \{(x_1, y_1), (x_2, y_2), ..., (x_M, y_M) | x_i \in \mathcal{X}, y_i \in \mathcal{T} \text{ }\forall i \in[M]\}$$

\noindent Այժմ ֆիքսենք ցանկացած $\delta$ դրական թիվ: Օգտվելով լեմմա \ref{rad_lemma_main}-ից առնվազն $1-\frac{\delta}{2}$ հավանականությամբ տեղի ունի հետևյալ անհվասարությունը $f \in \mathcal{F}$-ի և $W \in \mathcal{V}$-ի համար՝

\begin{align}
\label{ineq_rad_6}
L(\mathcal{T}, \hat{f}, \hat{W}) \leq  L(\mathcal{T}, f, W) + \frac{4\sqrt{2} \eta Q\sqrt{n}}{M} \E_{\sigma \sim \{\pm1\}^{Md}} \sup_{\substack{ f  \in \mathcal{F}  }}\sum_{i=1}^M \sum_{j=1}^d \sigma_{ij}f(x_i)+    4B\sqrt{\frac{\log \left( \frac{4}{\delta} \right)}{2M}}
\end{align}

\noindent Ըստ \ref{prop_log_hinge} հատկության $\forall i \in [N]$ համար, քանի որ $T_i \subseteq T$, տեղի ունի հետևյալը՝
\begin{align}
\label{prop_ineq_1}
L(\mathcal{T}_i, \hat{f}, \hat{W}_{\mathcal{T}_i}) \leq L(\mathcal{T}, \hat{f}, \hat{W} )
\end{align}
 որտեղ $\hat{W}_{\mathcal{T}_i}$ կազմված է $T_i$ առաջադրանքում մասնակցող դասերին համապատասխան  $\hat{W}$ մատրիցայի տողերից: Համաձայն $L(\mathcal{T}_i, \hat{f})$ սահմանման ակնհայտ է, որ՝
\begin{align}
\label{prop_ineq_2}
L(\mathcal{T}_i, \hat{f}) \leq L(\mathcal{T}_i, \hat{f}, \hat{W}_{\mathcal{T}_i})
\end{align}

Միավորելով \ref{ineq_rad_6}, \ref{prop_ineq_1}  և \ref{prop_ineq_2}-ը $\forall i \in [N]$՝ կունենանք՝


\begin{align}
\label{ineq_rad_7}
L(\mathcal{T}_i, \hat{f}) \leq  L(\mathcal{T}, f, W) + \frac{4\sqrt{2} \eta Q\sqrt{n}}{M} \E_{\sigma \sim \{\pm1\}^{Md}} \sup_{\substack{ f  \in \mathcal{F}  }}\sum_{i=1}^M \sum_{j=1}^d \sigma_{ij}f(x_i)+    4B\sqrt{\frac{\log \left( \frac{4}{\delta} \right)}{2M}}
\end{align}

Այժմ \ref{ineq_rad_7} անհավասարության աջ և ձախ մասերը ըստ բոլոր $i $-երի գումարենք և անհավասարության երկու մասերը բաժանենք $N$-ի կստանանք՝

\begin{align}
\label{ineq_rad_8}
\frac{1}{N}\sum_{i=1}^NL(\mathcal{T}_i, \hat{f}) \leq  L(\mathcal{T}, f, W) + \frac{4\sqrt{2} \eta Q\sqrt{n}}{M} \E_{\sigma \sim \{\pm1\}^{Md}} \sup_{\substack{ f  \in \mathcal{F}  }}\sum_{i=1}^M \sum_{j=1}^d \sigma_{ij}f(x_i)+    4B\sqrt{\frac{\log \left( \frac{4}{\delta} \right)}{2M}}
\end{align}

Նկատենք որ \ref{ineq_rad_8} անհասարության ձախ մասը $\mathcal{T}_1,\mathcal{T}_2, ...,\mathcal{T}_N$ առաջադրանքների Էմպիրիկ վերահսկիչ միջին կորուստն է $\hat{f}$ ներկայացման համար՝


\begin{align}
\label{emp_eq}
\hat{L}(\hat{f}) = \frac{1}{N}\sum_{i=1}^NL(\mathcal{T}_i, \hat{f})
\end{align}

\ref{emp_eq}-ը տեղադրենք \ref{ineq_rad_8}-ի մեջ՝
 
\begin{align}
\label{ineq_rad_9}
\hat{L}(\hat{f}) \leq  L(\mathcal{T}, f, W) + \frac{4\sqrt{2} \eta Q\sqrt{n}}{M} \E_{\sigma \sim \{\pm1\}^{Md}} \sup_{\substack{ f  \in \mathcal{F}  }}\sum_{i=1}^M \sum_{j=1}^d \sigma_{ij}f(x_i)+    4B\sqrt{\frac{\log \left( \frac{4}{\delta} \right)}{2M}}
\end{align}

Համաձայն լեմմա \ref{task_conc_lemm}-ի առնվազն $1-\frac{\delta}{2}$ հավանականությամբ $\hat{f}$ ներկայացման համար տեղի ունի հետևյալը՝


\begin{equation}
\label{lemm_ineq}
\hat{L}(\hat{f}) \geq L(\hat{f}) - B\sqrt{\frac{\left(k+1\right)\log \left(\frac{2}{\delta}\right) }{2(n+1)}}
\end{equation}



Կիրառելով պատահույթների միավորման բանաձևը՝ \ref{ineq_rad_9}, \ref{lemm_ineq}-ից  հետևում է, որ առնվազն $1-\delta$ հավանականությամբ՝

 
\begin{align*}
\label{ineq_rad_10}
L(\hat{f}) \leq  L(\mathcal{T}, f, W) + \frac{4\sqrt{2} \eta Q\sqrt{n}}{M} \E_{\sigma \sim \{\pm1\}^{Md}} \sup_{\substack{ f  \in \mathcal{F}  }}\sum_{i=1}^M \sum_{j=1}^d \sigma_{ij}f(x_i)+    4B\sqrt{\frac{\log \left( \frac{4}{\delta} \right)}{2M}} + B\sqrt{\frac{\left(k+1\right)\log \left(\frac{2}{\delta}\right) }{2(n+1)}}
\end{align*}

 \end{proof}

\pagebreak
\medskip
\begin{thebibliography}{9}
\phantomsection
\addcontentsline{toc}{section}{Գրականություն} 

\bibitem{bib_item_1}
S. Pan and Q. Yang.
\textit{A survey on transfer learning,}  Knowledge and Data Engineering, IEEE Transactions on,
22(10):1345–1359, 2010.

\bibitem{bib_item_2}
K. Weiss, T. M. Khoshgoftaar, and D. Wang. \textit{ A survey of transfer learning,} 
Journal of Big Data, vol. 9, no. 3, 2016.

\bibitem{bib_item_3}
C. Tan, F. Sun, T. Kong, W. Zhang, C. Yang, and C. Liu.
\textit{A survey on deep transfer learning,}  https://arxiv.org/abs/1808.01974, 2018.



\bibitem{bib_item_6}
Devlin, J., Chang, M.-W., Lee, K., and Toutanova. \textit{K. Bert: Pretraining of deep bidirectional transformers for language understanding,} arXiv preprint arXiv:1810.04805, 2018.


\bibitem{bib_item_7}
Logeswaran, L. and Lee, H. \textit{An efficient framework for
learning sentence representations,}  In Proceedings of the
International Conference on Learning Representations,
2018.


\bibitem{bib_item_4}
K. Simonyan and A. Zisserman. \textit{ Very deep convolutional networks
for large-scale image recognition,} In ICLR, 2015

\bibitem{bib_item_5}
K. He, X. Zhang, S. Ren, and J. Sun. \textit{Deep
residual learning for image recognition,} In
Proceedings of CVPR, pages 770–778, 2016.
arxiv.org/abs/1512.03385

\bibitem{bib_item_8}
A. Razavian, H. Azizpour, J. Sullivan, and S. Carlsson. \textit{ CNN Features off-the-shelf: an Astounding Baseline
for Recognition,} CoRR, abs/1403.6382, 2014.

\bibitem{bib_item_9}
M.  Mohri, A. Rostamizadeh and A. Talwalkar.  \textit{Foundations of Machine Learning}, MIT press, 2018.

\bibitem{bib_item_10}
Shai Shalev-Shwartz and Shai Ben-David. \textit{Understanding Machine Learning: From Theory to Algorithms,} Cambridge University Press, New York, NY, USA, 2014.

\bibitem{bib_item_11}
A. Maurer. \textit{A vector-contraction inequality for rademacher
complexities}, In International Conference on Algorithmic Learning Theory, pp. 3–17. Springer, 2016.

\end{thebibliography}

\end{document}
