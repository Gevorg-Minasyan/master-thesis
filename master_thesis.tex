\documentclass[11pt]{article}

\usepackage[top=2cm,bottom=3cm,left=3cm,right=2cm]{geometry}
\usepackage{amsmath}


\DeclareMathOperator*{\argmax}{argmax}
\DeclareMathOperator*{\argmin}{argmin}
\DeclareMathOperator*{\E}{\mathbb{E}}

\usepackage{amsfonts}
\usepackage{amsthm}
\usepackage{fontspec}
\usepackage{hyperref}
\usepackage[thinc]{esdiff}
\usepackage{cite}
\hypersetup{
    colorlinks=true,
    linkcolor=blue,
    filecolor=magenta,      
    urlcolor=cyan,
    citecolor=cyan
}


% this package for increase or decrease vertical space between section chapter paragraph etc.
\usepackage{titlesec}

%tikz package for drawing graphs%
\usepackage{tkz-berge}
\usepackage{tikz}
\usetikzlibrary{decorations.markings}
\usetikzlibrary{positioning,chains,fit,shapes,calc}
\renewcommand{\figurename}{Նկ.}
% the figure insert specific place with [H] param
\usepackage{float}


% increase distance between two lines%
\renewcommand{\baselinestretch}{1.5}

% increase vertical space between section subsection%
\titlespacing*{\section}
{0pt}{8ex plus 1ex minus .2ex}{10ex plus .2ex}
\titlespacing*{\subsection}
{0pt}{5ex plus 1ex minus .2ex}{3ex plus .2ex}


\setmainfont[
BoldItalicFont=arnamu_italic_bold.ttf,
BoldFont      =arnamu_bold.ttf,
ItalicFont    =arnamu_italic.ttf]{arnamu.ttf}

\renewcommand{\contentsname}{Բովանդակություն}
\renewcommand{\refname}{Գրականություն}

\renewcommand{\tablename}{Աղյուսակ}
%\usepackage{mathtools}
%\newcommand\defeq{\stackrel{\mathclap{\normalfont\mbox{def}}}{=}}
\newcommand\defeq{\mathrel{\overset{\makebox[0pt]{\mbox{\normalfont\tiny\sffamily def}}}{=}}}

\sloppy
\begin{document}

\newtheorem{theorem}{Թեորեմ}
\newtheorem{lemma}{Լեմմա}
\newtheorem{corollary}{Հետևանք}
\newtheorem{preposition}{Պնդում}
\newtheorem{defination}{Սահմանում}


\theoremstyle{definition} %After this line new defined commands by \newthorem will be non italic.%
\newtheorem{innercustomcase}{Դեպք}
\newenvironment{customcase}[1]
  {\renewcommand\theinnercustomcase{#1}\innercustomcase}
  {\endinnercustomcase}
\newtheorem{case}{Դեպք}

\raggedbottom


\begin{titlepage}

\begin{center}
\large  Երևանի Պետական Համալսարան\\
\large Ինֆորմատիկայի և Կիրառական Մաթեմատիկայի Ֆակուլտետ\\
 Թվային անալիզի և մաթեմատիկական մոդելավորման ամբիոն\\
 \vspace{2mm}
\hrule width \hsize height 2pt  \kern 1mm \hrule width \hsize height 2pt 
\vspace{50mm}
\textbf{\huge Մագիստրոսական Թեզ\\}\noindent \newline \newline
\textbf{\large Թեմա՝}	\hspace{3mm} Տրանսֆերային ուսուցման որոշակի մեթոդի ընդհանրացման \\ սխալանքի գնահատման մասին
\end{center}
\vspace{25mm}
\begin{flushright}
\textbf{\large Ուսանող՝ \hspace{18mm}}			Մինասյան Գևորգ \\
\vspace{4mm}
\textbf{\large Ղեկավար՝ \hspace{2mm}}			ֆիզ. մաթ. գիտ. թեկնածու \\  Հ.Է. Դանոյան
\end{flushright}
\vspace{56mm}
\centering{\large Երևան -- 2019}
\end{titlepage}
\pdfbookmark{Բովանդակություն}{contents}
\tableofcontents
\newpage





\section*{\hfill 
 \center{Նշանակումներ և սահմանումներ}
 \hfill} \noindent
\phantomsection
\addcontentsline{toc}{section}{Նշանակումներ և սահմանումներ}
 
 $\mathcal{X}$-ով նշանակենք բոլոր հնարավոր տվյալների օրինակները, իսկ $\mathcal{C}$-ով նշանակենք բոլոր պիտակների կամ դասերի բազմությունը։ Յուրաքանչյուր $c \in \mathcal{C}$ դասին համպատասխանում է $\mathcal{X}$ բազմության վրա որոշված ինչ-որ $\mathcal{D}_c(x)$ բաշխում, այն ցույց է տալիս, թե $x$ օրինակը ինչքանով է $c$ դասին համապատասխան։ Ուսուցումը կատարվում է $\mathcal{F}$ ներկայացումների ֆունկցիաների դասի վրա։ $\forall f \in \mathcal{F}$  ֆունկցիա $\mathcal{X}$ տվյաների բազմությունը արտապատկերում $d$-չափանի էվկլիդյան $\mathcal{R}^d$տարծություն՝ $f:\mathcal{X}\rightarrow\mathcal{R}^d$, բացի այդ կդիտարկենք միայն սահմանափակ ֆունկցիաները՝
 $$||f(x)|| \leq R \text{    } \forall x \in \mathcal{X} \text{ և } R > 0։$$ 


%\noindent Նաև կենթադրենք, որ դասերի վրա կա ինչ-որ $\rho$ բաշխում, որը բնութագրում է, թե ինչպես են այդ դասերը հանդիպում չպիտակավորված տվյալներում:

\subsection*{Վերահսկվող առաջադրանքներ}

\par Այժմ կնկարագրենք այն առաջադրանքները, որոնց միջոցով փորձարկվելու է ներկայացումների $f$ ֆուկցիան: $k+1$ դասերից բաղկացած $\mathcal{T}$ վերահսկվող առաջադրանքը, բաղկացած է $$\{c_1, ..., c_{k+1}\} \subseteq \mathcal{C}$$
միմյանցից տարբեր դասերից: Կենթադրենք որ վերահսկվող առաջադրանքները ունեն $\mathcal{P}(\mathcal{T})$ բաշխում, որը բնութագրում է այդ առաջադրանքը դիտարկվելու հավանականությունը: $k+1$ դասերից բաղկացած վերահսկվող առաջադրանքների բաշխումը հետևյալն է՝ $$\mathcal{P}(\mathcal{T} \text{ } |\text{ }  |\mathcal{T}| = k +1)$$ Պիտակավորված տվյալների բազմությունը $\mathcal{T}$ առաջադրանքի համար բաղկացած է $m$ հատ միմյանցից անկախ և միևնույն բաշխումից ընտրրված օրինակներից: Այդ օրինակները ընտրվում են ստորև նկարագրված պրոցեսով:

\textit{$c \in \{c_1, ..., c_{k+1}\} $   դասը ընտրվում է ըստ $\mathcal{D}_{\mathcal{T}}$ բաշխման, որից հետո $x$ օրինակը ընտրվում է $\mathcal{D}_c$ բաշխումից: Դրանք միասին ձևավորում են պիտակավորված $(x, c)$ զույգը, որը ունի հետևյալ բաշխումը՝
$$\mathcal{D}_{\mathcal{T}} (x, c) = \mathcal{D}_{c}(x)\mathcal{D}_{\mathcal{T}}(c):$$}

\subsection*{Վերահսկվող ներկայացումների գնահատման չափը}
\par $f$ ներկայացումների ֆունկցիաի որակի գնահատումը կատարվում է $\mathcal{T}$ բազմադաս դասակարգման առաջադրանքի միջոցով՝ օգտագործելով գծային դասակարգիչ։  Ֆիքսենք $\mathcal{T} = \{c_1, ..., c_{k+1}\}$ առաջադրանքը։ $\mathcal{T}$ առաջադրանքի բազմադաս դասակարգիչը ֆուկցիա է՝ $g:\mathcal{X} \rightarrow \mathcal{R}^{k+1}$, որի արժեքի կորդինատները ինդեքսավորված են $\mathcal{T}$ առաջադրանքի դասերով։ $(x, y) \in \mathcal{X} \times \mathcal{T}$ կետում $g$ դասակարգիչով պայմանավորված կորուստը սահմանենք հետևյալ կերպ՝
$$l(\{ g(x)_y-g(x)_{y'}\}_{y \neq y'}  ),$$
որը ֆունկցիա կախված $k$ չափանի վեկտորից,  այն ստացվում է $k+1$ չափանի  $g(x)$ վեկտորի կորդինատների տարբերությունից, բացի այդ $\{ g(x)_y-g(x)_{y'}\}_{y \neq y'}$ վեկտորի կոմպոնենտները կամայական հերթականությամբ կարելի է համարակալել և $l$-ի արժեքը կախված չէ վեկտորի կոմպոնենտների համարակալման հերթականությունից։  Պրակտիկայում մեծ կիրառություն ունեցող երկու կորուստի ֆունկցիաներ ենք դիտարկելու աշխատանքում՝ ստանդարտ հինջ կորստի ֆունկցիան որը սահմանվում է հետևյալ կերպ՝
$$l(v) = \max\{0, 1+\max_{i}\{-v_i\}\} $$ և լոգիստիկ կորստի ֆունկցիան՝
$$l(v) = \log_2(1+\sum_{i}{e^{-v_i}}),$$
որտեղ $v \in \mathcal{R}^k$։ $\mathcal{T}$ առաջադրանքի համար $g$ դասակարգիչի կորուստը հետևյալն է՝
$$L(\mathcal{T}, g) \defeq \E_{(x, c) \sim \mathcal{D}_{\mathcal{T}}} \left [ l(\{ g(x)_c-g(x)_{c'}\}_{c \neq c'}  ) \right ]$$

$f$ ներկայացումների ֆունկցիան օգտագործելու նպատակով,  $g(x) = Wf(x)$ տեսքի դասակարգիչներն ենք դիտարկելու, որտեղ $W \in \mathcal{R}^{(k+1)\times d }$, որը ունի սահմանափակ նորմ՝ $||W|| \leq Q \text{ և }Q >0։$
$\mathcal{W}$-ով նշանակենք սահմանափակ նորմ ունեցող մատրիցաների բազմությունը՝
$$\mathcal{V} = \{W: ||W|| \leq Q \text{ և } Q > 0\}$$
 $\mathcal{T}$ առաջադրանքի համար $g(x) = Wf(x)$ ներկայացումից կախված գծային դասակարգչի կորստի ֆունկցիան հետևյալն է՝
$$L(\mathcal{T}, f, W) \defeq \E_{(x, c) \sim \mathcal{D}_{\mathcal{T}}} \left [ l(\{ Wf(x)_c-Wf(x)_{c'}\}_{c \neq c'}  ) \right ]$$
 
 Ֆիքսելով որևէ $f$ ներկայացում կարելի լավագույն $W$ գտնել, այնպես որ $f$-ից կախված գծային դասակարգչի կորուստը լինի ամենափոքրը, ուստի $f$ ներկայացման վերահսկիչ կորուստը $\mathcal{T}$ առաջադրանքի համար կսահմանենք, այն կորուստը, երբ լավագույն $W$ ենք ընտրել $f$-ի համար՝
 $$L(\mathcal{T}, f) \defeq \inf_{W \in \mathcal{V}} L(\mathcal{T}, f, W)$$


\begin{defination}[վերահսկիչ միջին կորուստ]
$k+1$ դասերից բաղկացած առաջադրանքների վերահսկիչ միջին կորուստը $f$ ներկայացման համար սահմանվում է որպես՝ 
$$L(f) \defeq \E_{\mathcal{T} \sim \mathcal{P}} \left [L (\mathcal{T}, f) \text{ } | \text{ } |\mathcal{T}| = k+1\right]$$
\end{defination}

\begin{defination}[էմպիրիկ վերահսկիչ միջին կորուստ]
Դիցուք ունենք միմյանցից անկախ $\mathcal{P}(\mathcal{T} \text{ } |\text{ }  |\mathcal{T}| = k +1)$ բաշխումից ընտրված $N$ հատ առաջադրանքներ՝ $\mathcal{T}_1, ..., \mathcal{T}_N$:
էմպիրիկ վերահսկիչ միջին կորուստը $f$ ներկայացման համար հետևյալն է՝ 
$$\hat{L}(f) \defeq \frac{1}{N}\sum_{i=1}^{N}L (\mathcal{T}_i, f)$$
\end{defination}
\pagebreak
\section*{\hfill Օժանդակ արդյունքներ \hfill} \noindent

\phantomsection
\addcontentsline{toc}{section}{Օժանդակ արդյունքներ}


\begin{lemma}[Հոֆդինգի անհավասարություն]
\label{hofding_inq}
Դիցուք $Z_1, ..., Z_m$ անկախ և միևնույն բաշխման պատահական մեծություններ են և $\bar{Z} = \frac{1}{m}\sum_{i=1}^m{Z_i}$: Ենթադրենք $\E[\bar{Z}] = \mu$ և յուրաքանչյուր $i$-ի համար $\mathbb{P}[a \leq Z_i \leq b] = 1$: Այդ դեպքում ցանկացած $\epsilon > 0$ թվի համար տեղի ունի հետևյալը՝
$$\mathbb{P}\left[ \frac{1}{m}\sum_{i=1}^m{Z_i}-\mu > \epsilon \right] \leq e^{\frac{-2m\epsilon^2}{(b-a)^2}}$$ 
և
$$\mathbb{P}\left[ \frac{1}{m}\sum_{i=1}^m{Z_i}-\mu < -\epsilon \right] \leq e^{\frac{-2m\epsilon^2}{(b-a)^2}}$$ 
\end{lemma}

\begin{lemma}
\label{task_conc_lemm}
Դիցուք ունենք միմյանցից անկախ $\mathcal{P}(\mathcal{T} \text{ } |\text{ }  |\mathcal{T}| = k +1)$ բաշխումից ընտրված $N$ հատ առաջադրանքներ՝ $\mathcal{T}_1, ..., \mathcal{T}_N$ և ֆիքսենք կամայական $f \in \mathcal{F}$ ներկայացում։ $\hat{L}(f)$ էմպիրիկ վերահսկիչ միջին կորուստն է $f$ ներկայացման համար,  իսկ $L(f)$-ը վերահսկիչ միջին կորուստը և դիցուք $|\cup_{i=1}^N{T_i}| = n$։
Այդ դեպքում առնվազն $1-\delta$ հավանականությամբ տեղի ունի հետևյալ անհավասարությունը։
\begin{equation}
\hat{L}(f) \geq L(f) - B\sqrt{\frac{\left(k+1\right)\log \left(\frac{1}{\delta}\right) }{2n}}
\end{equation}
որտեղ $B$ ինչ-որ դրական հաստատուն է։
\end{lemma}
\begin{proof}[Ապացույց]
Օգտվելով $L(\mathcal{T}_i, f)$ սահմանումից և օգտագործելով $f$-ի սահմանափակությունը հեշտ է համոզվել որ գոյություն ունի $B$ դրական թիվ այնպես որ կամայական $i \in [N]$ տեղի ունի հետևալը՝
$$0 \leq L(T_i,f) \leq B$$
Այժմ նկատենք որ \hyperref [hofding_inq]{Հոֆդինգի լեմմայի} պայմանները բավարարված են և օգտվելով այդ լեմմայի անհավասարությունից կունենաք՝
$$\mathbb{P}[\hat{L}(f) - L(f)]< -\epsilon] \leq e^{\frac{-2N\epsilon^2}{B^2}}$$
որտեղից և հավանականության $\mathbb{P}[A] = 1 - \mathbb{P}[\bar A]$ հատկությունը օգտագործելով՝
$$\mathbb{P}[\hat{L}(f) - L(f) \geq -\epsilon] \geq 1 -e^{\frac{-2N\epsilon^2}{B^2}} $$
$e^{\frac{-2N\epsilon^2}{B^2}}$ հավասարեցնենք $\delta$-ի՝
$$\delta = e^{\frac{-2N\epsilon^2}{B^2}}$$
և լուծելով այն $\epsilon$-ի նկատմաբ՝ կունենաք հետևյալը՝
$$\epsilon = B \sqrt{ \frac{\log\left(\frac{1}{\delta}\right)}{2N}} $$
Այսպիսով առնվազն $1-\delta$ հավանականությամբ տեղի ունի հետևյալ անահավսարությունը՝
$$\hat{L}(f) \geq L(f) - B \sqrt{ \frac{\log\left(\frac{1}{\delta}\right)}{2N}}$$
Նկատենք որ $n \leq (k+1)N$, որտեղից անմիջապես հետևում է հետևյալ անհավասարությունը՝
$$\sqrt{\frac{k+1}{n}} \geq \sqrt{\frac{1}{N}}$$
Օգտագործելով վերջին անհավասարությունը կունենանք, որ առնվազն $1-\delta$ հավանականությամբ տեղի ունի
$$\hat{L}(f) \geq L(f) - B\sqrt{\frac{\left(k+1\right)\log \left(\frac{1}{\delta}\right) }{2n}}$$
անհավասարությունը։
\end{proof}
\begin{preposition}
\label{prep_vec_ineq}
Կամայական $v \in \mathbb{R}^d$ վեկտորի համար տեղի ունի հետևյալը՝
$$||v|| \leq \sqrt{2}\E_{\sigma \sim \{\pm1\}^d} \left| \sum_{i=1}^d \sigma_iv_i \right|$$
\end{preposition}
\begin{theorem}
\label{contr_theorem}
Դիցուք $\mathcal{X}$-ը որևէ բազմություն է և $(x_1, x_2, ..., x_n) \in X^N$։ Տրված է նաև $\mathcal{F}$ ֆունկցիաների բազմություն, որի կամայական $f \in \mathcal{F}$ ֆունկցիա $\mathcal{X}$ բազմությունը արտապատկերում է $\mathbb{R}^d$ էվկլիդյան տարածություն՝ $f:\mathcal{X} \rightarrow \mathbb{R}^d$։ Դիցուք $h_i$ ֆունկցիաներ ունենք որոնք $\mathbb{R}^d$ էվկլիդյան տարածությունը արտապատկերում են  իրական թվերի $\mathbb{R}$ տարածություն՝
$h_i:\mathbb{R}^d \rightarrow \mathbb{R}$, կամայական $i \in [n]$ համար։ Կենթադրենք, որ բոլոր $h_i$ ֆունկցիաները, ինչ-որ $L$ դրական հաստատունով Լիպշից հատկությամբ օժտված ֆունկցիաներ են։ Այդ դեպքում տեղի ունի հետևյալ անհավասարությունը՝
\begin{equation}
\label{contradiction_ineq}
\E_{\sigma \sim \{\pm 1\}^n}\left[\sup_{f \in \mathcal{F}}  \sum_{i=1}^n{\sigma_ih_i(f(x_i))}  \right]    \leq \sqrt{2}L \E_{\sigma \sim \{\pm1\}^{nd}} \left[  \sup_{f \in \mathcal{F}}  \sum_{i=1}^n\sum_{j=1}^d{\sigma_{ij}f_j(x_i)}   \right]
\end{equation}
\end{theorem}


\noindent
Թեորեմ \ref{contr_theorem}-ը կարելի է ընդհանրացնել  $h_i(v, y) \in \mathbb{R}$ ֆունկցիաների համար, որտեղ $v \in \mathbb{R}^d$, $y \in \mathcal{Y}$ և $h_i$ ֆունկցիաները ըստ $v$ փոփոխականի $L$ հաստատունով Լիպշից հատկությամբ օժտված ֆունկցիաներ են կամայական $y \in \mathcal{Y}$ համար։ 

\begin{theorem}
\label{contr_theorem}
Դիցուք $\mathcal{X}$-ը և $\mathcal{Y}$-ը որևէ բազմություններ են  և $(x_1, x_2, ..., x_n) \in X^N$։ Տրված է նաև $\mathcal{F}$ ֆունկցիաների բազմություն, որի կամայական $f \in \mathcal{F}$ ֆունկցիա $\mathcal{X}$ բազմությունը արտապատկերում է $\mathbb{R}^d$ էվկլիդյան տարածություն՝ $f:\mathcal{X} \rightarrow \mathbb{R}^d$։ Դիցուք $h_i$ ֆունկցիաներ ունենք՝ $$h_i:\mathbb{R}^d \times \mathcal{Y} \rightarrow \mathbb{R}$$ կամայական $i \in [n]$ համար։ Կենթադրենք, որ բոլոր $h_i(v, y)$ ֆունկցիաները, ինչ-որ $L$ դրական հաստատունով Լիպշից հատկությամբ օժտված ֆունկցիաներ են ըստ $v$-ի կամայական $y \in \mathcal{Y}$ համար։ Այդ դեպքում տեղի ունի հետևյալ անհավասարությունը՝
\begin{equation}
\label{contradiction_ineq}
\E_{\sigma \sim \{\pm 1\}^n}\left[\sup_{\substack{f \in \mathcal{F} \\ y \in \mathcal{Y}} }  \sum_{i=1}^n{\sigma_ih_i(f(x_i), y)}  \right]    \leq \sqrt{2}L \E_{\sigma \sim \{\pm1\}^{nd}} \left[  \sup_{f \in \mathcal{F}}  \sum_{i=1}^n\sum_{j=1}^d{\sigma_{ij}f_j(x_i)}   \right]
\end{equation}
\end{theorem}

\begin{proof}[Ապացույց]
Սկզբում ցույց տանք, որ բոլոր $i \in [n]$-երի համար և կամայական $g:\mathcal{F}\times \mathcal{Y} \rightarrow \mathbb{R}$  ֆունկցիոնալի համար տեղի ունի հետևյալ անհավասարությունը՝
\begin{equation}
\label{sup_ineq}
\E_{\epsilon \sim \{\pm1\}}\sup_{\substack{f \in \mathcal{F} \\ y \in \mathcal{Y}}}    {\epsilon h_i(f(x_i), y)} + g(f, y)   \leq    \sqrt{2}L 
\E_{\epsilon \sim \{\pm1\}^d}\sup_{\substack{f \in \mathcal{F} \\ y \in \mathcal{Y}}}      {   \sum_{j=1}^d    \epsilon_jf_j(x_i)} + g(f, y)
\end{equation}
Դիցուք $\delta > 0$ կամայական դրական թիվ է։ Այդ դեպքում համաձայն Ռադեմախերի փոփոխականի սահմանաման կունենանք՝
\begin{align*}
 2\E_{\epsilon \sim \{\pm1\}}\sup_{\substack{f \in \mathcal{F} \\ y \in \mathcal{Y}}}    {\epsilon h_i(f(x_i), y)} - \delta  
= \sup_{\substack{f, \bar{f} \in \mathcal{F}  \\ y \in \mathcal{Y}}}    {h_i(f(x_i), y)  + g(\bar{f}, y) - h_i(\bar{f}(x_i), y) + g(\bar{f}, y) - \delta}
\end{align*}

Օգտվելով սուպրեմումի սահմանումից՝ գոյություն ունեն $f*, \bar{f}^* \in \mathcal{F}$ ֆունկցիաներ, որ տեղի ունի հետևյալը՝
\begin{align*}
 &\sup_{\substack{f, \bar{f} \in \mathcal{F}  \\ y \in \mathcal{Y}}}    {h_i(f(x_i), y)  + g(\bar{f}, y) - h_i(\bar{f}(x_i), y) + g(\bar{f}, y) -\delta}  \leq \\
 &\leq \sup_{y \in \mathcal{Y}}   h_i(f^*(x_i), y) - h_i(\bar{f}^*(x_i), y) + g(f^*, y) + g(\bar{f}^*, y)
\end{align*}

Օգտագործելով $h_i$ ֆունկցաի Լիպշիցի հատկությամբ օժտված լինելը կունենանք՝ 

\begin{align*}
 &\sup_{y \in \mathcal{Y}}   h_i(f^*(x_i), y) - h_i(\bar{f}^*(x_i), y) + g(f^*, y) + g(\bar{f}^*, y) \leq \\
& \leq   {L||f^*(x_i) - \bar{f}^*(x_i)||} + \sup_{y \in \mathcal{Y}}  g(f^*, y) + g(\bar{f}^*, y)
\end{align*}

Պնդում \ref{prep_vec_ineq}-ը կիրառելով կստանանք՝
\begin{align*}
 &{L||f^*(x_i) - \bar{f}^*(x_i)||} + \sup_{y \in \mathcal{Y}}  g(f^*, y) + g(\bar{f}^*, y) \leq \\
& \leq  {\sqrt{2}L} \E_{\epsilon \sim \{\pm 1\}^d}    \left |  \sum_{j=1}^d \epsilon_j (f^*_j(x_i) - \bar{f}_j^*(x_i))  \right|                    + \sup_{y \in \mathcal{Y}}    g(f^*, y) + g(\bar{f}^*, y)  \leq \\
&\leq  {\sqrt{2}L} \E_{\epsilon \sim \{\pm 1\}^d}        \sup_{f, \bar{f} \in \mathcal{F}}      \left |  \sum_{j=1}^d \epsilon_j f_j(x_i) - \sum_{j=1}^d \epsilon_j\bar{f}_j(x_i)  \right|                    + \sup_{y \in \mathcal{Y}}    g(f, y) + g(\bar{f}, y) 
\end{align*}

Հեշտ է նկատել, որ կամայական ֆիքսված $\epsilon$-ի դեպքում 
$$\sup_{f, \bar{f} \in \mathcal{F}}      \left |  \sum_{j=1}^d \epsilon_j f_j(x_i) - \sum_{j=1}^d \epsilon_j\bar{f}_j(x_i)  \right|  = 
\sup_{f, \bar{f} \in \mathcal{F}}         \sum_{j=1}^d \epsilon_j f_j(x_i) - \sum_{j=1}^d \epsilon_j\bar{f}_j(x_i)    $$
և քանի որ $\sup_{y \in \mathcal{Y}}    g(f, y) + g(\bar{f}, y)$ ինվարիանտ է $f, \bar{f}$ ֆունկցիաների փոփոխման նկատմամբ, կունենանք՝ 

\begin{align*}
&  {\sqrt{2}L} \E_{\epsilon \sim \{\pm 1\}^d}    \left |  \sum_{j=1}^d \epsilon_j (f^*_j(x_i) - \bar{f}_j^*(x_i))  \right|                    + \sup_{y \in \mathcal{Y}}    g(f^*, y) + g(\bar{f}^*, y)  \leq \\
&\leq  {\sqrt{2}L} \E_{\epsilon \sim \{\pm 1\}^d}        \sup_{f, \bar{f} \in \mathcal{F}}        \sum_{j=1}^d \epsilon_j f_j(x_i) - \sum_{j=1}^d \epsilon_j\bar{f}_j(x_i)                      + \sup_{y \in \mathcal{Y}}    g(f, y) + g(\bar{f}, y)  = \\
&=  {\sqrt{2}L} \E_{\epsilon \sim \{\pm 1\}^d}        \sup_{f \in \mathcal{F}}        \sum_{j=1}^d \epsilon_j f_j(x_i)    + \sup_{y \in \mathcal{Y}}    g(f, y)    +  \E_{\epsilon \sim \{\pm 1\}^d}        \sup_{\bar{f} \in \mathcal{F}}   -  \sum_{j=1}^d \epsilon_j\bar{f}_j(x_i)                      + \sup_{y \in \mathcal{Y}}  g(\bar{f}, y) 
\end{align*}


Հաշվի առնելով Ռադեմախերի $\epsilon_j$ փոփոխականների սիմետրիկություը կստանանք՝

\begin{align*}
&{\sqrt{2}L} \E_{\epsilon \sim \{\pm 1\}^d}        \sup_{f \in \mathcal{F}}        \sum_{j=1}^d \epsilon_j f_j(x_i)    + \sup_{y \in \mathcal{Y}}    g(f, y)    +  \E_{\epsilon \sim \{\pm 1\}^d}        \sup_{\bar{f} \in \mathcal{F}}   -  \sum_{j=1}^d \epsilon_j\bar{f}_j(x_i)                      + \sup_{y \in \mathcal{Y}}  g(\bar{f}, y)  = \\
&= 2\left({\sqrt{2}L} \E_{\epsilon \sim \{\pm 1\}^d}        \sup_{f \in \mathcal{F}}        \sum_{j=1}^d \epsilon_j f_j(x_i)    + \sup_{y \in \mathcal{Y}}    g(f, y)  \right)    = \\
&=  2\left({\sqrt{2}L} \E_{\epsilon \sim \{\pm 1\}^d}        \sup_{\substack{f \in \mathcal{F}  \\ y \in \mathcal{Y}  }}     \sum_{j=1}^d \epsilon_j f_j(x_i)    +   g(f, y)  \right)    
\end{align*}

Այսպիսով կամայական $\delta > 0$ դրական թվի համար՝
$$\E_{\epsilon \sim \{\pm1\}}\sup_{\substack{f \in \mathcal{F} \\ y \in \mathcal{Y}}}    {\epsilon h_i(f(x_i), y)} - \delta  \leq {\sqrt{2}L} \E_{\epsilon \sim \{\pm 1\}^d}        \sup_{\substack{f \in \mathcal{F}  \\ y \in \mathcal{Y}  }}     \sum_{j=1}^d \epsilon_j f_j(x_i)    +   g(f, y)   $$

Քանի որ վերջինս տեղի ունի ցանկացած  $\delta$-ի համար, այստեղից անմիջապես հետևում է \ref{sup_ineq} անհավասարությունը։

\par Այժմ ինդուկցիայի միջոցով ցույց տանք,
որ  ցանկացած $m \in \{0, ..., n\}$ համար տեղի ունի հետևյալ անհավասարությունը։

\begin{align*}
&\E_{\epsilon \sim \{\pm 1\}^n}\left[\sup_{\substack{f \in \mathcal{F} \\ y \in \mathcal{Y}} }  \sum_{i=1}^n{\epsilon_ih_i(f(x_i), y)}  \right]    \leq \sqrt{2}L \E_{\sigma \sim \{\pm1\}^{md}} \left[  \sup_{f \in \mathcal{F}}  \sum_{i=1}^m\sum_{j=1}^d{\sigma_{ij}f_j(x_i)}   \right] +\\ &+\E_{\epsilon \sim \{\pm 1\}^{n-m}}\left[\sup_{\substack{f \in \mathcal{F} \\ y \in \mathcal{Y}} }  \sum_{i=m+1}^n{\epsilon_ih_i(f(x_i), y)}  \right]   
\end{align*}

\ref{contradiction_ineq} անհավասարությունը անմիջապես հետևում է՝ վերցնելով $m = n$։ Երբ $m=0$ անհավասարության երկու կողմերում նույն արտահայտությունն է գրված և հետևաբար տեղի ունի անհավասարությունը։ Կատարենք ինդուկցիոն ենթադրություն և համարենք անհավասարությունը տեղի ունի $(m-1)$-ի համար, որտեղ $m \leq n$։



\begin{align*}
&\E_{\epsilon \sim \{\pm 1\}^n}\left[\sup_{\substack{f \in \mathcal{F} \\ y \in \mathcal{Y}} }  \sum_{i=1}^n{\epsilon_ih_i(f(x_i), y)}  \right]   
 \leq \sqrt{2}L \E_{\sigma \sim \{\pm1\}^{(m-1)d}} \left[  \sup_{f \in \mathcal{F}}  \sum_{i=1}^{m-1}\sum_{j=1}^d{\sigma_{ij}f_j(x_i)}   \right] +\\ 
 &+\E_{\epsilon \sim \{\pm 1\}^{n-m+1}}\left[\sup_{\substack{f \in \mathcal{F} \\ y \in \mathcal{Y}} }  \sum_{i=m}^n{\epsilon_ih_i(f(x_i), y)}  \right] = \\  
 &= \E_{\substack{\epsilon \sim \{ \pm 1\}^{n-m}   \\ \sigma \sim \{ \pm 1\}^{(m-1)d} }}     \E_{\epsilon_m \sim \{ \pm1 \}} \left [          \sup_{\substack{f \in \mathcal{F} \\ y \in \mathcal{Y}}}     \left(\epsilon_m h_m(f(x_m), y)   +   \sqrt{2}L 				 \sum_{i=1}^{m-1}\sum_{j=1}^d{\sigma_{ij}f_j(x_i)}  + \sum_{i = m+1}^n \epsilon_ih_i(f(x_i), y)				\right )	\right ]
\end{align*}


Սահմանենք $$g(f, y) = \sqrt{2}L\sum_{i=1}^{m-1}\sum_{j=1}^d{\sigma_{ij}f_j(x_i)}  + \sum_{i = m+1}^n\epsilon_ih_i(f(x_i), y)$$
և տեղադրելով այն վերջինիս մեջ և օգտագործելով \ref{sup_ineq} անհավասարությունը կստանանք՝


\begin{align*}
 &\E_{\substack{\epsilon \sim \{ \pm 1\}^{n-m}   \\ \sigma \sim \{ \pm 1\}^{(m-1)d} }}     \E_{\epsilon_m \sim \{ \pm1 \}} \left [          \sup_{\substack{f \in \mathcal{F} \\ y \in \mathcal{Y}}}     \left(\epsilon_m h_m(f(x_m), y)   +   g(f, y)			\right )	\right ] \leq \\
 &\leq  \E_{\substack{\epsilon \sim \{ \pm 1\}^{n-m}   \\ \sigma \sim \{ \pm 1\}^{(m-1)d} }}     \E_{\sigma_m \sim \{ \pm1 \}^d} \left [          \sup_{\substack{f \in \mathcal{F} \\ y \in \mathcal{Y}}}     \left( \sum_{j = 1}^d  \sigma_{mj}f(x_m) +   g(f, y)			\right )	\right ] =\\
 &= \sqrt{2}L \E_{\sigma \sim \{\pm1\}^{md}} \left[  \sup_{f \in \mathcal{F}}  \sum_{i=1}^m\sum_{j=1}^d{\sigma_{ij}f_j(x_i)}   \right] + \E_{\epsilon \sim \{\pm 1\}^{n-m}}\left[\sup_{\substack{f \in \mathcal{F} \\ y \in \mathcal{Y}} }  \sum_{i=m+1}^n{\epsilon_ih_i(f(x_i), y)}  \right] 
\end{align*}


\end{proof}
  
\begin{theorem}
\label{rad_comp_th}
Դիցուք $\mathcal{G}$ ֆուկցիաների բազմությունը, որի յուրաքանչյուր ֆունկցիա  $Z$-ը արտապատկերոմ է $[0, 1]$ և $S = \{z_i\}_{i=1}^m$ m հզորությամբ միմյանցից անկախ և միևնույն բաշխումից ընտրված օրինակների բազմություն է։ Այդ դեպքում ցանկացած $\delta$ դրական թվի համար առվազն $1 - \delta$ հավանականությամբ բոլոր $g \in \mathcal{G}$ ֆունկցիաների համար տեղի ունի հետևյալ անհավասարությունները՝
\begin{equation}
\E[g(z)] \leq \frac{1}{m}\sum_{i=1}^mg(z_i) + 2\mathcal{R}_m(\mathcal{G}) + \sqrt{\frac{\log\left( \frac{1}{\delta} \right)}{2m}}
\end{equation}
և
\begin{equation}
\E[g(z)] \leq \frac{1}{m}\sum_{i=1}^mg(z_i) + 2\mathcal{R}_S(\mathcal{G}) + 3\sqrt{\frac{\log \left( \frac{2}{\delta} \right)}{2m}}
\end{equation}
\end{theorem}

Դիցուք ունենք միմյանցից անկախ $\mathcal{P}(\mathcal{T} \text{ } |\text{ }  |\mathcal{T}| = k +1)$ բաշխումից ընտրված $N$ հատ առաջադրանքներ՝ $\mathcal{T}_1, ..., \mathcal{T}_N$  և $\mathcal{T} = \cup_{i=1}^N{\mathcal{T}_i}$։ Միավորված առաջադրանքի հզորությունը $n$ է՝  $|\mathcal{T}| =n $։ Այժմ ենթադրենք միավորված $\mathcal{T}$ առաջադրանքի համար ունենք միմյանցին ակախ և $D_{\mathcal{T}}$ բաշխումից ընտրված $M$ օրինակներ՝
$$S = \{(x_1, y_1), (x_2, y_2), ..., (x_M, y_M) | x_i \in \mathcal{X}, y_i \in \mathcal{T} \text{ և } i \in [M] \}$$
$\mathcal{T}$   առաջադրանքի համար դիցուք $g(x) = Wf(x)$ գծային դասակարգչն է ըստ $f \in \mathcal{F}$ ներկայացման, որտեղ $W$-ն $(n+1) \times d$ չափանի մատրիցա է և $W \in \mathcal{V}$։ $g(x)$ դասակարգչի էմպիրիկ սխալանքը $S$ բազմության վրա սահմանենք հետևյալ կերպ՝
$$\hat{L}(\mathcal{T},f, W) = \frac{1}{M}\sum_{i=1}^Ml(\{(Wf(x_i))_{y_i} - (Wf(x_i))_{y_j}\}_{y_i \neq y_j})$$ 
Ալգորիթմը որով սովորելու ենք ներկայցման ֆունկցիա $\mathcal{F}$ դասից հետևյալն է՝
$$(\hat{f}, \hat{W}) = \argmin_{\substack{f \in \mathcal{F} \\ W \in \mathcal{V}}} \hat{L}(\mathcal{T},f, W)$$
որտեղ $\hat{f}$ փնտրվող ներկայացումն է։ Այսպիսով ալգորթմը ըստ $f$ ներկայացման և գծային դասկարգիչի $W$ մատրիցայի  մինիմիզացնում է $\mathcal{T}$ առաջադրանքի վերահսկիչ էմպիրիկ սխալանքը $S$ օրինակների բազմության վրա։

\begin{lemma}
\label{rad_lemma_main}
Դիցուք $\delta$-ն կամայական դրական թիվ է։ Այդ դեպքում առնվազն $1-\delta$ հավանականությամբ կամայական $f \in \mathcal{F}$ ներկայացման և կամայական $W \in \mathcal{V}$ մատրիցայի համար տեղի ունի հետևյալ անհավասարությունը՝
$$L(\mathcal{T}, \hat{f}, \hat{W}) \leq L(\mathcal{T}, f, W) + Gen_M$$,
\end{lemma}
\begin{proof}[Ապացույց]
 Սահմանենք $G$ ֆունկցիաների բազմությունը հետևյալ կերպ՝
$$G = \left \{ (x, y) \mapsto  g_{f, W}(x, y) = \frac{1}{B}l(\{[Wf(x)]_y - [Wf(x)]_{y'}\}_{y\neq y'}   ) | f \in \mathcal{F}, W \in \mathcal{V} \right \}$$
Վերցնենք $Z = \mathcal{X} \times \mathcal{T}$ և $S = \left \{z_i = (x_i, y_i) \right \}_{i=1}^M$, կիրառելով \ref{rad_comp_th} թեորեմը $G$ ֆունկցիաների բազմության համար կունենանք՝
\begin{align}
\label{rad_ineq_1}
\E[g(z)] \leq \frac{1}{m}\sum_{i=1}^mg(z_i) + \frac{2}{M}\E_{\sigma \sim \{\pm1\}^M} \sup_{\substack{ f  \in \mathcal{F}  \\ W \in \mathcal{V}}}\sum_{i=1}^M \sigma_ig_{f,W}(z_i) +    3\sqrt{\frac{\log \left( \frac{2}{\delta} \right)}{2m}}
\end{align}

Այժմ ցույց տանք, որ ցանկացած $W \in \mathcal{V}$  և $i \in [M]$ համար $h_i(f(x_i), W) = g_{f,W}(z_i)$ ֆունկցիան ըստ $f(x_i)$-ի ինչ-որ $L$ հաստատունով օժտված է Լիպշիցի հատկությամբ։ Ներմուծենք $\Phi_y(f(x), W)$ ֆունկցիան, այնպես որ $h_i = \frac{1}{B} l \circ \Phi_{y_i}$։ Ֆիքսենք որևէ $y \in \mathcal{T}$ դաս և մնացած $n$ դասերը համարակալենք  $\mathcal{T} \setminus \{y\} =\{y'_1, y'_2, ..., y'_n\} $։ $\Phi_y : \mathbb{R}^d \times \mathcal{V} \rightarrow \mathbb{R}^n$ որի տեսքը հետևյալն է՝
$$\Phi_y(x, W) = (w_yx-w_{y'_{i}}x)_{i \in [n]}$$

Ըստ $x$ փոփոխականի $\Phi_y$  ֆունկցիայի Յակոբյանը նշանակենք $J_{\Phi_y} $-ով։
$$J_{\Phi_y}  = 
 \begin{pmatrix}
 \frac{\partial \Phi_{y1}}{\partial x_1} & \frac{\partial \Phi_{y1}}{\partial x_2} & \cdots & \frac{\partial \Phi_{y1}}{\partial x_d} \\
  \frac{\partial \Phi_{y2}}{\partial x_1} & \frac{\partial \Phi_{y2}}{\partial x_2} & \cdots & \frac{\partial \Phi_{y2}}{\partial x_d} \\
  \vdots  & \vdots  & \ddots & \vdots  \\
  \frac{\partial \Phi_{yn}}{\partial x_1} & \frac{\partial \Phi_{yn}}{\partial x_2} & \cdots & \frac{\partial \Phi_{yn}}{\partial x_d} \\
 \end{pmatrix} =
 \begin{pmatrix}
 w_{y1} - w_{{y'}_11} & w_{y2} - w_{{y'}_12} & \cdots & w_{yd} - w_{{y'}_1d} \\
   w_{y1} - w_{{y'}_21} & w_{y2} - w_{{y'}_22} & \cdots & w_{yd} - w_{{y'}_2d} \\
  \vdots  & \vdots  & \ddots & \vdots  \\
  w_{y1} - w_{{y'}_n1} & w_{y2} - w_{{y'}_n2} & \cdots & w_{yd} - w_{{y'}_nd} \\
 \end{pmatrix}
 $$

\begin{align*}
||J_{\Phi_y}||_F &= \sqrt{\sum_{i = 1}^n \sum_{k = 1}^d    \left(w_{yk} - w_{y'_ik}\right)^2} = \sqrt{n\sum_{k =1}^d w^2_{yk}   - 2 \sum_{i=1}^n \sum_{k=1}^d w_{yk} w_{y'_ik}  + \sum_{i=1}^n \sum_{k=1}^d  w^2_{y'_ik} } \\
&\leq  \sqrt{nQ^2+2nQ^2+nQ^2} = 2Q\sqrt{n}
\end{align*}
Այսպիսով $\Phi_y$ ֆունկցիան  ըստ $x$-ի փոփոխականի $2Q\sqrt{n}$ հաստատունով Լիպշիցի հատկությամբ օժտված ֆունկցիա է և քանի որ $l$-ը $\eta$ հաստատունով Լիպշիցի հատկությամբ էր օժտված, ապա կունենաք որ $h_i$ ֆունկցիաները բոլոր $i \in [M]$ համար $\frac{2 \eta Q\sqrt{n}}{B}$ հաստատունով ըստ $f(x_i)$-ի Լիպշիցի հատկություն ունի ցանկացած $W \in \mathcal{V}$ մատրիցայի համար։


Նկատենք  որ թեորեմ  \ref{contr_theorem}-ի պայմանները բավարարված են և կիրառելով այն կունենանք՝
$$\E_{\sigma \sim \{\pm1\}^M} \sup_{\substack{ f  \in \mathcal{F}  \\ W \in \mathcal{V}}}\sum_{i=1}^M \sigma_ig_{f,W}(z_i) \leq  \frac{2\sqrt{2} \eta Q\sqrt{n}}{B}        \E_{\sigma \sim \{\pm1\}^{Md}} \sup_{\substack{ f  \in \mathcal{F}  }}\sum_{i=1}^M \sum_{j=1}^d \sigma_{ij}f(x_i)$$


Վերջինս տեղադրենք \ref{rad_ineq_1}-ի մեջ և անհավասարության երկու կողմը բազմապատկենք $B$-ով, ցանկացած $g \in G$ համար կունենանք՝

\begin{align*}
\E[Bg(z)] \leq \frac{1}{M}\sum_{i=1}^MBg(z_i) + \frac{4\sqrt{2} \eta Q\sqrt{n}}{M} \E_{\sigma \sim \{\pm1\}^{Md}} \sup_{\substack{ f  \in \mathcal{F}  }}\sum_{i=1}^M \sum_{j=1}^d \sigma_{ij}f(x_i)+    3B\sqrt{\frac{\log \left( \frac{2}{\delta} \right)}{2M}}
\end{align*}
որտեղից էլ՝
\begin{align}
\label{ineq_rad_2}
L(\mathcal{T}, f, W) \leq \hat{L}(\mathcal{T}, f, W) + \frac{4\sqrt{2} \eta Q\sqrt{n}}{M} \E_{\sigma \sim \{\pm1\}^{Md}} \sup_{\substack{ f  \in \mathcal{F}  }}\sum_{i=1}^M \sum_{j=1}^d \sigma_{ij}f(x_i)+    3B\sqrt{\frac{\log \left( \frac{2}{\delta} \right)}{2M}}
\end{align}
որը տեղի ունի $\forall f \in \mathcal{ F}$ և $\forall W \in \mathcal{V}$։ Քանի որ  \ref{ineq_rad_2}-ը տեղի ունի $\forall f \in \mathcal{ F}$ և $\forall W \in \mathcal{V}$, հետևաբար այն տեղի ունի նաև $\hat{f}$ և $\hat{W}$-ի համար՝
\begin{align}
\label{ineq_rad_3}
L(\mathcal{T}, \hat{f}, \hat{W}) \leq \hat{L}(\mathcal{T}, \hat{f}, \hat{W}) + \frac{4\sqrt{2} \eta Q\sqrt{n}}{M} \E_{\sigma \sim \{\pm1\}^{Md}} \sup_{\substack{ f  \in \mathcal{F}  }}\sum_{i=1}^M \sum_{j=1}^d \sigma_{ij}f(x_i)+    3B\sqrt{\frac{\log \left( \frac{2}{\delta} \right)}{2M}}
\end{align}

Դիցուք $f^*, W^* = \argmin_{f \in \mathcal{F}, W \in \mathcal{V}}L(\mathcal{T}, f, W)$։ Կիրառելով Հոֆդինգի անհավասարությունը առնվազն $1- \frac{\delta}{2}$ հավանականությամբ տեղի ունի հետևյալը՝
$$\hat{L}(\mathcal{T}, f,^* W^*) \leq L(\mathcal{T}, f,^* W^*) + B\sqrt{\frac{\log\frac{2}{\delta}}{2M}}$$
Հաշվի առնելով որ $\hat{L}(\mathcal{T}, \hat{f}, \hat{W})  \leq \hat{L}(\mathcal{T}, f,^* W^*) $՝ \ref{ineq_rad_3} անհավասարությունը կարող ենք գրել հետևյալ կերպ՝
\begin{align}
\label{ineq_rad_4}
L(\mathcal{T}, \hat{f}, \hat{W}) \leq  L(\mathcal{T}, f,^* W^*) + \frac{4\sqrt{2} \eta Q\sqrt{n}}{M} \E_{\sigma \sim \{\pm1\}^{Md}} \sup_{\substack{ f  \in \mathcal{F}  }}\sum_{i=1}^M \sum_{j=1}^d \sigma_{ij}f(x_i)+    4B\sqrt{\frac{\log \left( \frac{2}{\delta} \right)}{2M}}
\end{align}

\noindent Հեշտ է նկատել որ \ref{ineq_rad_4} տեղի ունի  $\forall f \in \mathcal{ F}$ և $\forall W \in \mathcal{V}$ համար՝
\begin{align}
\label{ineq_rad_5}
L(\mathcal{T}, \hat{f}, \hat{W}) \leq  L(\mathcal{T}, f, W) + \frac{4\sqrt{2} \eta Q\sqrt{n}}{M} \E_{\sigma \sim \{\pm1\}^{Md}} \sup_{\substack{ f  \in \mathcal{F}  }}\sum_{i=1}^M \sum_{j=1}^d \sigma_{ij}f(x_i)+    4B\sqrt{\frac{\log \left( \frac{2}{\delta} \right)}{2M}}
\end{align}

Կիրառելով պատահույթների միավորման բանաձևը \ref{ineq_rad_5} տեղի ունի առնվազն $1-\delta$ հավանականությամբ և լեմման ապացուցված է։

\end{proof}

\begin{theorem}
Դիցուք $\delta$ կամայական դրական թիվ է, այդ դեպքում առնվազն $1-\delta$ հավանականությամբ տեղի ունի հետևյալ անհավասարությունը՝ 
$$L(\hat{f}) \leq L(T, f, W) + Gen_{M, n} \text{  } \forall f \in \mathcal{F} \text{ և } \forall{W} \in \mathcal{V}$$
Որտեղ $M$ ուսուցման օրինակների քանակն է, իսկ $N$-ը առաջադրանքների քանակը:
\end{theorem}

\begin{proof}[Ապացույց] Առաջին հերթին կարելի է հեշտությամբ համոզվել որ դիտարկվող հինջ և լոգիստիկ կորստի ֆունկցիաները բավարարում են հետևյալ հատկությանը՝
\begin{equation}
\label{prop_log_hinge}
\forall I \subseteq [t] \text{   }   l(\{v_i\}_{i\in I}) \leq l(\{v_i\}_{i \in [t]})
\end{equation}
Դիցուք ունենք $N$ հատ միմյանցից անկախ $\mathcal{T}_1,\mathcal{T}_2, ...,\mathcal{T}_N$  առաջադրանքները ընտրված 
$$\mathcal{P}(\mathcal{T} \text{ } |\text{ }  |\mathcal{T}| = k +1)$$
բաշխումից և $\mathcal{T} = \cup_{i=1}^N\mathcal{T}_i$, որի հզորությունը հավասար է $n+1$-ի: Ինչպես նաև ունենք $M$ հատ օրինակների ուսուցման բազմությունը ընտրված $\mathcal{D_\mathcal{T}}$ բաշխումից՝
$$S = \{(x_1, y_1), (x_2, y_2), ..., (x_M, y_M) | x_i \in \mathcal{X}, y_i \in \mathcal{T} \text{ }\forall i \in[M]\}$$

\noindent Այժմ ֆիքսենք ցանկացած $\delta$ դրական թիվ: Օգտվելով լեմմա \ref{rad_lemma_main}-ից առնվազն $1-\frac{\delta}{2}$ հավանականությամբ տեղի ունի հետևյալ անհվասարությունը $f \in \mathcal{F}$-ի և $W \in \mathcal{V}$-ի համար՝

\begin{align}
\label{ineq_rad_6}
L(\mathcal{T}, \hat{f}, \hat{W}) \leq  L(\mathcal{T}, f, W) + \frac{4\sqrt{2} \eta Q\sqrt{n}}{M} \E_{\sigma \sim \{\pm1\}^{Md}} \sup_{\substack{ f  \in \mathcal{F}  }}\sum_{i=1}^M \sum_{j=1}^d \sigma_{ij}f(x_i)+    4B\sqrt{\frac{\log \left( \frac{4}{\delta} \right)}{2M}}
\end{align}

\noindent Ըստ \ref{prop_log_hinge} հատկության $\forall i \in [N]$ համար, քանի որ $T_i \subseteq T$, տեղի ունի հետևյալը՝
\begin{align}
\label{prop_ineq_1}
L(\mathcal{T}_i, \hat{f}, \hat{W}_{\mathcal{T}_i}) \leq L(\mathcal{T}, \hat{f}, \hat{W} )
\end{align}
 որտեղ $\hat{W}_{\mathcal{T}_i}$ կազմված է $T_i$ առաջադրանքում մասնակցող դասերին համապատասխան  $\hat{W}$ մատրիցայի տողերից: Համաձայն $L(\mathcal{T}_i, \hat{f})$ սահմանման ակնհայտ է, որ` 
\begin{align}
\label{prop_ineq_2}
L(\mathcal{T}_i, \hat{f}) \leq L(\mathcal{T}_i, \hat{f}, \hat{W}_{\mathcal{T}_i})
\end{align}

Միավորելով \ref{ineq_rad_6}, \ref{prop_ineq_1}  և \ref{prop_ineq_2}-ը $\forall i \in [N]$՝ կունենանք՝


\begin{align}
\label{ineq_rad_7}
L(\mathcal{T}_i, \hat{f}) \leq  L(\mathcal{T}, f, W) + \frac{4\sqrt{2} \eta Q\sqrt{n}}{M} \E_{\sigma \sim \{\pm1\}^{Md}} \sup_{\substack{ f  \in \mathcal{F}  }}\sum_{i=1}^M \sum_{j=1}^d \sigma_{ij}f(x_i)+    4B\sqrt{\frac{\log \left( \frac{4}{\delta} \right)}{2M}}
\end{align}

Այժմ \ref{ineq_rad_7} անհավասարության աջ և ձախ մասերը ըստ բոլոր $i $-երի գումարենք և անհավասարության երկու մասերը բաժանենք $N$-ի կստանանք՝

\begin{align}
\label{ineq_rad_8}
\frac{1}{N}\sum_{i=1}^NL(\mathcal{T}_i, \hat{f}) \leq  L(\mathcal{T}, f, W) + \frac{4\sqrt{2} \eta Q\sqrt{n}}{M} \E_{\sigma \sim \{\pm1\}^{Md}} \sup_{\substack{ f  \in \mathcal{F}  }}\sum_{i=1}^M \sum_{j=1}^d \sigma_{ij}f(x_i)+    4B\sqrt{\frac{\log \left( \frac{4}{\delta} \right)}{2M}}
\end{align}

Նկատենք որ \ref{ineq_rad_8} անհասարության ձախ մասը $\mathcal{T}_1,\mathcal{T}_2, ...,\mathcal{T}_N$ առաջադրանքների Էմպիրիկ վերահսկիչ միջին կորուստն է $\hat{f}$ ներկայացման համար՝


\begin{align}
\label{emp_eq}
\hat{L}(\hat{f}) = \frac{1}{N}\sum_{i=1}^NL(\mathcal{T}_i, \hat{f})
\end{align}

\ref{emp_eq}-ը տեղադրենք \ref{ineq_rad_8}-ի մեջ՝
 
\begin{align}
\label{ineq_rad_9}
\hat{L}(\hat{f}) \leq  L(\mathcal{T}, f, W) + \frac{4\sqrt{2} \eta Q\sqrt{n}}{M} \E_{\sigma \sim \{\pm1\}^{Md}} \sup_{\substack{ f  \in \mathcal{F}  }}\sum_{i=1}^M \sum_{j=1}^d \sigma_{ij}f(x_i)+    4B\sqrt{\frac{\log \left( \frac{4}{\delta} \right)}{2M}}
\end{align}

Համաձայն լեմմա \ref{task_conc_lemm}-ի առնվազն $1-\frac{\delta}{2}$ հավանականությամբ $\hat{f}$ ներկայացման համար տեղի ունի հետևյալը՝ 


\begin{equation}
\label{lemm_ineq}
\hat{L}(\hat{f}) \geq L(\hat{f}) - B\sqrt{\frac{\left(k+1\right)\log \left(\frac{2}{\delta}\right) }{2(n+1)}}
\end{equation}



Կիրառելով պատահույթների միավորման բանաձևը՝ \ref{ineq_rad_9}, \ref{lemm_ineq}-ից  հետևում է, որ առնվազն $1-\delta$ հավանականությամբ՝

 
\begin{align*}
\label{ineq_rad_10}
L(\hat{f}) \leq  L(\mathcal{T}, f, W) + \frac{4\sqrt{2} \eta Q\sqrt{n}}{M} \E_{\sigma \sim \{\pm1\}^{Md}} \sup_{\substack{ f  \in \mathcal{F}  }}\sum_{i=1}^M \sum_{j=1}^d \sigma_{ij}f(x_i)+    4B\sqrt{\frac{\log \left( \frac{4}{\delta} \right)}{2M}} + B\sqrt{\frac{\left(k+1\right)\log \left(\frac{2}{\delta}\right) }{2(n+1)}}
\end{align*}

 \end{proof}

\pagebreak
\medskip
\begin{thebibliography}{9}
\phantomsection
\addcontentsline{toc}{section}{Գրականություն} 

\bibitem{bib_item_1}
John R Firth.
\textit{A synopsis of linguistic theory,}  1930-1955. Studies in linguistic analysis, 1957.

\bibitem{bib_item_2}
Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Corrado, and Jeff Dean. \textit{ Distributed representations of words and phrases and their compositionality.} 
In C. J. C. Burges, L. Bottou, M. Welling, Z. Ghahramani, and K. Q. Weinberger, editors, Advances in Neural Information
Processing Systems 26, pages 3111–3119. Curran Associates, Inc., 2013c.

\bibitem{bib_item_3}
Karen Sparck Jones. \textit{A statistical interpretation of term specificity and its application in retrieval.}
Journal of documentation, 28(1):11–21, 1972.

\bibitem{bib_item_4}
Tomas Mikolov, Quoc V Le, and Ilya Sutskever. \textit{Exploiting similarities among languages for machine
translation.} arXiv preprint arXiv:1309.4168, 2013b.

\bibitem{bib_item_5}
Sepp Hochreiter and J¨urgen Schmidhuber. \textit{Long short-term memory.} Neural computation, 9(8):
1735–1780, 1997.

\bibitem{bib_item_6}
Yoshua Bengio, R´ejean Ducharme, Pascal Vincent, and Christian Jauvin. \textit{A neural probabilistic
language model.} Journal of machine learning research, 3(Feb):1137–1155, 2003.

\bibitem{bib_item_7}
Ilya Sutskever, Oriol Vinyals, and Quoc V Le. \textit{Sequence to sequence learning with neural networks.}
In Advances in neural information processing systems, pages 3104–3112, 2014.

\bibitem{bib_item_8}
Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. \textit{Neural machine translation by jointly
learning to align and translate.} arXiv preprint arXiv:1409.0473, 2014.


\bibitem{bib_item_9}
Ramesh Nallapati, Bowen Zhou, Cicero dos Santos, C¸ a glar Gul¸cehre, and Bing Xiang. \textit{ Abstractive
text summarization using sequence-to-sequence rnns and beyond.} CoNLL 2016, page 280, 2016.

\bibitem{bib_item_10}
Kelvin Xu, Jimmy Ba, Ryan Kiros, Kyunghyun Cho, Aaron Courville, Ruslan Salakhudinov, Rich
Zemel, and Yoshua Bengio. Show, attend and tell: \textit{Neural image caption generation with visual
attention.} In International Conference on Machine Learning, pages 2048–2057, 2015.


\bibitem{bib_item_11}
Oriol Vinyals, Alexander Toshev, Samy Bengio, and Dumitru Erhan. Show and tell: \textit{A neural
image caption generator.} In Proceedings of the IEEE conference on computer vision and pattern
recognition, pages 3156–3164, 2015.

\bibitem{bib_item_12}
Guillaume Lample, Miguel Ballesteros, Sandeep Subramanian, Kazuya Kawakami, and Chris Dyer.
\textit{Neural architectures for named entity recognition.} In Proceedings of NAACL-HLT, pages 260–
270, 2016.

\bibitem{bib_item_13}
Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D Manning, Andrew Ng, and
Christopher Potts. \textit{Recursive deep models for semantic compositionality over a sentiment treebank.} In Proceedings of the 2013 conference on empirical methods in natural language processing,
pages 1631–1642, 2013.

\bibitem{bib_item_14}
Sanjeev Arora, Yuanzhi Li, Yingyu Liang, Tengyu Ma, and Andrej Risteski. Rand-walk: \textit{A latent
variable model approach to word embeddings.} arXiv preprint arXiv:1502.03520, 2015

\bibitem{bib_item_15}
Gerard Salton. \textit{The smart retrieval systemexperiments in automatic document processing.} 1971.

\bibitem{bib_item_16}
Gerard Salton and Christopher Buckley.\textit{Term-weighting approaches in automatic text retrieval.
Information processing and management,} 24 (5):513–523, 1988.


\bibitem{bib_item_17}
John S Breese, David Heckerman, and Carl Kadie. \textit{Empirical analysis of predictive algorithms for
collaborative filtering.} In Proceedings of the Fourteenth conference on Uncertainty in artificial
intelligence, pages 43–52. Morgan Kaufmann Publishers Inc., 1998.

\bibitem{bib_item_18}
Zi Yin, Keng-hao Chang, and Ruofei Zhang. \textit{Deepprobe: Information directed sequence understanding and chatbot design via recurrent neural networks.} In Proceedings of the 23rd ACM
SIGKDD International Conference on Knowledge Discovery and Data Mining, pages 2131–2139.
ACM, 2017.

\bibitem{bib_item_19}
Andrea Frome, Greg S Corrado, Jon Shlens, Samy Bengio, Jeff Dean, Tomas 	olov, et al. \textit{Devise:
A deep visual-semantic embedding model.} In Advances in neural information processing systems,
pages 2121–2129, 2013.

\bibitem{bib_item_20}
Eliya Nachmani, Elad Marciano, Loren Lugosch, Warren J Gross, David Burshtein, and Yair Beery.
\textit{Deep learning methods for improved decoding of linear codes.} arXiv preprint arXiv:1706.07043,
2017.

\bibitem{bib_item_21}
Scott Deerwester, Susan T Dumais, George W Furnas, Thomas K Landauer, and Richard Harsh-
man. \textit{Indexing by latent semantic analysis. Journal of the American society for information
science,} 41(6):391, 1990.

\bibitem{bib_item_22}
Kenneth Ward Church and Patrick Hanks. \textit{ Word association norms, mutual information, and
lexicography.} Computational linguistics, 16(1):22–29, 1990.

\bibitem{bib_item_23}
Yoshiki Niwa and Yoshihiko Nitta. \textit{Co-occurrence vectors from corpora vs. distance vectors from
dictionaries.} In Proceedings of the 15th conference on Computational linguistics-Volume 1, pages
304–309. Association for Computational Linguistics, 1994.


\bibitem{bib_item_24}
Omer Levy and Yoav Goldberg. \textit{Neural word embedding as implicit matrix factorization.} In
Advances in neural information processing systems, pages 2177–2185, 2014.

\bibitem{bib_item_25}
Ronan Collobert and Jason Weston. 2008. \textit{ A uni-
fied architecture for natural language processing: Deep
neural networks with multitask learning.} In Proceed-
ings of the 25th International Conference on Machine
Learning.

\bibitem{bib_item_26}
Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. \textit{Efficient estimation of word representations
in vector space.} ICLR Workshop, 2013.

\bibitem{bib_item_27}
X. Rong. \textit{word2vec parameter learning explained.} arXiv:1411.2738, 2014.
https://arxiv.org/abs/1411.2738

\bibitem{bib_item_28}
Gutmann, M. and Hyvarinen, A. (2010). \textit{ Noise-contrastive estimation: A new estimation principle for
unnormalized statistical models.} In Proceedings of The Thirteenth International Conference on Artificial
Intelligence and Statistics (AISTATS’10).

\bibitem{bib_item_29}
J. Pennington, R. Socher, and C. D. Manning. \textit{GloVe: Global vectors for word representation.} In EMNLP, 2014.

\end{thebibliography}

\end{document}
