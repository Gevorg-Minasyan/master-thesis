\documentclass[11pt]{article}

\usepackage[top=2cm,bottom=3cm,left=3cm,right=2cm]{geometry}
\usepackage{amsmath}


\DeclareMathOperator*{\argmax}{argmax}
\DeclareMathOperator*{\argmin}{argmin}
\DeclareMathOperator*{\E}{\mathbb{E}}

\usepackage{amsfonts}
\usepackage{amsthm}
\usepackage{fontspec}
\usepackage{hyperref}
\usepackage{cite}
\hypersetup{
    colorlinks=true,
    linkcolor=blue,
    filecolor=magenta,      
    urlcolor=cyan,
    citecolor=cyan
}


% this package for increase or decrease vertical space between section chapter paragraph etc.
\usepackage{titlesec}

%tikz package for drawing graphs%
\usepackage{tkz-berge}
\usepackage{tikz}
\usetikzlibrary{decorations.markings}
\usetikzlibrary{positioning,chains,fit,shapes,calc}
\renewcommand{\figurename}{Նկ.}
% the figure insert specific place with [H] param
\usepackage{float}


% increase distance between two lines%
\renewcommand{\baselinestretch}{1.5}

% increase vertical space between section subsection%
\titlespacing*{\section}
{0pt}{8ex plus 1ex minus .2ex}{10ex plus .2ex}
\titlespacing*{\subsection}
{0pt}{5ex plus 1ex minus .2ex}{3ex plus .2ex}


\setmainfont[
BoldItalicFont=arnamu_italic_bold.ttf,
BoldFont      =arnamu_bold.ttf,
ItalicFont    =arnamu_italic.ttf]{arnamu.ttf}

\renewcommand{\contentsname}{Բովանդակություն}
\renewcommand{\refname}{Գրականություն}

\renewcommand{\tablename}{Աղյուսակ}
%\usepackage{mathtools}
%\newcommand\defeq{\stackrel{\mathclap{\normalfont\mbox{def}}}{=}}
\newcommand\defeq{\mathrel{\overset{\makebox[0pt]{\mbox{\normalfont\tiny\sffamily def}}}{=}}}

\sloppy
\begin{document}

\newtheorem{theorem}{Թեորեմ}
\newtheorem{lemma}{Լեմմա}
\newtheorem{corollary}{Հետևանք}
\newtheorem{preposition}{Պնդում}

\theoremstyle{definition} %After this line new defined commands by \newthorem will be non italic.%
\newtheorem{innercustomcase}{Դեպք}
\newenvironment{customcase}[1]
  {\renewcommand\theinnercustomcase{#1}\innercustomcase}
  {\endinnercustomcase}
\newtheorem{case}{Դեպք}

\raggedbottom


\begin{titlepage}

\begin{center}
\large  Երևանի Պետական Համալսարան\\
\large Ինֆորմատիկայի և Կիրառական Մաթեմատիկայի Ֆակուլտետ\\
 Թվային անալիզի և մաթեմատիկական մոդելավորման ամբիոն\\
 \vspace{2mm}
\hrule width \hsize height 2pt  \kern 1mm \hrule width \hsize height 2pt 
\vspace{50mm}
\textbf{\huge Մագիստրոսական Թեզ\\}\noindent \newline \newline
\textbf{\large Թեմա՝}	\hspace{3mm}  Բառերի ներդրված վեկտորների ներկայացումների մասին
\end{center}
\vspace{25mm}
\begin{flushright}
\textbf{\large Ուսանող՝ \hspace{18mm}}			Մինասյան Գևորգ \\
\vspace{4mm}
\textbf{\large Ղեկավար՝ \hspace{2mm}}			ֆիզ. մաթ. գիտ. թեկնածու \\  Հ.Է. Դանոյան
\end{flushright}
\vspace{64mm}
\centering{\large Երևան -- 2019}
\end{titlepage}
\pdfbookmark{Բովանդակություն}{contents}
\tableofcontents
\newpage
\section*{\hfill Ներածություն \hfill} \noindent
\phantomsection
\addcontentsline{toc}{section}{Ներածություն}

\par Կառուցվածքային կամ համակարգված տվյալների  (օրինակ՝ տվյալների բազաների աղյուսակները) հետ աշխատանքը արագ և էֆեկտիվ է համակարգչի միջոցով։ Սակայն մարդիկ  հաղորդակցվում են միմյանց հետ, օգտագործելով բառեր՝  ձևվաորելով ոչ կառուցվածքային տվյալներ։ Ոչ կառուցվածքային տվյալների մշակման համար չկան ստանդարտացված  մեթոդներ։ Առկա  ծրագրավորման լեզուների միջոցով սահմանվում է որոշակի կանոների բազմություն, որով աշխատելու է ծրագիրը։ Ոչ կառուցվածքային տվյալների համար այս կանոնների բազմությունը բավականին վերացական է և դժվար է այն հստակ սահմանել:

Հազարավոր տարիների ընթացքում մարդու ուղեղը ձեռք է բերել հսկայական փորձ հասկանալու բնական լեզուն։ Մարդիկ կարողանում են հասկանալ կարդացած տեքստը և կապել այն իրական աշխարհի հետ, կարողանում են այնտեղ նկարագրվող օբյեկնտերի իրական տեսքի մասին պատկերացում կազմել, զգալ այն հույզերը որն առաջացնում է այդ տեքստի բովանդակությունը։   Դեռևս համակարգիչը չի կարողանում բնական լեզուն հասկանալ այնպես ինչպես որ՝ մարդը։


Բնական լեզվի մշակումը արhեստական բանականության  ենթաճյուղ է, որն ուսումնասիրում է մշակել և հասկանալ մարդկային լեզուն համակարգիչի միջոցով,  մասնավորապես, ինչպես համակարգչային ծրագրերի միջոցով մշակել և վերլուծել մեծ քանակությամբ բնական լեզուների տվյալներ: Բնական լեզվի մշակումը հնարավորություն է տալիս համակարգիչներին կարդալ տեքստը, լսել խոսքը և մեկնաբանել այն։

Մեքենայական ուսուցման ալգորիթմները՝ հատկապես նեյրոնային ցանցերը,  լայն տարածում ունեն բնական լեզվի մշակման ոլորտում,  որոնցով կարելի է հասնել բարձր արդյունքների բնական լեզվի մշակման բազմաթիվ խնդիրներում, որոնցից են լեզվի մոդելավորումը,  քերականական և իմաստաբանական վերլուծությունը, մեքենայական թարգմանությունը և այլն։  

\section*{\hfill Բառերի ներդրված ներկայուցումներ \hfill} \noindent
\phantomsection
\addcontentsline{toc}{section}{Բառերի ներդրված ներկայուցումներ}

\par Բառերի էմբեդինգները ներկայացնում են բառի իմաստը վեկտորի միջոցով։ Բազմաթիվ մոդելներով են կառուցվում այդ վեկտորները, որոնց միավորող փիլիսոփայությունը այն է, որ բառի իմաստը որոշվում է կոնտեքստից՝ այն բառերից որոնց հետ միաժամանակ հանդիպում է տեքստում:  Ֆիրթի \cite{bib_item_1} կողմից լեզվաբանության մեջ առաջ է քաշված այսպես կոչված դիստրիբյուշնալ հիպոթեզը այն է՝ \textit{նման կոնտեքստներում հանդիպող բառերը հակված են նման իմաստներ ունենալ:}  Դիսկրետ մեծությունների՝ բառերի, արտապատկերումը դեպի էվկլիդյան տարածություն, հնարավոր է դարձնում բառերի միջև կատարել հանրհաշվական գործողություններ։ Դիսկրետ բառերի համար  գումարման գործողության սահմանում տալը հնարավոր չէ, բայց դիտարկելով դրանց վեկտորական ներկայացումները՝ գործողությունը դառնում է իմաստալից։  Ունենալով վեկտորական ներկայացումները՝  հնարավոր է դառնում բառի վեկտորների համար կատարել գծային և ոչ գծային ձևափոխություններ, բառերի միջև հարաբերությունները արտահայտել սկալյար արտադրյալի և կոսինուսի միջոցով։  Այս գործողությունների օգտագործմամբ կարելի է ստանալ բառերի միջև իմաստաբանական և ձևաբանական հարաբերությունները \cite{bib_item_2},  դոկումենտների(փաստաթղթերի) միջև նմանությունները \cite{bib_item_3} և համեմատել տարբեր լեզուների բառարանները \cite{bib_item_4}։ Բազմաթիվ կիրառական խնդիրներ մոդելավորելիս հիմքում օգտագործվում են բառերի վեկտորական ներկայացումները։  Ռեկուրենտ նեյրոնային ցանցերը, LSTM\cite{bib_item_5} ցանցերը բառերի վեկտորական ներկայացումների հետ միասին  օգտագործվում  են լեզվի մոդելավորման \cite{bib_item_6}, մեքենայական թարգմանության \cite{bib_item_7,  bib_item_8},  տեքստի համառոտ շարադրման (ամփոփմանշ eng. text summarization) \cite{bib_item_9} և նկարից տեքստ, վերնագիր ստեղծման (image caption generation) \cite{bib_item_10, bib_item_11} խնդիրներում։ Այլ կարևոր կիրառություններ են հատուկ անունների ճանաչումը ( named entity recognition) \cite{bib_item_12},  սենտիմենտի որոշումը( sentiment analysis) \cite{bib_item_13} և գեներատիվ լեզվի մոդելները \cite{bib_item_14} և այլն։ Դիսկրետ մեծությունների վեկտորական ներկայացումները ոչ միայն օգտագործվում են բնական լեզվի մշակման խնդիրներում այլ նաև տեղեկատվական որոնման մեջ (eng. information retrieval) \cite{bib_item_13, bib_item_15, bib_item_16 }, խորհրդատվական համակարգերում \cite{bib_item_17, bib_item_18}, նկարների մշակման մեջ \cite{bib_item_19} և նույնիսկ կոդավորման տեսության խնդիրներում \cite{bib_item_20}։

\subsection*{\hfill Սահմանումներ և նշանակումներ \hfill} \noindent
\phantomsection
\addcontentsline{toc}{subsection}{Սահմանումներ և նշանակումներ}

\par Աշխատանքում դիտարկելու ենք $n$ բառերից բաղկացած $\mathcal{V}=  \{1, 2, ..., n  \}$ համարակալված բառարանը։ Յուրաքանչյուր $i$ բառի կհամապատասխանացնենք $v_{i}$ վեկտոր էմբեդինգը։ Բոլոր վեկտորները միասին վերցրած կստանանք $E \in \mathbb{R}^ { n \times d}$ էմբեդինգ մատրիցան, որի $i$-րդ տողը $i$ բառի $E_i, . = v_{i}$ էմբեդինգն է։ Համակարգված մեծածավալ տեքստերի հավաքածուն կանվանենք կորպուս և կնշանակենք $\mathcal{C}$-ով, այն իրենից ներկայացնում է ինչ-որ  $w_1, w_2, ..., w_T$ բառերի հաջորդականություն։ $E$ էմբեդինգ մատրիցայի ուսուցումը կատարվում է որևէ ալգորիթմով, որը մուտքում ստանում է $\mathcal{C}$ կորպուսը։

\newpage

\subsection*{\hfill Ներդրված վեկտորներ և մատրիցային վերլուծություն \hfill} \noindent
\phantomsection
\addcontentsline{toc}{subsection}{Ներդրված վեկտորներ և մատրիցային վերլուծություն}

\par Վեկտոր Էմբեդինգների կառուցման սկզբնական պարզագույն մեթոդներում օգտագործվել է մատրիցային վերլուծությունը։ Մատրիցային վերլուծությամբ ստացված ներդրված վեկտորները մեքենայական ուսուցման և բնական լեզվի մշակման  ալգորիթմների մեծ դաս է, այդ թվում թաքնված սեմանտիկ վերլուծությունը կամ ինդեքսավորումը (LSA/LSI) ։ Համառոտ ներկայացնենք բացահայտ մատրիցային վերլուծությամբ ստացվող հայտնի մեթոդներից մի քանիսը։

\paragraph{Փաստաթղթի էմբեդինգներ թաքնված սեմանտիկ ինդեքսավորման օգտագործմամբ։} Առաջին անգամ Դիրվեստերի\cite{bib_item_21} և այլոց կողմից ներկայացված LSI մեթոդը փաստաթղթերի վերլուծության և տեղեկատվական որոնման համար հզոր գործիք է։ Դիցուք ունենք $m$ հատ $d_1, d_2, ..., d_m$ փաստաթղթերի հավաքածուն, որը կազմված է $n$ բառերից բաղկացած $\mathcal{V}$ բառարանից։ Յուրաքանչյուր $d_i$ փաստաթուղթ $m_i$ երկարությամբ ինչ-որ $w_{i1}, w_{i2}, ..., w_{im_i}$ բառերի հաջորդականություն է, որտեղ $w_{ij} \in \mathcal{V}$ և $1\leq i \leq m$, $1\leq j \leq m_i$։ Փաստաթղթերի հավաքածուից կառուցվում է $n \times m$ չափանի բառ-փաստաթուղթ $X$  մատրիցան, որի $x_{ij}$ տարը ամենապարզ դեպքում կարող է լինել $i$-րդ բառի հանդես գալու քանակը $d_j$ փաստաթղթում։ Սակայն քանակների մատրիցան հաճախ կրկնվող բառերի նկատմամբ կարող է  բայես պարունակել (անցանկալի առավելություններ տալ) (կողմնակալ կարող է լինել), այս թերությունից խոսափելու համար այսպես կոչված քանակների վերակշռման տարբեր եղանակներ են առաջարկվել, որոնցից ամենահայտնին $TF-IDF$ կոչված մատրիցան է \cite{bib_item_3, bib_item_16}։ 

\par $TF-IDF$ վերակշռման եղանակով նվազեցվում է հաճախ հանդիպող ոչ անհրաժեշտ բառերի  կշիռները, նախորդ պարզագույն քանակների  դեպքում այդ բառերին առավելություն էր տրվում։ Այսպիսի բառերի առկայությունը աղավաղում է փաստաթղթերի միջև իրական տարբերությունների հայտնաբերմանը։ Այս պատճառով ներմուծվում է նաև այն փաստաթղթերի քանակը, որոնք պարունակում են տվյալ բառը։  Օգտագործելով այս լրացուցիչ մեծությունը՝ կարելի է հայտնաբերել, որ բառերն են շատ հաճախ հանդիպում ամբողջ փաստաթղթերում և դրանց տալ փոքր կշիռ։  
\par Դիցուք $n_{ij}$-ն $i$-րդ բառի քանակն է $d_j$ փաստաթղթում և $n_i$-ն այն փաստաթղթերի քանակը, որ պարունակում են $i$-րդ բառը։  Ապա $TF-IDF$ մատրիցայի տարրը հետևյալն է՝ $$x_{ij} = n_{ij}  \log_2{\frac{m}{n_i+1}}:$$
Այժմ դիտարկենք $X$ մատրիցայի սինգուլյար վերլուծությունը՝  $$X= UDV^T։$$ Փաստաթղթերի ներդրման վեկտորների $E$ մատրիցան ստացվում է հետևալ կերպ՝
$$E = f_{\alpha, d}(X) = U_{1:d}D^{\alpha} _{1:d},$$
որտեղ $\alpha$-ն սովորաբար ընտրվում է $0.5$ կամ $1$։
\paragraph{Բառերի էմբեդինգներ թաքնված սեմանտիկ վերլուծության օգտագործմամբ։} Դիցուք ունենք  $\mathcal{C}$ տեքստերի հավաքածուն կազմված $\mathcal{V}$ բառարանից, որտեղ $|\mathcal{V}| = n$։ Ինչպես նաև տրված է $m$ հզորությամբ կոնտեքստների $\mathcal{K}$ վերջավոր բազմությունը։ Դիտարկենք բառ-կոնտեքստ $n\times m$ չափանի $X$ մատրիցան, որի տողերը ինդեքսավորված են բառերով, սյուները՝ կոնտեքստներով։ Ամենապարզ դեպքում  $x_{ij}$ տարը, $i$-րդ բառի և $c_j$ կոնտեքստի համատեղ հայնտվելու քանակն է։ Մասնավորապես $\mathcal{K}$ կոնտեքստների բազմությունը կարելի է վերցնել $\mathcal{V}$  բառարանը, այս դեպքում $X$-ը կլինի $n\times n$ չափանի սիմետրիկ մատրիցա, որի սյունները նույնպես ինդեքսավորված կլինեն բառերով, ինչպես՝ տողերը։ Ֆիքսենք որևէ $k$ բնական թիվ՝ անվանելով այն ֆիքսված պատուհանի չափ, որը դիտարկվող $\mathcal{C}$ կորպուսում $k$ հատ հարևան բառերի հաջորդականություն է։ Յուրաքանչյուր $(i, j)$ բառազույգի համար $x_{ij}$ տարը այն քանակն է, որ  $k$ պատուհանի ներսում միաժամանակ հայտնվում են $i$ և $j$ բառերը։ 
\par Պարզագույն քանակներով ստեղծված $X$ մատրիցայի դեպքում, ինչպես թաքնված սեմանտիկ ինդեքսավորման մոդելում, այնպես էլ այս դեպքում հաճախ կրկնվող բառերին անցանկալի առավելություններ են տրվում։ Վերոնշյալ խնդիրը վերանում է, երբ $X$ մատրիցայի վերակշռված տարբերակներն են ընտրվում, որոնցից առավել հայտնի են PMI \cite{bib_item_22}, դրական PMI \cite{bib_item_23}  և տեղաշարժված PMI մատրիցաները  \cite{bib_item_23}։ 
\par PMI(Pointwise Mutual Information) համեմատական  չափ է, այն ցույց է տալիս $x$ և $y$ պատահույթների միաժամանակ հանդես գալու հավանականությունը՝ այն համեմատելով այդ մեծությունների  անկախ լինելու դեպքում սպասվող հավանականության հետ՝
$$I(x, y) = \log_2 \frac{P(x, y)}{P(x)P(y)}։$$
\par Դիտարկվող $i$ բառի և $c$ կոնտեքստի միջև PMI մեծությունը սահամանվում է հետևյալ կերպ՝
$$PMI(i, c) =\log_2 \frac{P(i, c)}{P(i)P(c)}։ $$
Համարիչը ցույց է տալիս երկու բառերի տեքստում միասին հայտնվելը, իսկ հայտարարը՝  միասին հայտնվելու սպասվող քանակը, եթե ենթադրենք նրանց անկախությունը։ Այսպիսով վերոնշյալ հարաբերությունը գնահատում է, թե ինչքան հաճախ են երկու բառեր միասին հայտնվում, քան կհայտնվեին պատահաբար։  PMI-ի միջոցով կարելի է հայտնաբերել միմյանց հետ ուժեղ ասոցածված բառերը։
\par $PMI$ արժեքները փոփոխվում են բացասական անվերջությունից դրական անվերջություն, սակայն բացասական $PMI$-ը, որը նշանակում է բառերը ավելի քիչ են միմյանց հետ հայտնվում, քան պատահաբար կհայտնվեին, պահանջում է աշխատել հսկայական չափի տեքստերի հավաքածուի՝ կորպուսի, հետ։ Այս պատճառով սովորաբար օգտագործվում է $PPMI$ վերակշռման եղանակը՝
$$PPMI(i, c) = \max \left(\log_2 \frac{P(i, c)}{P(i)P(c)}, 0 \right)։$$

\par Դիցուք $n_{ij}$-ն $i$ բառի $c_j$ կոնտեքստում հայտվելու քանակն է, այդ դեպքում՝
$$p_{ij} = \frac{n_{ij}}{\sum_{i=1}^{n}\sum_{j=1}^{m}n_{ij}} \quad p_i = \frac{\sum_{j=1}^{m}n_{ij}}{\sum_{i=1}^{n}\sum_{j=1}^{m}n_{ij}} \quad q_j = \frac{\sum_{i=1}^{n}n_{ij}}{\sum_{i=1}^{n}\sum_{j=1}^{m}n_{ij}}։$$
Օգտագործելով  $p_{ij}$, $p_{i}$ և $q_{j}$ թվերը՝ PMI և PPMI մատրիցաների էլեմենտները տրվում են հետևյալ կերպ՝
$$PMI_{ij} = \log_2 \frac{p_{ij}}{p_iq_j} \quad PPMI_{ij} = \max \left (\log_2 \frac{p_{ij}}{p_iq_j}, 0\right)։$$
Այժմ ենթադրենք $UDV^T$ պարզագույն քանակների X, վերակշռված PMI կամ PPMI մատրիցաներից որևէ մեկի սինգուլյար վերլուծությունն է, ապա ներդրված վեկտորների $E$ մատրիցան տրված $d$ չափողականության և $\alpha$ հիպերպարամետրի դեպքում հաշվվում է հետևյալ կերպ՝
$$E = f_{\alpha, d}(X) = U_{1:d}D^{\alpha} _{1:d}։$$
$\alpha$ հիպերպարամետրը կարող է կամայական թիվ լինել $[0, 1]$ ինտերվալից, որը սիմետրիկության համար սովորաբար ընտրվում է $0.5$, սակայն $\alpha=0$ կամ $\alpha=1$ դեպքերը նույնպես հաճախ են ընտրվում։
\pagebreak

\subsection*{\hfill Ներդրված վեկտորների կառուցման արդի ալգորիթմներ \hfill} \noindent
\phantomsection
\addcontentsline{toc}{subsection}{Ներդրված վեկտորների կառուցման արդի ալգորիթմներ}
\par Նեյրոնային ցանցով լեզվի մոդելները \cite{bib_item_6, bib_item_25}, որոնք ոչ գծային և ոչ ուռուցիկ մեթոդներ են, առաջարկում են ներդրված վեկտորների կառուցման մեկ այլ եղանակ՝ բառի ներդրված վեկտորը պարզապես ցանցի ներքին ներկայացումն է տվյալ բառի համար։ Բառերի ներդրված վեկտորների և  լեզվի վիճակագրական մոդելի ուսուցումը կատարվում է միաժամանակ։ Այս մոտեցմամբ ներդրված վեկտորների կառուցման համար նեյրոնային ցանցերի տարբեր կառուցվածքներ են առաջարկվել՝  պարզագույն բազմաշերտ ցանցերից մինչև ռեկուրենտ ցանցեր։ Սակայն 2013 թվականին Միքայիլովի և այլոց կողմից \cite{bib_item_26} առաջարկված լոգ-գծային երկու մոդելները արդյունավետությամբ գերազանցեցին նախորդ բոլոր  բարդ կառուցվածք ունեցող ցանցերին և ամենակարևորը ժամանակային բարդությունը արմատապես նվազեցվեց։ Հնարավոր եղավ օգտագործել շատ ավելի մեծ տեքստերի հավաքածու և ավելի ճշգրիտ ներդրված վեկտորներ ստանալ, որոնք հուսալիորեն կարիելի է օգտագործել տարբեր տեսակի բնական լեզվի մշակման մոդելների հիմքում։ Ստորև համառոտ ներկայացնենք ներդրված վեկտորների կառուցման հիմնական մեթոդները։

\subsubsection*{\hfill \textit{Word2vec} մեթոդների ընտանիք \hfill} \noindent
\phantomsection
\addcontentsline{toc}{subsubsection}{ \textit{Word2vec} մեթոդների ընտանիք}

\par Word2vec մեթոդների ընտանիքում մոդելների երկու տարբերակ է առաջարկվել։ Երկու տարբերակում էլ բառերի ներդրված վեկտորնեը սկզբնարժեքավորվում են պատահականորեն։ Այնուհետև հերթականորեն դիտարկվում է  տեքստերի հավաքածուն՝ ֆիքսված պատուհանի չափով  յուրաքանչյուր բառի համար մոտակա բառերը համարվում են դիտարկվող բառի կոնտեքստը։ Ընդհանուր դեպքում ստոխաստիկ գրադիենտային վայրէջքի օպտիմիզացիոն մեթոդով մինիմիզացվում է  հերթական բառի և կոնտեքստ բառերի միջև սկալյար արտադրյալը։ Ամեն անգամ, երբ երկու բառեր հանդիպում են նման կոնտեքստում, սկալյար արտադրյալի փոքրացման շնորհիվ նրանց միջև կապը ուժեղանում է, հեռավորությունը՝ փոքրանում։ Սակայն միայն փոքրացնելով նման կոնտեքստներում հանդիպող բառերի վեկտորների միջև հեռավորությունը՝ հանգում ենք հետևյալ խնդրին։ Անսահմանափակ տեքստերի հավաքածուի դեպքում մինիմալ վիճակը կլինի այն, որ բոլոր վեկտորները հավասարվելու են կամ գտնվելու են միևնույն դիրքում, որը ակնհայտորեն ցանկալի չէ։ Այս խնդիրը շրջանցելու նպատակով \textit{word2vec}-ում 	սկզբում ներկայացվում է հիերարխիկ սոֆթմաքսը \cite{bib_item_27}, իսկ ավելի ուշ negative sampling-ը։ Վերջինս ավելի պարզ և արդյունավետ է։ Երբ յուրաքանչյուր անգամ նման կոնտեքստում հանդիպող բառերի միջև հեռավորությունները փոքրացվում են, նախապես տրված քանակով պատահական բառեր են ընտրվում և դիտարկվող բառից հեռավորությունները՝ մեծացվում։  Այս կերպ ապահովվում է ոչ նման բառերի միմյանցից մեծ հեռավորության վրա գտնվելը։ \textit{word2vec}-ում առաջարկված մոդելների երկու տարբերակներն են՝


\par 1. \textit{Կոնտեքստից կանխատեսել դիտարկվող բառը։} $w_t$ բառի կանխատեսումը կատարվում է օգտագործելով՝ այդ բառի ֆիքսված $k$ պատուհանի շրջակայքում գտնվող բառերը՝ $w_{t-k},w_{t-k+1},...,w_{t-1},w_{t+1},...,w_{t+k-1},w_{t+k}։$ 
\par 2.  \textit{Դիտարկվող բառից կանխատեսել կոնտեքստը։} $w_{t-k},w_{t-k+1},...,w_{t-1},w_{t+1},...,w_{t+k-1},w_{t+k}$  կոնտեքստի կանխատեսումը կատարվում է՝ օգտագործելով $w_t$ դիտարկվող բառը։ 

Դիցուք ունենք $w_1,w_2,...,w_T$ հաջորդական բառերով կազմված կորպուսը և որևէ ֆիքսված $k$ պատուհանի չափ։ $c_t$-ով նշանակենք $w_t$ բառի կոնտեքստը՝ $$c_t \defeq w_{t-k},w_{t-k+1},...,w_{t-1},w_{t+1},...,w_{t+k-1},w_{t+k},$$
որտեղ $k+1\leq t \leq T- k$։ $w$ բառի ներդրված վեկտորը նշանակենք՝ $v_w$-ով, իսկ կոնտեքստ վեկտորը՝ $u_w$-ով։ $c_t$ կոնտեքստին համապատասխան բառերի վեկտորների միջինը նշանակենք՝ $$u_{c_t} \defeq \frac{1}{2k} \sum_{-k \leq j \leq k} u_{t+j}։$$\par Վերոնշյալ կոնտեքստից բառի կանխատեսման դեպքում կոնտեքստի բառերի վեկտորները գումարվում են և օգտագործվում դիտարկվող բառի կանխատեսման համար։ Նպատակային կորստի ֆունկցիան, որը պետք է մաքսիմիզացնել, հետևյալն է՝
$$\frac{1}{T-2k}\sum_{t=k+1}^{T-k}\log p(w_t|c_t),$$
որտեղ $p(w_t|c_t)$ հավանականությունը սահմանվում է սոֆթմաքս ֆունկցիայի միջոցով՝
$$p(w_t|c_t) = \frac{e^{v_{w_t}^Tu_{c_t} }}{{\sum_{w \in V} e^{{v^T_w}u_{c_t}}}}։$$ 
\par  Բառից կոնտեքստի կանխատեսման դեպքում կոնտեքստի յուրաքանչյուր բառ կանխատեսվում է անկախ, երբ տրված է դիտարկվող բառը՝
$$\frac{1}{T-2k}\sum_{i=k+1}^{T-k}\sum_{-k \leq j \leq k}  \log p(w_{t+j}|w_t),$$ որտեղ $p(w_{t+j}|w_t)$ հավանակնությունը սահմանվում է հետևյալ կերպ՝ 
$$p(w_{t+j}|w_t) = \frac{e^{u_{w_{t+j}}^Tv_{w_t} }}{{\sum_{w \in V} e^{{u^T_w}v_{w_t}}}}։$$ 

Սակայն սոֆթմաքսի օգտագործումը կորստի ֆունկցիայում շատ ժամանակատար է՝  գրադիենտի հաշվման գործողությունների քանակը համեմատական է $V$ բառարանի չափին, որը տարբեր կորպուսների դեպքում տասնյակ հազարաների կարող է հասնել, երբեմն նույնիսկ միլիոնների։ Միքայիլովի աշխատանքում երկու տարբեր լուծումներ է առաջարկվում ալգորիթմի արագացման համար։ Առաջինը հիերարխիկ սոֆթմաքսի  օգտագործումն է, որի օգնությամբ գնահատվում է սոֆթմաքս ֆունկցիան և բարդությունը $O(\log_2|V|)$ է։ Երկրորդ առավել հայտնի լուծման այլընտրանքը \textit{բացասական օրինակների ընտրության}\cite{bib_item_28}  եղանակն է։ Բացասական օրինակների ընտրության դեպքում  կոնտեքստից բառի կանխատեսման մոդելի համար յուրաքանչյուր $w_t$ ընթացիկ բառի համար հետևյալ նպատակային ֆունկցիան է մաքսիմիզացվում՝
$$\log \sigma(v^T_{w_t}u_{c_t}) + \sum_{i=1}^s \log \sigma( - v^T_{\tilde{w}_i}u_{c_t}),  \quad \sigma(x) = \frac{e^x}{1+e^x}$$
որտեղ $\sigma(x)$-ը կոչվում է լոգիստիկ ֆունկցիա, $s$-ը բացասական օրինակների քանակն է և որպես հիպերպարամետր նախապես տրված է լինում, իսկ $\tilde{w_i}$ բացասական օրինակները ընտրվում են նախօրոք ֆիքսված $P(w)$ հավանականային բաշխումից, մասնավորապես այն կարող է լինել դիսկրետ հավասարահավանական բաշխումը։ Բառից կոնտեքստի կանխատեսման մոդելի դեպքում $w_t$ ընթացիկ բառի կորստի ֆունկցիան բացասական օրինակների ընտրության դեպքում հետևյալն է՝
$$\log \sigma(u^T_{w_{t+j}}v_{w_t}) + \sum_{i=1}^s \log \sigma( - u^T_{\tilde{w}_i}v_{w_t})։$$

\pagebreak 

\subsubsection*{\hfill  Գլոբալ վեկտորներ բառերի ներկայացման համար \hfill} \noindent
\phantomsection
\addcontentsline{toc}{subsubsection}{Գլոբալ վեկտորներ բառերի ներկայացման համար}
\par Բառերի վեկտորական ներկայացման հաջորդ հայտնի  \textit{glove} կոչված մոդելը ներկայացվել է 2014 թվականին Պենինգթոնի \cite{bib_item_28} և այլոց կողմից։ Բառերի ներդրված վեկտորների ուսուցման մոդելները կարելի է բաժանել երկու ընտանիքների՝
գլոբալ մատրիցային վերլուծություն օգտագործող մեթոդներ (օրինակ. թաքնված սեմանտիկ վերլուծությունը) և լոկալ  կոնտեքստ օգտագործող մեթոդներ (օրինակ. \textit{word2vec})։ Այս երկու ընտանիքներում առկա մեթոդները զգալի թերություններ ունեն։ Լոկալ կոնտեքստ օգտագործող մեթոդները, ինչպիսին է \textit{word2vec} մեթոդների ընտանիքը, անալոգ բառերի հայտնաբերման առաջադրանքում ճշգրիտ է աշխատում, սակայն կորպուսի բառերի համատեղ հայտվելու գլոբալ քանակները  չի օգտագործում, փոխարենը այս մեթոդների ուսուցումը կատարվում է առանձին լոկալ պատուհաններում հայտնված բառերով։ Մինչդեռ թաքնված սեմանտիկ վերլուծության նման մոդելները, որոնք օգտագործում են կորպուսի բառերի համատեղ հայտնվելու գլոբալ քանակները, համեմատաբար ճշգրտությամբ զիջում են \textit{word2vec} ընտանիքի մեթոդներին։ \textit{Glove} մոդելով ներդրված վեկտորների կառուցման ալգորիթմը գործում է գլոբալ քանակների վրա, բայց ներդրված վեկտորական տարածությանը ունենում է \textit{word2vec}-ի պես ճշգրիտ  իմաստաբանական կառուցվածք։ Բառերի վեկտորական ներկայացումներ կառուցելու համար կոնտեքստում բառերի միասին հայտնվելու քանակական մեծությունները հիմնական ինֆորմացիայի աղբյուրն է, սակայն թե ինչպես է քանակներից բառի իմաստը գեներացվում դեռևս բաց հարց է։  Այս մոդելով բացատրվում է ներդրված վեկտորների իմաստաբանական կառուցվածք ունենալը՝ բացահայտ օգտագործելով կորպուսի գլոբալ  քանակները որպես իմաստի գեներացման ինֆորմացիայի հիմնական աղբյուր։ Դիցուք $X$-ը բառերի համատեղ հայտնվելու քանակների մատրիցան է, որի $x_{ij}$ տարը $j$-րդ բառի հայնվելու քանակն է $i$-րդ բառի կոնտեքստում։ $x_i = \sum_{k} {x_{ik}}$ այն քանակն է, որ կամայական բառ հայտնվում է $i$-րդ բառի կոնտեքստում։ Այժմ՝ $$\hat p_{ij} = \frac{x_{ij}}{x_i}$$ $i$-րդ բառի կոնտրեքստում  $j$-րդ բառի հայտնվելու հավանականության գնահատականն է։ Դիտարկենք մի պարզ օրինակ, որը ցույց է տալիս, թե բառի իմաստային  ինչպիսի առանձնահատկություններ են կրում համտեղ հայտնվելու հավականության գնահատականները։  Վերցնենք $i=ice$,  $j=steam$ բառազույգը, որոնց համար դիտարկելով տարբեր բառերի հետ համատեղ հայտվելու հավանականությունները՝ բառազույգի հարբերության մասին պատկերացում կարելի է կազմել։ Այն բառերը, որոնք կապված են «ice» բառի հետ և կապված չեն «steam» բառի հետ, օրինակի համար $k = solid$, այս դեպքում $\hat p_{ik}/ \hat  p_{jk}$ հարաբերությունը կլինի մեծ։  Նմանապես $k = gas$ վերցնելով՝ որը «steam» բառի հետ է կապված, բայց ոչ «ice» բառի, վերոնշյալ հարաբերությունը կլինի փոքր։ Այն բառերը որոնք կապված չեն ո՛չ $«ice»$ բառի, ո՛չ $«steam»$ բառի հետ, ինչպիսիք են $«water»$ և $«fashion»$ բառերը, հարաբերությունը մեկին մոտ է լինելու։ Աղյուսակ \ref{glove_tb1}-ում բերված են հավանականությունների գնահատականները և նկարագրված հարբերությունները, որոնք հաստատում են վերոնշյալ ենթադրությունները։

\begin{center}
\begin{table}[H]
\begin{tabular}{ c| c| c| c| c } 
 Հավանականություն և հարաբերություն & k = solid  & k = gas & k = water & k = fashion\\ 
 \hline
$p(k|ice)$ & $1.9 \times 10 ^{-4}$& $6.6 \times 10^{-5}$& $3.0 \times 10^{-3}$& $1.7 \times 10^{-5}$\\
$p(k|steam)$ & $2.2 \times 10 ^{-5}$& $7.8 \times 10^{-4}$& $2.2 \times 10^{-3}$& $1.8 \times 10^{-5}$\\
$p(k|ice)/p(k|steam)$ &8.9& $8.5 \times 10 ^{-2}$&1.36&0.96 \\ 
\end{tabular}
\caption{«ice» և «steam» բառերի և ընտրված կոնտեքստ բառերի հետ համատեղ հայտնվելու հավանականային գնահատականները ստացված շուրջ 6 միլիարդ բառեր պարունակող տեքստերի հավաքածուից։ \label{glove_tb1}}
\end{table}
\end{center}

Ուսումնասիրելով աղյուսակ \ref{glove_tb1}-ի թվերը, պարզ է դառնում, որ բառերի համատեղ հայտնվելու հավանականաթյունների հարաբերությունները  ավելի լավ են տարբերակում դիտարկվող բառի հետ կապված բառերը, ոչ կապված բառերից։  Այս հանգամանքը հաշվի առնելով՝ բառերի ներդրված վեկտորների ուսուցման մոդելում ավելի նպատակահարմար է օգտագործել բառերի համատեղ հայտնվելու հավանակնությունների հարբերությունները, այլ ոչ թե միայն հավանակնությունները։ Նկատենք $\frac{\hat p_{ik}}{\hat p_{jk}}$ հարաբերությունը կախված $i$, $j$ և $k$ բառերից, ընդհանուր դեպքում ներդրված վեկտորների ուսուցման մոդելը ունի հետևյալ տեսքը՝ 
\begin{equation}
F(v_{i}, v_{j}, \widetilde{v}_k) = \frac{\hat p_{ik}}{\hat p_{jk}},
\label{glove_first_eq}
\end{equation}
որտեղ  	$v_i, v_j \in \mathbb{R}^d$ համապատասխանաբար $i$ և $j$ բառերի ներդրված վեկտորներն են, իսկ $\widetilde{v}_k \in \mathbb{R}^d$ $k$ բառի կոնտեքստ վեկտորն է։  Վերոնշյալ հավասարուման մեջ դեռևս չսահմանված $F$ ֆունկցիան պետք է կիրառել բառերի ներդրված վեկտորների և կոնտեքստ վեկտորների նկատմամբ, որի արդյունքը պետք է մոտարկի $\frac{\hat p_{ik}}{\hat p_{jk}}$ հարաբերությանը։ 
$F$ ֆունկցիայի ընտրության տարբերակները բազմաթիվ են, սակայն որոշ հատկություններ հաշվի առնելով՝ այդ ընտրությունը դառնում է միակը։ Հատկություններից մեկը այն է, որ $\frac{\hat p_{ik}}{\hat p_{jk}}$ հարաբերության ինֆորմացիան պետք է ներառել բառերի ներդրված վեկտորների տարածությունում։ Քանի որ վեկտորական տարածություններին բնորոշ են գծային կառուցվածքները, ապա հարբերության ինֆորմացիան կարելի է  ներառել վեկտորների տարբերության միջոցով։ Այս նպատկով, կարելի է դիտարկել այն ֆունկցիաները որոնք կախված ենք դիտարկող երկու բառերի ներդրված վեկտորների տարբերությունից՝
\begin{equation}
F(v_i-v_j, \widetilde{v}_k) = \frac{\hat p_{jk}} {\hat p_{jk}}։
\end{equation}
 $F$ ֆունկցիան կարելի է ընտրել բարդ պարամետրացված ֆունկցիաների ընտանիքից (օրինակ նեյրոնային ցանց), բայց այդ դեպքում ներդրված վեկտորական տարածությունում գծային կառուցվածքները չեն պահպանվելու։ Գծային կառուցվածքների պահպանման նպատակով $F$ ֆունկցիայի վեկտոր արգումենտները փոխարինվում է դրանց սկալյար արտադրյալով՝
 $$F((v_i-v_j)^T \widetilde{v}_k) = \frac{\hat p_{ik}}{\hat p_{jk}}։$$
 Եթե պահանջենք $F$-ը լինի հոմոմորֆիզմ $\left ( \mathbb{R}, + \right )$ և $\left (\mathbb{R}_{>0}, \times  \right )$ խմբերի միջև, ապա կունենանք հետևյալը՝
  \begin{equation}
 F((v_i - v_j)^T \widetilde{v}_k) = \frac{F(v_i^T\widetilde{v}_k)}{F(v_j^T\widetilde{v}_k)} 
  \end{equation}
 \begin{equation}
 F(v_i^T\widetilde{v}_k) = \hat p_{ik} = \frac{x_{ik}}{x_i}։
  \end{equation}
 Ակնհայտ է $e^x$ ցուցչային ֆունկցիան հանդիսանում է վերոնշյալ հոմոմորֆիզմի լուծումը և այս դեպքում $v_i^T\widetilde{v}_k$ սկալյար արտադրյալը կունենա հետևյալ տեսքը՝
 \begin{equation}
 v_i^T\widetilde{v}_k = \log \hat p_{ik} = \log x_{ik} - \log x_i։
 \label{glove_dot_prod_eq}
 \end{equation}
 Նկատենք որ բառ-կոնտեքստ $X$ մատրիցայում կարելի է ազատորեն փոխել բառերի և կոնտեքստների  ինդեքսավորման առանցքները, քանի որ այդ մատրիցան սիմետրիկ է, հետևաբար նաև $v$ բառ վեկտորի և $\widetilde{v}$ կոնտեքստ վեկտորների նշանակությունների փոփոխման նկակտմաբ պետք է ինվարիանտ լինի ներդված վեկտորների կառուցման մոդելը։ \ref{glove_dot_prod_eq} հավասարումը կարելի է ավելի պարզեցնել՝ քանի որ $\log x_i$ անդամը $k$ բառից կախված չէ, ապա այն կարելի է փոխարնել $b_i$ պարամետրով $i$ բառի համար, ինչպես նաև հաշվի առնելով վերոնշյալ սիմետրիկությունը, ավելացնել նաև $k$ կոնտեքստի համար $\widetilde{b}_k$ պարամետերը։
 \begin{equation}
v_i^T \widetilde{v} + b_i + \widetilde{b}_k = \log x_{ik}
\label{glove_dot_bias_eq}
\end{equation}

\ref{glove_dot_bias_eq} հավասարումը չի կարելի վերջնական համարել, քանի որ $\log x_{ik}$ որոշված չէ, երբ $i$ և $k$ բառազույգը կորպուսում ֆիքսված պատուհանի ներսում չի հայտնվել՝ $x_{ik} =0$։ Այս խնդիրը կարելի է շրջանցել կատարելով միվոր տեղաշարժ լոգարիթմում՝ $\log \left(x_{ik} + 1\right)։$ Հաճախ հանդիպող բառազույգերին անցանկալի առավելություններ չտալու նպատակով ներմուծվում վերակշռման $f(x_{ik})$ ֆունկցիան։ $f$-ի ընտրության տարբերակները բազմաթիվ են, հեղինակները ընտրել են հետևյալ ֆունկցիան՝ 
 \begin{align}
 f(x) = \begin{cases}
\left ( \frac{x}{x_{max}} \right)^\alpha & \quad x < x_{max} \\
1 & \quad \text{հ.դ.}
 \end{cases}
 \end{align}
 Փորձնական ճանապարով հիպերպարամետրերը ընտրվել են՝  $x_{max} = 100$ և $\alpha = \frac{3}{4}$։ 
$Glove$ մոդելում ներդրված վեկտորների կառուցման համար, \ref{glove_dot_bias_eq} բանաձևի օգտագործմամբ մինիմիզացվում է կշռված փոքրագույների քառակուսացման հետևյալ կորստի  ֆունկցիան՝
\begin{equation}
J = \sum_{i,j=1}^{n}{f(x_{ij})(v_i^T\widetilde{v_j}+b_i+\widetilde{b_j} - \log(x_{ij} + 1))^2}։
\label{glove_opt_eq}
\end{equation}

\pagebreak

\section*{\hfill Բառերի ներդրված ներկայացումների որոշ տեսական հիմնավորումներ \hfill} \noindent

\phantomsection
\addcontentsline{toc}{section}{Բառերի ներդրված ներկայացումների որոշ տեսական հիմնավորումներ}

\par \cite{bib_item_24} աշխատանքում փորձնական և որոշ տեսական հիմնավորումների միջոցով ցույց է տրվում, որ բառերի ներդրված  վեկտորների արդի մեթոդները կապված են ավելի հին $PMI$ մատրիցայի վրա հիմնված մոդելների հետ։ $PMI$ մատրիցայի պարզագույն տարբերակում   դիտարկվում է սիմետրիկ մատրիցա, որի յուրաքանչյուր տող և սյուն ինդեքսավորված է բառերով:  Այդ սիմետրիկ մատրիցայի $(w, w')$ ինդեքսով տարրն է՝ 
\begin{equation}
PMI(w, w') = \log \frac{p(w, w')}{p(w)p(w')},
\end{equation} որտեղ $p(w, w')$-ն այն էմպիրիկ հավանականություն է, որ $w$ և $w'$  բառերը տեքստերի հավաքածում կհայտնվեն որոշակի չափով պատուհանի ներսում, իսկ $p(w)$-ն  այն հավանականությունն է որ տեքստերի հավաքածուից պատահական ընտրված բառը $w$-ն է։ Այնուհետև բառերի ներդրված վեկտորները ստացվում են վերոնշյալ $PMI$ մատրիցայի սինգուլյար վերլուծության միջոցով։ Փորձնական եղանակով պարզվում է $PMI$ մատրիցան մոտարկվում է փոքր ռանգի մատրիցայով: Գոյություն ունեն բառերի ներդրված  վեկտորներ, որոնց չափը շատ փոքր է համեմատած դիտարկվող բառարանում պարունակող բառերի քանակի հետ և որոնց համար՝ 
\begin{equation}
v_w^Tv_w' \approx PMI(w, w')։
\label{pmi_approx}
\end{equation}
\cite{bib_item_14} աշխատանքում առաջարվում է տեքստ գեներացնելու հավանականային նոր մոդել, որտեղ դուրս բերված բանաձևը, որը տեղի ունի որոշակի նախնական ենթադրությունների բավարարման դեպքում,  ուղակիորեն  բացատրում է \ref{pmi_approx} մոտարկումը: Կարևոր ենթադրություններից մեկն այն է որ, \textit{բառերի ներդրված վեկտորները տարածական իզոտրոպ են, ինչը նշանակում է, որ վեկտորները չունեն նախընտրելի ուղղություն տարածությունում}: $n$ վեկտորներ $d$ չափանի տարածությունում իզոտրոպ լինելու համար անհրաժեշտ է $d << n$: Բացի այդ այս մոդելով տեսականորեն բացատրվում է անալոգ բառերի հայտնաբերման խնդրի  արդյունավետ լուծումը բառերի ներդրված վեկտորների միջոցով՝ կատարելով  գծային հանրահաշվի պարզագույն վեկտորական գործողություններ։ Այն որոշակիորեն միավորում է ներդրված վեկտորների կառուզման բազմաթիվ մոդելներ և մեթոդների ընտանիքներ, մասնավորապես \text{word2vec} մեթոդների ընտանիքը, $glove$ մոդելը և այլն, բացի այդ բառերի ներդված վեկտորների օգտագործմամբ  բազմաթիվ խնդիրներ տեսական հիմնավորումներ են ստանում, որնցից են անալոգ բառերի հայտնաբերումը, բազմիմաստ բառերի ներդրված վեկտորների և դրանց տարբեր իմաստների ներդրված վեկտորների միջև կապը։ Տեսականորեն բացատրվում է, թե ինչպես բազմիմաստ բառը կարել է ստանալ տարբեր իմաստներին համապատասխան ներդրված վեկտորների գծային սուպերպոզիցիայի միջոցով: Տեքստերի  հավաքածուի՝ կորպուսի գեներացիան մոդելը դիտարկում է որպես դինամիկ պրոցես, որտեղ $t$-րդ բառը ծնվում է   $t$-րդ քայլի ժամանակ: Պրոցեսը պայմանավորված է $c_t$ դիսկուրս վեկտորի պատահական քայլով, որի կորդինատները ցույց են տալիս, թե ինչի մասին է խոսվում տվյալ պահին։ Յուրաքանչյուր բառ ունի $t$ ժամանակից անկախ, թաքնված մի  վեկտոր, որը իր և դիսկուրս վեկտորի հետ կորելացիաներ է պարունակում։ Նկարագրված ենթադրությունները մոդելավորում է լոգ-գծային գեներատիվ մոդելով՝
 \begin{equation}
 p \left (w_t | c_t \right ) \propto e ^{c^T_tv_w } 
 \label{gen_model_eq}
 \end{equation}
        \par Վերոնշյալ $c_t$ դիսկուրս վեկտոր  կատարում է փոքր պատահական քայլ ($c_{t+1}$ վեկտորը ստացվում է $c_t$-ին գումարելով փոքր տեղաշարժի վեկտոր՝ $c_{t+1} = c_t + \delta _t$), այնպես որ մոտիկ բառերը գեներացվեն նմանատիպ դիսկուրս վեկտորներից։ Քանի որ մոդելը պահպանելու է կորպուսում բառազույգերի համատեղ հայտնվելու հավանականությունները, ապա դիսկուրս վեկտորին պատահական բայց ոչ մշտական մեծ տեղաշարժեր թույլատրված են, քանի որ դրանց ազդեցությունը այդ հավանականությունների վրա աննշան է լինելու։ 
        
$n$-ով նշանակենք բառերի քանակը և $d$-ով՝ դիսկուրս վեկտորի տարածության չափը, որտեղ $1 \leq d \leq n$:  Նախնական ենթադրենք, որ ինչ որ տիրույթում բառերի ներդված վեկտորները հավասարաչափ են բաշխված, որը   Բայեսյան վիջակագրությունում հայտնի նախնական ենթադրությունն է տվյալների հավանականային բաշխման վերաբերյալ։ Ըստ այս նախնական ենթադրության՝ բառերի ներդրված վեկտորները  գեներացվել են միմյանցից անկախ և նույնական $v = s \hat v$, պատահական մեծությունների արտադրյալին համապատասխան բաշխումից,   որտեղ $\hat v$-ն սֆերային նորմալ բաշխման պատահական մեծությունն է, իսկ $s$-ը սկալյար պատահական մեծություն է, որի մաթսպասումն է $\tau$  և միշտ վերևից սահմանափակ է $\kappa$-ով: Հարկ է նշել, որ $s$-ի ընտրությունից է կախված մոդելավորման իրատեսական լինելը, սակայն այն այդքան էլ կարևոր չի տեսական հիմնավորումների համար։ Դիսկուրս վեկտորի պատահական քայլը հավասարաչափ է բաշված $\mathcal{C}$ միավոր սֆերայում։ Պատահական քայլի երկարությունը $l_2$ նորմով ամենաշատը $\frac{\epsilon_2}{d}$ է։ Նկարագրված ենթադրությունների դեպքում \ref{gen_model_eq} հավասարման նորմալաիզացիայի $Z_c = \sum_w{e^{v^T_wc}}$ գործակիցը կոնցենտրացվում է որևէ $Z$ հաստատունի շրջակայքում, որն ամփոփված է հետևյալ լեմմայում՝ 
\begin{lemma}[\cite{bib_item_14}]
Ներդրված վեկտորների վերոնշյալ Բայեսյան ենթադրությունների բավարարման դեպքում՝
\begin{equation}
p((1-\epsilon_z)Z \leq Z_c \leq (1+\epsilon_z)Z) \geq 1- \delta,
\label{part_concent_eq}
\end{equation}
որտեղ $\epsilon_z = \widetilde O(\frac{1}{\sqrt{n}})$ և $\delta = e^{-\Omega \left({\log^2n} \right)}$։
\end{lemma}

$Z_c$ նորմալիզացիոն գործակցի հաստատունի շուրջ կոնցենտրացիայի դեպքում, հնարավոր է լինում բանաձևներ ստանալ կորպուսում $w$ բառի  $p(w)$ և $w, w'$ բառերի ֆիքսված $q$ չափանի պատուհանի ներսում հայտնվելու $p(w, w')$ հավանականությունների մոտարկման համար։ Բանաձևերի տեսքը ձևակերպված է հիմնական աշխատանքի հիմնական թեորեմում՝
\begin{theorem}[\cite{bib_item_14}]
\label{main_thorem}
Եթե բառերի ներդրված վեկտորները բավարարում են  \ref{part_concent_eq} անհավասարությանը և ֆիքսված պատուհանի չափը $q$ է, ապա
\begin{equation}
\log p_q(w, w') = \frac{||v_w+v_{w'}||_2^2}{2d} - 2\log Z + \gamma \pm \epsilon
\label{pair_eq}
\end{equation}
\begin{equation}
\log p(w) = \frac{||v_w||_2^2}{2d} - \log Z \pm \epsilon
\label{mono_eq}
\end{equation}
\begin{equation}
PMI_q(w, w') = \frac{v^T_wv_{w'}}{d} + \gamma \pm O(\epsilon),
\label{pmi_eq}
\end{equation}
որտեղ $\epsilon = O(\epsilon_z) + \widetilde O(\frac{1}{d}) + O(\epsilon_2)$ և $\gamma = \log \left( \frac {q(q-1)}{2} \right)$։
\end{theorem}


Բառերի ներդրված վեկտորների բաշխման վերբերյալ կատարված Բայեզյան նախնական ենթադրությունները կարելի է մեղմացնել և փոխարինել այդ վեկտորների որոշակի հատկություննով։ Կարելի է ենթադրել,  որ գոյություն ունեն բառերի ներդված վեկտորներ, որոնք գտնվում են որևէ տիրույթում, որտեղ այս վեկտորները տարածական իզոտրոպ են հետևյալ կերպ։ \textit{Գրեթե բոլոր միավոր $c \in \mathcal{C}$ վեկտորների համար  $Z_c = \sum_w{e^{v^T_wc}}$ գումարը շատ մոտ է ինչ-որ $Z$  հաստատունի:} 
\par Ներդրված վեկտորների ստացման օպտիմիզացիոն նպատակային ֆուկնցիայի դուրս բերման համար  օգտագործվում է \ref{main_thorem} թերոեմի բանաձևերը։ Դիցուք կորպուսում  $x_{w, w'}$-ը $w$ և $w'$ բառազույգի  նույն պատհունանի ներսում հայտնվելու քանակն է։ Դիսկուրս վեկտորի հաջորդական քայլերի արդյունքում գեներացված բառերը իրարից անկախ չեն, սակայն եթե պատահական քայլը այնքան երկար լինի, որ դիսկուրս տարուծության միավոր սֆերայի որևէ ուղության վրա կետրոնացված դիսկուրսներ չլինեն, որոնցից գեներացվել են բառերը, ապա $x_{w, w'}$ քանակների բաշխումը շատ մոտ է լինելու ընդհանրացված բինոմական դիսկրետ բաշխմանը՝
\begin{equation}
Mul(\widetilde L, \{p(w, w')\}) = \prod_{(w, w')}{\frac{\widetilde L!}{x_{w, w'}!}p(w, w')^{x_{w, w'}}}։
\end{equation}

Ենթադրելով այս մոտարկումը, \cite{bib_item_14} աշխատանքում ցույց է տրվում, որ տրված $x_{w, w'}$ քանակներով  կորպուսը գեներացնելու մեծագույն հավանականություն կունենա այն մոդելը, որի ներդրված վեկտորները կբավարարեն հետևյալ մինիմիզացիայի խնդրին՝
\begin{equation}
\min_{\{v_w\}, C}{\sum_{w, w'}{x_{w, w'}} \left(\log(x_{w, w'}) - ||v_w+v_{w'}||_2^2 - C  \right)^2}։
\label{sn_obj_eq}
\end{equation}

Օգտագործելով \ref{pmi_eq} հավասարությունը՝  ճշմարտանմանության մաքսիմումի գնահատման միջոցով կարելի է ստանալ նմանատիպ օպտիմիզացիոն նպատակային ֆունկցիա՝ 
\begin{equation}
\min_{\{v_w\}}{\sum_{w, w'}{x_{w, w'}} \left(PMI(w, w') - v^T_wv_{w'}\right)^2}։
\label{pmi_obj_eq}
\end{equation}

Այս երկու նպատակային ֆունկցիաների լուծումը գտնելը պահանջում է կատարել մատրիցայի կշռված սինգուլյար վերլուծության, որը $NP$ բարդություն ունի, սակայն փորձնական եղանակով պարզվում է գրադիենտային վայերջքի իջեցման եղանակով հնարավոր է մինիմիզացնել վերոնշյալ երկու նպատակային ֆունկիցաները։ 

Համեմատելով $glove$ մոդելում փորձարարական ճանապարհներով դուրս բերված \ref{glove_opt_eq} օպտիմիզացիոն նպատակային ֆուկցիան \ref{sn_obj_eq}-ի հետ՝  \ref{glove_opt_eq}-ում մասնակցող անդամները, օգտագործելով թեորեմ \ref{main_thorem} բանաձևերը, իմաստ են ստանում, մասնավորապես $b_w = ||v_w||_2^2$։
\par $Word2vec$ ընտանիքի կոնտեքստից բառ կանխատեսման մոդելի վերափոխված մի տարբերակ դիտարկենք։ $w_{k+1}$ բառի հայտնվելու հավանականությունը կախված նախորդ $w_1, w_2, ..., w_k$ հետևյալն է՝
\begin{equation}
p(w_{k+1}| \{w_i\}_{i=1}^k) \propto e^{\frac{1}{k}\sum_{i=1}^{k}{v^T_{w_{k+1}}v_{w_i}}}։
\label{cbow_eq}
\end{equation}
\par Ցույց տանք, որ հավանականության մեջ մասնակցող նախորդ $k$ բառերի վեկտորական միջինը տեսականորեն կարելի է հիմնավորել։ Դիտարկենք դիսկուրս վեկտորի պատահական քայլով պայմանավորված գեներատիվ մոդելի պարզեցված տարբերակը։ Դիսկուրս $c$ վեկտորի նմուշը վերցնել միավոր վեկտորների $\mathcal{C}$ տարածության հավասարահավանական բաշխումից, որից հետո $k$ պատուհանի բառերի նմուշը վերցնել հետևյալ բաշխումից՝
 $$(w_1, w_2, ..., w_k) \sim e^{\frac{\sum_{i=1}^{k}{c^Tv_{w_i}}}{Z_c}}։$$ Բացի այդ ենթադրենք բոլոր $c$ վեկտորների համար $Z_c = Z$ հաստատունի։
 
 \begin{preposition}
 Պարզեցված գեներատիվ մոդելի դեպքում՝
 $$\max_{c  \in C}{p(c|w_1, ..., w_k)} = \frac{\sum_{i=1}^{k}{v_{w_i}}}{||\sum_{i=1}^{k}{v_{w_i}}||_2}։$$
 
 \end{preposition}
 
 Նկատենք որ, $p(c|w_1, ..., w_k)$ հավանականության մաքսիմումը  ըստ $c$-ի և $word2vec$-ի վերափոխված մոդելում մասնակցող $k$ բառերի վեկտորական միջինը միմյանցից տարբերվում է հաստատուն գործակցով, որը փորձնական աշխատանքներում հաշվարկի նվազեցման նպատակներով հաջախ բաց է թողնվում։
 
 \par Արդեն նշել ենք որ անալոգ բառերի հայտնաբերման խնդիրը բարձր արդյունավետությամբ հնարավոր է լուծել հանրահաշվի վեկտորոական պարզագույն գործողությունների շնորհիվ։
 \begin{equation}
 d = \argmax_{d} v^T_d(v_c+v_b-v_a),
 \label{anal_opt_eq}
 \end{equation}
 որտեղ բառերի ներդրված վեկտորները նորմալացված են այնպես որ $||v_d||_2^2 = 1$։ \ref{anal_opt_eq}-ը կարելի է գրել նաև հետևյալ կերպ՝
 \begin{equation}
 d = \argmin_{d} || v_a - v_b - v_c + v_d||_2^2։
  \label{anal_norm_opt_eq}
 \end{equation} 
\ref{anal_opt_eq}-ի և \ref{anal_norm_opt_eq}-ի հավասարությունը  հետևում է $||v_a - v_b - v_c + v_d||_2^2 =  ||v_a - v_b - v_c + v_d||_2^2 + ||v_d||_2^2 + 2v^T_d(v_c+v_b-v_a)$ և $||v_d||_2^2 = 1$ հավասարություններից։
\pagebreak

\section*{\hfill Սեմանտիկ ներկայացումների համեմատական ուսուցում \hfill} \noindent


Դիցուք տրված  է $S =\{(x_j, x^+_j, x^-_{j1}, ..., x^-_{jk})\}_{j=1}^M$ ուսուցման բազմությունը և ֆունկցիաների $F$ բազմությունը, $f \in F$, $f: \mathcal{X} \to \mathcal{R}^d$ արտապատկերում է։ $F$ ֆուկնցաների դասի սահմանափակումը $S$ բազմության վրա  նշանակենք $\mathcal{F} \circ S$-ով որը հետևյալ բազմությունն է՝
$$\mathcal{F} \circ S = \{((f(x), f(x^+), f(x^-_{1}), ..., f(x^-_{k}))) | f \in \mathcal{F} \text{ և }  (x, x^+, x^-_{1}, ..., x^-_{k}) \in S \}։$$
$\mathcal{F}$ ֆունկցիաների դասի Ռադեմախերի բարդությունը $S$ բազմության նկատմամ հետևյալն է՝
$$R(\mathcal{F} \circ S) = \E_{\sigma \sim \{\pm 1\} ^ {(k+2)dM}} \left [{\sup_{f \in \mathcal{F}}{\langle \sigma, f_{|S} \rangle}} \right]։$$ 
\par Դիտարկենք $g:\mathcal{X} \to \mathcal{R}^{Td}$ արտապատկերումը, որտեղ  $g = (f^1, ..., f^T)$ և յուրանքանչյուր $f^t \in \mathcal{F}$։ $G$-ով նշանակենք նկարգրված $g$ արտապատեկերումների դասը։ 

Այժմ ցույց տանք հետևյալ անհավասարությունը՝ $R(\mathcal{G} \circ S) \leq TR(\mathcal{F} \circ S)$։
\begin{align*}
R(\mathcal{G} \circ S) &= \E_{\sigma \sim \{\pm 1\} ^ {(k+2)TdM}} \left [{\sup_{g \in \mathcal{G}}{\langle \sigma, g_{|S} \rangle}} \right] \leq \E_{\substack{\sigma^1 \sim \{\pm 1\} ^ {(k+2)dM} \\ \vdots \\ \sigma^T \sim \{\pm 1\} ^ {(k+2)dM}}} \left [{\sum_{t=1}^T\sup_{f^t \in \mathcal{F}}{\langle \sigma, f^t_{|S} \rangle}}\right] \\
&= \E_{\substack{\sigma^1 \sim \{\pm 1\} ^ {(k+2)dM} \\ \vdots \\ \sigma^T \sim \{\pm 1\} ^ {(k+2)dM}}} \left [{\sum_{t=1}^T\sup_{f^t \in \mathcal{F}}{\langle \sigma, f^t_{|S} \rangle}}\right] 
=\sum_{t=1}^T \E_{\sigma^t \sim \{\pm 1\} ^ {(k+2)dM} } \left [{\sup_{f^t \in \mathcal{F}}{\langle \sigma, f^t_{|S} \rangle}}\right]  \\
&= \sum_{t=1}^T R(\mathcal{F} \circ S) = TR(\mathcal{F} \circ S)
 \end{align*}
\phantomsection
\addcontentsline{toc}{section}{Սեմանտիկ ներկայացումների համեմատական ուսուցում}


\pagebreak

\section*{\hfill 
 \center{Վերահսկվող ուսուցմամբ ստացվող ներկայացումերի պիտանելիությունը այլ առաջադրանքներում}
 \hfill} \noindent
\phantomsection
\addcontentsline{toc}{section}{ Վերահսկվող ուսուցմամբ ստացվող ներկայացումերի պիտանելիությունը այլ առաջադրանքներում}
 
 $\mathcal{X}$-ով նշանակենք բոլոր հնարավոր տվյալների օրինակները, իսկ $\mathcal{C}$-ով նշանակենք բոլոր պիտակների կամ դասերի բազմությունը։ Յուրաքանչյուր $c \in \mathcal{C}$ դասին համպատասխանում է $\mathcal{X}$ բազմության վրա որոշված ինչ-որ $\mathcal{D}_c(x)$ բաշխում, այն ցույց է տալիս, թե $x$ օրինակը ինչքանով է $c$ դասին համապատասխան։ Ուսուցումը կատարվում է $\mathcal{F}$ ներկայացումների ֆունկցիաների դասի վրա։ $\forall f \in \mathcal{F}$  ֆունկցիա $\mathcal{X}$ տվյաների բազմությունը արտապատկերում $d$-չափանի էվկլիդյան $\mathcal{R}^d$տարծություն՝ $f:\mathcal{X}\rightarrow\mathcal{R}^d$, բացի այդ կդիտարկենք միայն սահմանափակ ֆունկցիաները՝
 $$||f(x)|| \leq R \text{    } \forall x \in \mathcal{X} \text{ և } R > 0։$$ 


\noindent Նաև կենթադրենք, որ դասերի վրա կա ինչ-որ $\rho$ բաշխում, որը բնութագրում է, թե ինչպես են այդ դասերը հանդիպում չպիտակավորված տվյալներում:

\subsection*{Վերահսկվող առաջադրանքներ}

\par Այժմ կնկարագրենք այն առաջադրանքները, որոնց միջոցով փորձարկվելու է ներկայացումների $f$ ֆուկցիան: $k+1$ դասերից բաղկացած $\mathcal{T}$ վերահսկվող առաջադրանքը, բաղկացած է $$\{c_1, ..., c_{k+1}\} \subseteq \mathcal{C}$$
միմյանցից տարբեր դասերից: Պիտակավորված տվյալների բազմությունը $\mathcal{T}$ առաջադրանքի համար բաղկացած է $m$ հատ միմյանցից անկախ և միևնույն բաշխումից ընտրրված օրինակներից: Այդ օրինակները ընտրվում են ստորև նկարագրված պրոցեսով:

\textit{$c \in \{c_1, ..., c_{k+1}\} $   դասը ընտրվում է ըստ $\mathcal{D}_{\mathcal{T}}$ բաշխման, որից հետո $x$ օրինակը ընտրվում է $\mathcal{D}_c$ բաշխումից: Դրանք միասին ձևավորում են պիտակավորված $(x, c)$ զույգը, որը ունի հետևյալ բաշխումը՝
$$\mathcal{D}_{\mathcal{T}} (x, c) = \mathcal{D}_{c}(x)\mathcal{D}_{\mathcal{T}}(c):$$}

\subsection*{Վերահսկվող ներկայացումների գնահատման չափը}

\iffalse
\section*{\hfill Գեներատիվ ուսուցում \hfill} \noindent

\phantomsection
\addcontentsline{toc}{section}{Գեներատիվ ուսուցում}


\par Մինչդեռ գիտության մեջ որոշ երևույթներ կարելի է ճշգրիտ մոդելավորել, սակայն հաճախ բարդ համակարգերը մաթեմատիկորեն մասամբ միայն կարելի է ճշգրիտ բացատրել։ Բացի այդ հնարավոր է բազմաթիվ խնդիրների վարկածային մոդելները անորոշություն և թերի ինֆորմացիա ունենան։ Մեքենայական ուսուցումը և վիճակագրությունը ոչ դետերմինիստիկ մոդելների համար ուսումնասիրման մեթոդներ է առաջարկում՝  դիտարկվող փոփոխականների հավանկանային խտության գնահատման միջոցով։ Հավանականային խտության վերաբերյալ նախնական ենթադրութուններ կարելի է կատարել և խտության գնահատումը իրականացնել՝ օգտագործելով փորձնական տվյալները։ Ուստի տրված $x_1, ..., x_n$ փոփոխականներով խնդրում համակարգը կարող է որոշվել բոլոր այդ փոփոխականների համատեղ հավանականային բաշխմամբ՝ $p  \left ( x_1, ..., x_n \right )$։
\par Բազմանթիվ ոլորտներում, կան բարդ և խճճված խնդիրներ, որոնց համար ամբողջական մոդելների ձեռքով կառուցումը  տեսական և քանակական մոտեցմամբ  բարդ է։ Տվյալների մեծ հասանելիությունը և հաշվողական մեքենաների հզորությունը հնարավոր է դարձնում ստեղծել տվյալների հիման վրա հավանականային մոդելներ՝ հեռու մնալով կանոների վրա հիմնված և ձեռքով սահմանված մոդելներից։ 

\par
Մեքենայական ուսուցման մոդելները կարելի է բաժանել երկու խմբի՝ դիսկրիմինատիվ և գեներատիվ։


Գեներատիվ կամ Բայեսյան հավանականային մոդելներում համակարգի մուտքի և ելքի փոփոխականները ներկայացվում են համատեղ հավանականային բաշխման միջոցով։ Այս փոփոխականները կարող են լինել դիսկրետ և անընդհատ ինչպես նաև կարող են լինել բազմաչափ։ Քանի որ գեներատիվ մոդելով որոշվում է բոլոր փոփոխականների բաշխումը, լրիվ  հավանականության և Բայեսի բանաձևերի միջոցով   այն կարող է նաև օգտագործվել մեքենայական ուսուցման դասակարգման և ռեգրեսիայի խնդիրներում։ $(x_1, ..., x_n)$ տեսքի $n$ փոփոխականների համար մենք ունենք համատեղ հավանականային բաշխումը $p(x_1, ..., x_n)$ տեսքով։ Եթե Տրված համատեղ հավանականային բաշխումը ճշգրտորեն արտահայտում է փոփոխականների միջև հարաբերությունները ( հնարավոր է ոչ դետերմինիստիկ), ապա այն ուղակիորեն կարելի է օգտագործել ինֆերենսի համար։  Այն կարելի է կատարել հավանականային տեսության պարզագույն ձևափոխություններով, ինչպիսիք են մարգինալացումը, պայմանականացումը Բայեսի բանաձևը՝
$$p(x_j) = \sum_{\forall x_i, i \neq j}{p(x_1, ..., x_n)}$$
$$p(x_j | x_k) = \frac {p(x_j, x_k)} {p(x_k)}$$
$$p(x_j | x_k) = \frac {p(x_j)p(x_k |x_j)} {p(x_k)}$$

\newpage
\fi
\pagebreak
\medskip
\begin{thebibliography}{9}
\phantomsection
\addcontentsline{toc}{section}{Գրականություն} 

\bibitem{bib_item_1}
John R Firth.
\textit{A synopsis of linguistic theory,}  1930-1955. Studies in linguistic analysis, 1957.

\bibitem{bib_item_2}
Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Corrado, and Jeff Dean. \textit{ Distributed representations of words and phrases and their compositionality.} 
In C. J. C. Burges, L. Bottou, M. Welling, Z. Ghahramani, and K. Q. Weinberger, editors, Advances in Neural Information
Processing Systems 26, pages 3111–3119. Curran Associates, Inc., 2013c.

\bibitem{bib_item_3}
Karen Sparck Jones. \textit{A statistical interpretation of term specificity and its application in retrieval.}
Journal of documentation, 28(1):11–21, 1972.

\bibitem{bib_item_4}
Tomas Mikolov, Quoc V Le, and Ilya Sutskever. \textit{Exploiting similarities among languages for machine
translation.} arXiv preprint arXiv:1309.4168, 2013b.

\bibitem{bib_item_5}
Sepp Hochreiter and J¨urgen Schmidhuber. \textit{Long short-term memory.} Neural computation, 9(8):
1735–1780, 1997.

\bibitem{bib_item_6}
Yoshua Bengio, R´ejean Ducharme, Pascal Vincent, and Christian Jauvin. \textit{A neural probabilistic
language model.} Journal of machine learning research, 3(Feb):1137–1155, 2003.

\bibitem{bib_item_7}
Ilya Sutskever, Oriol Vinyals, and Quoc V Le. \textit{Sequence to sequence learning with neural networks.}
In Advances in neural information processing systems, pages 3104–3112, 2014.

\bibitem{bib_item_8}
Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. \textit{Neural machine translation by jointly
learning to align and translate.} arXiv preprint arXiv:1409.0473, 2014.


\bibitem{bib_item_9}
Ramesh Nallapati, Bowen Zhou, Cicero dos Santos, C¸ a glar Gul¸cehre, and Bing Xiang. \textit{ Abstractive
text summarization using sequence-to-sequence rnns and beyond.} CoNLL 2016, page 280, 2016.

\bibitem{bib_item_10}
Kelvin Xu, Jimmy Ba, Ryan Kiros, Kyunghyun Cho, Aaron Courville, Ruslan Salakhudinov, Rich
Zemel, and Yoshua Bengio. Show, attend and tell: \textit{Neural image caption generation with visual
attention.} In International Conference on Machine Learning, pages 2048–2057, 2015.


\bibitem{bib_item_11}
Oriol Vinyals, Alexander Toshev, Samy Bengio, and Dumitru Erhan. Show and tell: \textit{A neural
image caption generator.} In Proceedings of the IEEE conference on computer vision and pattern
recognition, pages 3156–3164, 2015.

\bibitem{bib_item_12}
Guillaume Lample, Miguel Ballesteros, Sandeep Subramanian, Kazuya Kawakami, and Chris Dyer.
\textit{Neural architectures for named entity recognition.} In Proceedings of NAACL-HLT, pages 260–
270, 2016.

\bibitem{bib_item_13}
Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D Manning, Andrew Ng, and
Christopher Potts. \textit{Recursive deep models for semantic compositionality over a sentiment treebank.} In Proceedings of the 2013 conference on empirical methods in natural language processing,
pages 1631–1642, 2013.

\bibitem{bib_item_14}
Sanjeev Arora, Yuanzhi Li, Yingyu Liang, Tengyu Ma, and Andrej Risteski. Rand-walk: \textit{A latent
variable model approach to word embeddings.} arXiv preprint arXiv:1502.03520, 2015

\bibitem{bib_item_15}
Gerard Salton. \textit{The smart retrieval systemexperiments in automatic document processing.} 1971.

\bibitem{bib_item_16}
Gerard Salton and Christopher Buckley.\textit{Term-weighting approaches in automatic text retrieval.
Information processing and management,} 24 (5):513–523, 1988.


\bibitem{bib_item_17}
John S Breese, David Heckerman, and Carl Kadie. \textit{Empirical analysis of predictive algorithms for
collaborative filtering.} In Proceedings of the Fourteenth conference on Uncertainty in artificial
intelligence, pages 43–52. Morgan Kaufmann Publishers Inc., 1998.

\bibitem{bib_item_18}
Zi Yin, Keng-hao Chang, and Ruofei Zhang. \textit{Deepprobe: Information directed sequence understanding and chatbot design via recurrent neural networks.} In Proceedings of the 23rd ACM
SIGKDD International Conference on Knowledge Discovery and Data Mining, pages 2131–2139.
ACM, 2017.

\bibitem{bib_item_19}
Andrea Frome, Greg S Corrado, Jon Shlens, Samy Bengio, Jeff Dean, Tomas 	olov, et al. \textit{Devise:
A deep visual-semantic embedding model.} In Advances in neural information processing systems,
pages 2121–2129, 2013.

\bibitem{bib_item_20}
Eliya Nachmani, Elad Marciano, Loren Lugosch, Warren J Gross, David Burshtein, and Yair Beery.
\textit{Deep learning methods for improved decoding of linear codes.} arXiv preprint arXiv:1706.07043,
2017.

\bibitem{bib_item_21}
Scott Deerwester, Susan T Dumais, George W Furnas, Thomas K Landauer, and Richard Harsh-
man. \textit{Indexing by latent semantic analysis. Journal of the American society for information
science,} 41(6):391, 1990.

\bibitem{bib_item_22}
Kenneth Ward Church and Patrick Hanks. \textit{ Word association norms, mutual information, and
lexicography.} Computational linguistics, 16(1):22–29, 1990.

\bibitem{bib_item_23}
Yoshiki Niwa and Yoshihiko Nitta. \textit{Co-occurrence vectors from corpora vs. distance vectors from
dictionaries.} In Proceedings of the 15th conference on Computational linguistics-Volume 1, pages
304–309. Association for Computational Linguistics, 1994.


\bibitem{bib_item_24}
Omer Levy and Yoav Goldberg. \textit{Neural word embedding as implicit matrix factorization.} In
Advances in neural information processing systems, pages 2177–2185, 2014.

\bibitem{bib_item_25}
Ronan Collobert and Jason Weston. 2008. \textit{ A uni-
fied architecture for natural language processing: Deep
neural networks with multitask learning.} In Proceed-
ings of the 25th International Conference on Machine
Learning.

\bibitem{bib_item_26}
Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. \textit{Efficient estimation of word representations
in vector space.} ICLR Workshop, 2013.

\bibitem{bib_item_27}
X. Rong. \textit{word2vec parameter learning explained.} arXiv:1411.2738, 2014.
https://arxiv.org/abs/1411.2738

\bibitem{bib_item_28}
Gutmann, M. and Hyvarinen, A. (2010). \textit{ Noise-contrastive estimation: A new estimation principle for
unnormalized statistical models.} In Proceedings of The Thirteenth International Conference on Artificial
Intelligence and Statistics (AISTATS’10).

\bibitem{bib_item_29}
J. Pennington, R. Socher, and C. D. Manning. \textit{GloVe: Global vectors for word representation.} In EMNLP, 2014.

\end{thebibliography}

\end{document}
